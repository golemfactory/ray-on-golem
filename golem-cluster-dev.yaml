# Ray on Golem cluster name
cluster_name: golem-cluster

# The maximum number of workers the cluster will have at any given time
max_workers: 10

# The number of minutes that need to pass before an idle worker node is removed by the Autoscaler
idle_timeout_minutes: 2

# The cloud provider-specific configuration properties.
provider:
  type: "external"
  use_internal_ips: true
  module: "ray_on_golem.provider.node_provider.GolemNodeProvider"
  parameters:
    # Port of golem webserver that has connection with golem network
    webserver_port: 4578

    enable_registry_stats: false

    # Blockchain used for payments.
    # Goerli means running free nodes on testnet,
    # Polygon is for mainnet operations.
    network: "goerli"

    # Maximum amount of GLMs that's going to be spent for the whole cluster
    budget: 2

    # Params for creating golem demands (same for head and workers)
    node_config:
      demand:
        # if not provided, image_tag will be autodetected based on currently used python and ray versions
        # check available versions at https://registry.golem.network/explore/golem/ray-on-golem
        image_tag: "approxit/ray-on-golem:managers"
        capabilities: ["vpn", "inet", "manifest-support"]
        min_mem_gib: 0
        min_cpu_threads: 0
        min_storage_gib: 0

      cost_management:  # TODO: Consider more suitable parameter name
        # Estimated average load and duration for worker that tells cost management to pick the least expensive Golem provider offers first.
        # If not provided, offers will be picked at random.
        # Both values need to be defined or undefined together.
        average_cpu_load: 0.8
        average_duration_minutes: 20

        # Amount of GLMs for average usage which Golem provider offer will be rejected if exceeded.
        # Requires "average_cpu_load" and "average_duration_minutes" parameters to take effect.
        max_average_usage_cost: 0.5

        # Amount of GLMs for worker initiation which Golem provider offer will be rejected if exceeded.
        max_initial_price: 0.01

        # Amount of GLMs for CPU utilisation per second which Golem provider offer will be rejected if exceeded.
        max_cpu_sec_price: 0.0002

        # Amount of GLMs for each second that worker runs which Golem provider offer will be rejected if exceeded.
        max_duration_sec_price: 0.0002

# The files or directories to copy to the head and worker nodes
file_mounts:
  # remote_path: local_path
  {
    "/app/ray_on_golem": "./ray_on_golem",
    "/app/golem": "../golem-core-python/golem",
  }

rsync_exclude: [
  "**/__pycache__",
]

# Tells the autoscaler the allowed node types and the resources they provide
available_node_types:
  ray.head.default:
    # The minimum number of worker nodes of this type to launch
    min_workers: 0

    # The maximum number of worker nodes of this type to launch
    max_workers: 0

    # The node type's CPU and GPU resources
    resources: {"CPU": 1}

    node_config: {} # TODO: Demand description here
  ray.worker.default:
    min_workers: 1
    max_workers: 10
    resources: {"CPU": 1}
    node_config: {}

# List of commands that will be run to initialize the nodes (before `setup_commands`)
initialization_commands: [
  "cp -fR /app/golem/* $(python -c 'import site; print(site.getsitepackages()[0])')/golem"
]

# List of shell commands to run to set up nodes
setup_commands: []

# Custom commands that will be run on the head node after common setup.
head_setup_commands: []

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands: [
  "ray start --head --node-ip-address 192.168.0.3 --include-dashboard=True --dashboard-host 0.0.0.0 --disable-usage-stats --autoscaling-config=~/ray_bootstrap_config.yaml",
]

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands: [
  "ray start --address 192.168.0.3:6379",
]

# Authentication credentials that Ray will use to launch nodes
# auth:
  # Custom username for ssh
  # ssh_user: "root"

  # If ssh_private_key will be not provided, temporary key will be created and used
  # ssh_private_key: "~/.ssh/id_rsa"

# A list of paths to the files or directories to copy from the head node to the worker nodes
cluster_synced_files: []
