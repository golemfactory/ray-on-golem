{"version": 2, "width": 131, "height": 34, "timestamp": 1718275952, "env": {"SHELL": "/bin/bash", "TERM": "xterm-256color"}}
[0.022128, "o", "\u001b[?2004h\u001b]0;mateusz@fladra: ~/devel/ray-serve-test-0/ray-serve-whoami\u0007\u001b[01;32mmateusz@fladra\u001b[00m:\u001b[01;34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[00m$ "]
[1.79964, "o", "s"]
[1.958901, "o", "c"]
[2.135071, "o", "r"]
[2.310857, "o", "e"]
[2.694304, "o", "e"]
[2.854446, "o", "n"]
[3.126698, "o", "\r\n\u001b[?2004l\r"]
[3.130015, "o", "\u001b[!p\u001b[?3;4l\u001b[4l\u001b>\u001b[?1049h\u001b[22;0;0t\u001b[4l\u001b[?1h\u001b=\u001b[0m\u001b(B\u001b[1;34r"]
[3.131719, "o", "\u001b[H\u001b[2J\u001b[H\u001b[2J\u001b[H\u001b[2J\nGNU Screen version 4.09.00 (GNU) 30-Jan-22\r\n\nCopyright (c) 2018-2020 Alexander Naumov, Amadeusz Slawinski\r\nCopyright (c) 2015-2017 Juergen Weigert, Alexander Naumov, Amadeusz Slawinski\r\nCopyright (c) 2010-2014 Juergen Weigert, Sadrul Habib Chowdhury\r\nCopyright (c) 2008-2009 Juergen Weigert, Michael Schroeder, Micah Cowan, Sadrul Habib Chowdhury\r\nCopyright (c) 1993-2007 Juergen Weigert, Michael Schroeder\r\nCopyright (c) 1987 Oliver Laumann\r\n\nThis program is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as\r\npublished by the Free Software Foundation; either version 3, or (at your option) any later version.\r\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\r\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\r\n\nYou should have received a copy of the GNU General Public License along with this program (see"]
[3.13175, "o", " the file COPYING); if not, see\r\nhttps://www.gnu.org/licenses/, or contact Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02111-1301\r\n USA.\r\n\nSend bugreports, fixes, enhancements, t-shirts, money, beer & pizza to screen-devel@gnu.org\r\n\n\nCapabilities:\r\n+copy +remote-detach +power-detach +multi-attach +multi-user +font +color-256 +utf8 +rxvt +builtin-telnet\u001b[33;50H[Press Space or Return to end.]\r\n"]
[4.00651, "o", "\u001b[H\u001b[2J\u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[4.486558, "o", "\r(reverse-i-search)`': \u001b[K"]
[4.758862, "o", "\b\b\ba': source ~/.envs/ray-serve-test-0/bin/activ\u001b[7ma\u001b[0mte\b\b\b"]
[4.934968, "o", "\r(reverse-i-search)`ac': source ~/.envs/ray-serve-test-0/bin/\u001b[7mac\u001b[0mtivate\b\b\b\b\b\b\b\b"]
[5.945238, "o", "\r\u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ source ~/.envs/ray-serve-test-0/bin/activate\b\b\b\b\b\b\b\b\r\n"]
[5.946741, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[6.20008, "o", "\r\n"]
[6.200411, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[6.360536, "o", "\r\n"]
[6.361135, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[6.520909, "o", "\r\n"]
[6.521281, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[6.697002, "o", "\r\n"]
[6.697298, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[8.184315, "o", "r"]
[8.264444, "o", "a"]
[8.439965, "o", "y"]
[9.080687, "o", " "]
[9.240914, "o", "u"]
[9.272114, "o", "p"]
[9.560202, "o", " "]
[9.784805, "o", "-"]
[10.822473, "o", "\b\u001b[K"]
[11.030466, "o", "w"]
[11.190364, "o", "h"]
[11.238504, "o", "o"]
[11.356883, "o", "ami-old.yaml "]
[11.734546, "o", "\b\b\b\b\b\b\b\b\b\b\u001b[4P.yaml "]
[12.646646, "o", "-"]
[12.790568, "o", "-"]
[12.982902, "o", "y"]
[13.04671, "o", "e"]
[13.238569, "o", "s"]
[16.27945, "o", "\r\n"]
[18.018923, "o", "\u001b[37mCluster\u001b[39m: \u001b[1mray-serve-whoami\u001b[0m\r\n\n"]
[18.019216, "o", "2024-06-13 12:52:51,024 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[18.060252, "o", "Checking External environment settings\r\n"]
[18.078823, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[18.113656, "o", "  Starting webserver on http://127.0.0.1:4578...\r\n"]
[20.31098, "o", "  Webserver is not yet running, waiting additional `2` seconds...\r\n"]
[22.312987, "o", "  Webserver is not yet running, waiting additional `2` seconds...\r\n"]
[24.322586, "o", "  Starting webserver done\r\n"]
[24.738506, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n"]
[24.738677, "o", "    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼───────"]
[24.73881, "o", "─────────────────────┼─────────────────┤\r\n"]
[24.738907, "o", "    │  driver: erc20     │  99.6495 GLM   │  19.984891726238506889 GLM  │  accepted   │  0 GLM     │  0.121387385472382900 GLM  │  13.3239 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019199295651268392 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121387385472382900 GLM  │                 │\r\n    └───"]
[24.739025, "o", "─────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[24.747368, "o", "No head node found. Launching a new cluster. \u001b[4mConfirm [y/N]:\u001b[24m y \u001b[2m[automatic, due to --yes]\u001b[0m\r\n\n"]
[24.74933, "o", "Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\r\n\n"]
[24.749678, "o", "\u001b[36mAcquiring an up-to-date head node\u001b[39m\r\n"]
[24.750078, "o", "  \u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n    Requesting 1 nodes...\r\n"]
[25.32128, "o", "    \u001b[36mRequested nodes status after 0:00:00.012750\u001b[39m\r\n"]
[25.321474, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[1/9] Getting an agreement...\u001b[0m\r\n"]
[30.328795, "o", "    \u001b[36mRequested nodes status after 0:00:05.020349\u001b[39m\r\n"]
[30.328898, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[1/9] Getting an agreement...\u001b[0m\r\n"]
[35.333026, "o", "    \u001b[36mRequested nodes status after 0:00:10.024581\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[1/9] Getting an agreement...\u001b[0m\r\n"]
[40.340921, "o", "    \u001b[36mRequested nodes status after 0:00:15.032407\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[1/9] Getting an agreement...\u001b[0m\r\n"]
[45.347711, "o", "    \u001b[36mRequested nodes status after 0:00:20.039110\u001b[39m\r\n"]
[45.347939, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[1/9] Getting an agreement...\u001b[0m\r\n"]
[50.353727, "o", "    \u001b[36mRequested nodes status after 0:00:25.045219\u001b[39m\r\n"]
[50.353852, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[1/9] Getting an agreement...\u001b[0m\r\n"]
[55.379811, "o", "    \u001b[36mRequested nodes status after 0:00:30.071122\u001b[39m\r\n"]
[55.380411, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1mNo recommended providers were found. We are extending the search to all public providers, which might be less stable. Restart the cluster to try finding recommended providers again. If the problem persists please let us know at `#Ray on Golem` discord channel (https://chat.golem.network/)\u001b[0m\r\n"]
[60.3859, "o", "    \u001b[36mRequested nodes status after 0:00:35.077206\u001b[39m\r\n             [2/9] Creating activity on provider: Passive_Kova (0x0e7b49ec9282cf7603f3b0c1d000e9815a4137d0)...\r\n             [3/9] Adding activity to internal VPN...\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[65.393731, "o", "    \u001b[36mRequested nodes status after 0:00:40.085165\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[70.400888, "o", "    \u001b[36mRequested nodes status after 0:00:45.092418\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[75.407185, "o", "    \u001b[36mRequested nodes status after 0:00:50.098657\u001b[39m\r\n"]
[75.407358, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[80.4149, "o", "    \u001b[36mRequested nodes status after 0:00:55.106345\u001b[39m\r\n"]
[80.415082, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[85.422763, "o", "    \u001b[36mRequested nodes status after 0:01:00.114197\u001b[39m\r\n"]
[85.423213, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[90.430464, "o", "    \u001b[36mRequested nodes status after 0:01:05.121944\u001b[39m\r\n"]
[90.430681, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[95.4379, "o", "    \u001b[36mRequested nodes status after 0:01:10.129420\u001b[39m\r\n"]
[95.438092, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[100.445369, "o", "    \u001b[36mRequested nodes status after 0:01:15.136881\u001b[39m\r\n"]
[100.445498, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[105.454636, "o", "    \u001b[36mRequested nodes status after 0:01:20.146089\u001b[39m\r\n"]
[105.454806, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[110.460852, "o", "    \u001b[36mRequested nodes status after 0:01:25.152372\u001b[39m\r\n"]
[110.460972, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[115.464438, "o", "    \u001b[36mRequested nodes status after 0:01:30.155991\u001b[39m\r\n"]
[115.464472, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[120.469095, "o", "    \u001b[36mRequested nodes status after 0:01:35.160626\u001b[39m\r\n"]
[120.469292, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[125.476557, "o", "    \u001b[36mRequested nodes status after 0:01:40.168070\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[130.481275, "o", "    \u001b[36mRequested nodes status after 0:01:45.172699\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[135.490453, "o", "    \u001b[36mRequested nodes status after 0:01:50.181861\u001b[39m\r\n"]
[135.490684, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[140.498347, "o", "    \u001b[36mRequested nodes status after 0:01:55.189742\u001b[39m\r\n"]
[140.498553, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[145.504934, "o", "    \u001b[36mRequested nodes status after 0:02:00.196467\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[150.509752, "o", "    \u001b[36mRequested nodes status after 0:02:05.201191\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[155.517886, "o", "    \u001b[36mRequested nodes status after 0:02:10.209430\u001b[39m\r\n"]
[155.517971, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[160.526462, "o", "    \u001b[36mRequested nodes status after 0:02:15.217955\u001b[39m\r\n"]
[160.52662, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[165.534825, "o", "    \u001b[36mRequested nodes status after 0:02:20.226297\u001b[39m\r\n"]
[165.534902, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[170.541864, "o", "    \u001b[36mRequested nodes status after 0:02:25.233318\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[175.550746, "o", "    \u001b[36mRequested nodes status after 0:02:30.242262\u001b[39m\r\n"]
[175.550875, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[180.557582, "o", "    \u001b[36mRequested nodes status after 0:02:35.249070\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[185.564496, "o", "    \u001b[36mRequested nodes status after 0:02:40.256069\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[190.569172, "o", "    \u001b[36mRequested nodes status after 0:02:45.260714\u001b[39m\r\n"]
[190.569441, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[195.576961, "o", "    \u001b[36mRequested nodes status after 0:02:50.268539\u001b[39m\r\n"]
[195.577092, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[200.583624, "o", "    \u001b[36mRequested nodes status after 0:02:55.275122\u001b[39m\r\n"]
[200.58375, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[205.589478, "o", "    \u001b[36mRequested nodes status after 0:03:00.281005\u001b[39m\r\n"]
[205.589629, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[210.597837, "o", "    \u001b[36mRequested nodes status after 0:03:05.289382\u001b[39m\r\n"]
[210.597967, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[215.605437, "o", "    \u001b[36mRequested nodes status after 0:03:10.296993\u001b[39m\r\n"]
[215.605581, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[220.618617, "o", "    \u001b[36mRequested nodes status after 0:03:15.309974\u001b[39m\r\n"]
[220.618862, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[225.629212, "o", "    \u001b[36mRequested nodes status after 0:03:20.320542\u001b[39m\r\n"]
[225.629299, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[230.637944, "o", "    \u001b[36mRequested nodes status after 0:03:25.329357\u001b[39m\r\n"]
[230.638103, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[235.644505, "o", "    \u001b[36mRequested nodes status after 0:03:30.335999\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[240.652223, "o", "    \u001b[36mRequested nodes status after 0:03:35.343589\u001b[39m\r\n"]
[240.653514, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[245.657156, "o", "    \u001b[36mRequested nodes status after 0:03:40.348194\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[250.662081, "o", "    \u001b[36mRequested nodes status after 0:03:45.353542\u001b[39m\r\n"]
[250.662241, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[255.668838, "o", "    \u001b[36mRequested nodes status after 0:03:50.360339\u001b[39m\r\n"]
[255.669, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[260.67596, "o", "    \u001b[36mRequested nodes status after 0:03:55.367506\u001b[39m\r\n"]
[260.676113, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[265.681339, "o", "    \u001b[36mRequested nodes status after 0:04:00.372783\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[270.689806, "o", "    \u001b[36mRequested nodes status after 0:04:05.381303\u001b[39m\r\n"]
[270.689967, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[275.6962, "o", "    \u001b[36mRequested nodes status after 0:04:10.387772\u001b[39m\r\n"]
[275.696236, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[280.701293, "o", "    \u001b[36mRequested nodes status after 0:04:15.392792\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[286.054214, "o", "    \u001b[36mRequested nodes status after 0:04:20.745739\u001b[39m\r\n"]
[286.05433, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[291.062221, "o", "    \u001b[36mRequested nodes status after 0:04:25.753633\u001b[39m"]
[291.06369, "o", "\r\n"]
[291.063908, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[296.068759, "o", "    \u001b[36mRequested nodes status after 0:04:30.760351\u001b[39m\r\n"]
[296.068917, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[301.07671, "o", "    \u001b[36mRequested nodes status after 0:04:35.768175\u001b[39m\r\n"]
[301.076851, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[306.081086, "o", "    \u001b[36mRequested nodes status after 0:04:40.772472\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[311.088497, "o", "    \u001b[36mRequested nodes status after 0:04:45.780028\u001b[39m\r\n"]
[311.088628, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m"]
[311.088696, "o", "\r\n"]
[316.092917, "o", "    \u001b[36mRequested nodes status after 0:04:50.784415\u001b[39m\r\n"]
[316.093038, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[321.096927, "o", "    \u001b[36mRequested nodes status after 0:04:55.788520\u001b[39m\r\n"]
[321.09707, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[326.104277, "o", "    \u001b[36mRequested nodes status after 0:05:00.795341\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[331.112321, "o", "    \u001b[36mRequested nodes status after 0:05:05.803369\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[336.123865, "o", "    \u001b[36mRequested nodes status after 0:05:10.815358\u001b[39m\r\n"]
[336.124032, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[341.131311, "o", "    \u001b[36mRequested nodes status after 0:05:15.822559\u001b[39m\r\n"]
[341.131387, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[346.139986, "o", "    \u001b[36mRequested nodes status after 0:05:20.831261\u001b[39m\r\n"]
[346.140266, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[351.144432, "o", "    \u001b[36mRequested nodes status after 0:05:25.835975\u001b[39m\r\n"]
[351.144604, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[356.149286, "o", "    \u001b[36mRequested nodes status after 0:05:30.840819\u001b[39m\r\n"]
[356.149408, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[4/9] Deploying image...\u001b[0m\r\n"]
[361.156436, "o", "    \u001b[36mRequested nodes status after 0:05:35.848048\u001b[39m\r\n"]
[361.156486, "o", "             Failed to create activity, retrying. error='golem.resources.pooling_batch.exceptions.BatchTimeoutError: PoolingBatch(d6bb3f0de16d4850960848bd4c083c4d) did not finish in 300.0 seconds'\r\n             [1/9] Getting an agreement...\r\n             ["]
[361.156592, "o", "2/9] Creating activity on provider: head (0x0f116a0aec2c3c7c14902d467483ea5c38126f1f)...\r\n             [3/9] Adding activity to internal VPN...\r\n             [4/9] Deploying image...\r\n             [5/9] Starting VM container...\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[366.163658, "o", "    \u001b[36mRequested nodes status after 0:05:40.855010\u001b[39m\r\n"]
[366.163886, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[371.171575, "o", "    \u001b[36mRequested nodes status after 0:05:45.862905\u001b[39m\r\n"]
[371.171827, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[376.182645, "o", "    \u001b[36mRequested nodes status after 0:05:50.873718\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[381.193531, "o", "    \u001b[36mRequested nodes status after 0:05:55.884813\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[386.204272, "o", "    \u001b[36mRequested nodes status after 0:06:00.895378\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[391.213958, "o", "    \u001b[36mRequested nodes status after 0:06:05.905149\u001b[39m\r\n"]
[391.214234, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[396.221312, "o", "    \u001b[36mRequested nodes status after 0:06:10.912884\u001b[39m\r\n"]
[396.221512, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[401.231967, "o", "    \u001b[36mRequested nodes status after 0:06:15.923421\u001b[39m\r\n"]
[401.232127, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[406.243386, "o", "    \u001b[36mRequested nodes status after 0:06:20.934485\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[411.252426, "o", "    \u001b[36mRequested nodes status after 0:06:25.943541\u001b[39m\r\n      \u001b[37mnode0\u001b[39m: \u001b[1m[6/9] Running bootstrap commands...\u001b[0m\r\n"]
[416.263135, "o", "    \u001b[36mRequested nodes status after 0:06:30.954364\u001b[39m\r\n             [7/9] Starting ssh service...\r\n             [8/9] Checking SSH connection...\r\n"]
[416.263427, "o", "      \u001b[37mnode0\u001b[39m: \u001b[1m[9/9] Activity ready on provider: head (0x0f116a0aec2c3c7c14902d467483ea5c38126f1f)\u001b[0m\r\n    \u001b[32mAll 1 requested nodes ready\u001b[39m\r\n  Launched a new head node\r\n  \u001b[36mFetching the new head node\u001b[39m\r\n"]
[416.26801, "o", "  \r\n"]
[416.268217, "o", "\u001b[2m<1/1>\u001b[0m \u001b[36mSetting up head node\u001b[39m\r\n"]
[416.270061, "o", "  Prepared bootstrap config\r\n"]
[416.279575, "o", "  \u001b[37mNew status\u001b[39m: \u001b[1mwaiting-for-ssh\u001b[0m\r\n  \u001b[2m[1/7]\u001b[0m \u001b[36mWaiting for SSH to become available\u001b[39m\r\n"]
[416.279855, "o", "    Running `\u001b[1muptime\u001b[0m` as a test.\r\n"]
[416.287966, "o", "    \u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[416.714713, "o", "Warning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\n"]
[417.655074, "o", " 10:59:29 up 0 min,  0 user,  load average: 0.52, 0.15, 0.05\r\nShared connection to 192.168.0.4 closed.\r\n"]
[417.658838, "o", "    \u001b[32mSuccess.\u001b[39m\r\n"]
[417.664037, "o", "  Updating cluster configuration.\u001b[2m [hash=0ddddc96a1b38c2a442c66c60e1b1a420d568b29]\u001b[0m\r\n"]
[417.66827, "o", "  \u001b[37mNew status\u001b[39m: \u001b[1msyncing-files\u001b[0m\r\n"]
[417.668343, "o", "  \u001b[2m[2/7]\u001b[0m \u001b[36mProcessing file mounts\u001b[39m\r\n"]
[417.907048, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[418.723603, "o", "    \u001b[1m/root/ray-serve-whoami.py\u001b[0m from \u001b[1m./ray-serve-whoami.py\u001b[0m\r\n"]
[418.954627, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[419.774069, "o", "    \u001b[1m/root/ray-serve-whoami-client.py\u001b[0m from \u001b[1m./ray-serve-whoami-client.py\u001b[0m\r\n"]
[420.007376, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[421.059123, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[421.88124, "o", "  \u001b[2m[3/7]\u001b[0m No worker file mounts to sync\r\n"]
[421.8859, "o", "  \u001b[37mNew status\u001b[39m: \u001b[1msetting-up\u001b[0m\r\n  \u001b[2m[4/7]\u001b[0m \u001b[36mRunning initialization commands\u001b[39m\r\n"]
[421.897838, "o", "[DEBUG websocat] Done third phase of interpreting options.\r\n[DEBUG websocat] Done fourth phase of interpreting options.\r\n"]
[421.897995, "o", "[DEBUG websocat] Preparation done. Now actually starting.\r\n"]
[421.898293, "o", "[DEBUG websocat::sessionserve] Serving AsyncStdio to WsClient(\"ws://127.0.0.1:7465/net-api/v1/net/4518df86ca084b86aa9884071432a30c/tcp/192.168.0.4/22\") with Options { websocket_text_mode: false, websocket_protocol: None, websocket_reply_protocol: None, udp_oneshot_mode: false, udp_broadcast: false, udp_multicast_loop: false, udp_ttl: None, udp_join_multicast_addr: [], udp_join_multicast_iface_v4: [], udp_join_multicast_iface_v6: [], udp_reuseaddr: false, unidirectional: false, unidirectional_reverse: false, max_messages: None, max_messages_rev: None, exit_on_eof: false, oneshot: false, unlink_unix_socket: false, unix_socket_accept_from_fd: false, exec_args: [], ws_c_uri: \"ws://0.0.0.0/\", linemode_strip_newlines: false, linemode_strict: false, origin: None, custom_headers: [(\"Authorization\", [66, 101, 97, 114, 101, 114, 32, 57, 100, 51, 99, 99, 56, 49, 53, 54, 53, 51, 53, 52, 98, 100, 97, 57, 49, 55, 48, 56, 53, 101, 48, 102, 53, 98, 54, 100, 52, 101, 100])], custom_reply_headers: [], websocket_version: None, "]
[421.898428, "o", "websocket_dont_close: false, websocket_ignore_zeromsg: false, one_message: false, no_auto_linemode: false, buffer_size: 65536, broadcast_queue_len: 16, read_debt_handling: Silent, linemode_zero_terminated: false, restrict_uri: None, serve_static_files: [], exec_set_env: false, no_exit_on_zeromsg: false, reuser_send_zero_msg_on_disconnect: false, process_zero_sighup: false, process_exit_sighup: false, process_exit_on_disconnect: false, socks_destination: None, auto_socks5: None, socks5_bind_script: None, tls_domain: None, tls_insecure: false, headers_to_env: [], max_parallel_conns: None, ws_ping_interval: None, ws_ping_timeout: None, request_uri: None, request_method: None, request_headers: [], autoreconnect_delay_millis: 20, ws_text_prefix: None, ws_binary_prefix: None, ws_binary_base64: false, ws_text_base64: false, close_status_code: None, close_reason: None, asyncstdio: true, foreachmsg_wait_reads: false, announce_listens: false, timestamp_monotonic: false, print_ping_rtts: false, byte_to_exit_on: 28, max_"]
[421.898501, "o", "ws_message_length: 209715200, max_ws_frame_length: 104857600, preamble: [], preamble_reverse: [], compress_deflate: false, compress_zlib: false, compress_gzip: false, uncompress_deflate: false, uncompress_zlib: false, uncompress_gzip: false, jsonrpc_omit_jsonrpc: false, inhibit_pongs: None, max_sent_pings: None, lengthprefixed_header_bytes: 4, lengthprefixed_little_endian: false, lengthprefixed_skip_read_direction: false, lengthprefixed_skip_write_direction: false }\r\n[DEBUG websocat::stdio_peer] get_stdio_peer (async)\r\n[DEBUG websocat::stdio_peer] Setting stdin to nonblocking mode\r\n[DEBUG websocat::stdio_peer] Setting stdout to nonblocking mode\r\n[DEBUG websocat::stdio_peer] Installing signal handler\r\n[INFO  websocat::ws_client_peer] get_ws_client_peer\r\n"]
[421.912021, "o", "[INFO  websocat::net_peer] Connected to TCP 127.0.0.1:7465\r\n"]
[422.000835, "o", "[INFO  websocat::ws_client_peer] Connected to ws\r\n"]
[422.10106, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n[DEBUG websocat::readdebt] Fulfilling the debt of 40 bytes\r\n"]
[422.19429, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n[DEBUG websocat::readdebt] Fulfilling the debt of 1112 bytes\r\n"]
[422.293193, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n[DEBUG websocat::readdebt] Fulfilling the debt of 524 bytes\r\n"]
[422.293488, "o", "Warning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\n"]
[422.422895, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n"]
[422.520937, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n"]
[422.612622, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n[DEBUG websocat::readdebt] Fulfilling the debt of 588 bytes\r\n"]
[422.728415, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n[DEBUG websocat::readdebt] Fulfilling the debt of 28 bytes\r\n"]
[422.821004, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 908 bytes\r\n\u001b[100C"]
[423.096594, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C"]
[423.185883, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[109C[DEBUG websocat::readdebt] Fulfilling the debt of 36 bytes\r\n\u001b[36C"]
[423.226864, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[77C[DEBUG websocat::readdebt] Fulfilling the debt of 72 bytes\r\n\u001b[4C"]
[427.001793, "o", "[INFO  websocat::ws_peer] Received WebSocket ping\r\n\u001b[53C"]
[427.209396, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[94C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[22CLooking in indexes: https://pypi.dev.golem.network/simple\r\n"]
[427.251139, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 132 bytes\r\n\u001b[100CRequirement already satisfied: ray[serve]==2.9.3 in ./venv/lib/python3.10/site-packages (2.9.3)\r\n"]
[427.278621, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 156 bytes\r\n\u001b[100CRequirement already satisfied: jsonschema in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (4.22.0)\r\n"]
[427.278899, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 156 bytes\r\n\u001b[100CRequirement already satisfied: click>=7.0 in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (8.1.7)\r\n"]
[427.279522, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 156 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 156 bytes\r\n\u001b[69C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[110C[DEBUG websocat::readdebt] Fulfilling the debt of 156 bytes\r\n\u001b[38CRequirement already satisfied: frozenlist in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (1.4.1)\r\nRequirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from ray[serve]==2.9.3) (2.32.3)\r\n"]
[427.279767, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 172 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 156 bytes\r\n\u001b[69C"]
[427.280328, "o", "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from ray[serve]==2.9.3) (24.0)\r\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (5.27.0)\r\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from ray[serve]==2.9.3) (3.14.0)\r\n"]
[427.281071, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 164 bytes\r\n\u001b[100CRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/site-packages (from ray[serve]==2.9.3) (1.0.8)\r\n"]
[427.29939, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 1180 bytes\r\n\u001b[101C"]
[427.299687, "o", "Requirement already satisfied: aiosignal in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (1.3.1)\r\nRequirement already satisfied: pyyaml in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (6.0.1)\r\nRequirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /usr/local/lib/python3.10/site-packages (from ray[serve]==2.9.3) (20.26.2)\r\nRequirement already satisfied: grpcio>=1.42.0 in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (1.64.1)\r\nRequirement already satisfied: gpustat>=1.0.0 in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (1.1.1)\r\nRequirement already satisfied: aiohttp-cors in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (0.7.0)\r\nRequirement already satisfied: smart-open in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (7.0.4)\r\n[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 68 bytes\r\n\u001b[99C"]
[427.30039, "o", "Requirement already satisfied: colorful in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (0.5.6)\r\n"]
[431.001373, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 60 bytes\r\n\u001b[99CCollecting watchfiles\r\n"]
[431.001871, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 284 bytes\r\n\u001b[100C  Obtaining dependency information for watchfiles from https://pypi.dev.golem.network/packages/3d/ae/e7eddbdca559f14a9a38cf04782a5d715cf350aad498d0862fb02b4ebe10/watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n"]
[431.058134, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 244 bytes\r\n\u001b[100C"]
[431.058437, "o", "  Downloading https://pypi.dev.golem.network/packages/3d/ae/e7eddbdca559f14a9a38cf04782a5d715cf350aad498d0862fb02b4ebe10/watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C"]
[431.058498, "o", "[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[68C\u001b[?25l     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[431.061721, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n  [DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[61C\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m4.9 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[431.261916, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 68 bytes\r\n\u001b[99CCollecting uvicorn[standard]\r\n"]
[431.263767, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 252 bytes\r\n\u001b[100C"]
[431.264041, "o", "  Obtaining dependency information for uvicorn[standard] from https://pypi.dev.golem.network/packages/b2/f9/e6f30ba6094733e4f9794fd098ca0543a19b07ac1fa3075d595bf0f1fb60/uvicorn-0.30.1-py3-none-any.whl.metadata\r\n"]
[431.275921, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 204 bytes\r\n\u001b[100C"]
[431.276204, "o", "  Downloading https://pypi.dev.golem.network/packages/b2/f9/e6f30ba6094733e4f9794fd098ca0543a19b07ac1fa3075d595bf0f1fb60/uvicorn-0.30.1-py3-none-any.whl.metadata\r\n"]
[431.276469, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C\u001b[?25l"]
[431.276862, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[68C     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[431.278485, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n  [DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[61C\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m6.3 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[431.280198, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 156 bytes\r\n\u001b[100CRequirement already satisfied: py-spy>=0.2.0 in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (0.3.14)\r\n"]
[431.51081, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 68 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 252 bytes\r\n\u001b[68CCollecting fastapi<=0.108.0\r\n  Obtaining dependency information for fastapi<=0.108.0 from https://pypi.dev.golem.network/packages/d4/e0/d5d6482e992a1892f3a9a62f6a9154944ae5b276e7da1cf92faa02e3a107/fastapi-0.108.0-py3-none-any.whl.metadata\r\n"]
[431.541262, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 204 bytes\r\n\u001b[100C  Downloading https://pypi.dev.golem.network/packages/d4/e0/d5d6482e992a1892f3a9a62f6a9154944ae5b276e7da1cf92faa02e3a107/fastapi-0.108.0-py3-none-any.whl.metadata\r\n"]
[431.541558, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C\u001b[?25l"]
[431.542456, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[68C     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[431.543799, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n  [DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[61C"]
[431.544043, "o", "\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m24.9 kB\u001b[39m \u001b[31m293.3 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[431.716744, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 60 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 244 bytes\r\n\u001b[68CCollecting starlette\r\n  Obtaining dependency information for starlette from https://pypi.dev.golem.network/packages/fd/18/31fa32ed6c68ba66220204ef0be798c349d0a20c1901f9d4a794e08c76d8/starlette-0.37.2-py3-none-any.whl.metadata\r\n"]
[431.752412, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 204 bytes\r\n\u001b[100C  Downloading https://pypi.dev.golem.network/packages/fd/18/31fa32ed6c68ba66220204ef0be798c349d0a20c1901f9d4a794e08c76d8/starlette-0.37.2-py3-none-any.whl.metadata\b\n"]
[431.75272, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C\u001b[?25l"]
[431.752951, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[68C     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[431.753229, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n  [DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[61C"]
[431.753433, "o", "\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m5.9 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[431.753646, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 156 bytes\r\n\u001b[100C"]
[431.753812, "o", "Requirement already satisfied: opencensus in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (0.11.4)\r\n"]
[431.75451, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 164 bytes\r\n\u001b[100CRequirement already satisfied: prometheus-client>=0.7.1 in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (0.20.0)\r\n"]
[431.986285, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 60 bytes\r\n\u001b[99C"]
[431.986366, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 244 bytes\r\n\u001b[68CCollecting aiorwlock\r\n  Obtaining dependency information for aiorwlock from https://pypi.dev.golem.network/packages/0a/f8/8d22bbdb42ccd71716fb6fa8c8e10f05fc0964787a9e0a54b975d8c0fd73/aiorwlock-1.4.0-py3-none-any.whl.metadata\r\n"]
[432.001926, "o", "[INFO  websocat::ws_peer] Received WebSocket ping\r\n\u001b[49C"]
[432.040432, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[90C[DEBUG websocat::readdebt] Fulfilling the debt of 204 bytes\r\n\u001b[18C  Downloading https://pypi.dev.golem.network/packages/0a/f8/8d22bbdb42ccd71716fb6fa8c8e10f05fc0964787a9e0a54b975d8c0fd73/aiorwlock-1.4.0-py3-none-any.whl.metadata\r\n"]
[432.082712, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[68C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[109C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[37C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[78C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[6C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[47C[DEBUG websocat::readdebt] Fulfilling the debt of 156 bytes\r\n\u001b[106C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[16C[DEBUG websocat::readdebt] Fulfilling the debt of 180 bytes\r\n\u001b[75C"]
[432.083382, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[116C[DEBUG websocat::readdebt] Fulfilling the debt of 172 bytes\r\n\u001b[44C\u001b[?25l     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m5.6 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25hRequirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (1.10.15)\r\n[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 172 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 172 bytes\r\n\u001b[69CRequirement already satisfied: aiohttp>=3.7 in ./venv/lib/python3.10/site-packages (from ray[serve]==2.9.3) (3.9.5)\r\nRequirement already satisfied: async-timeout<5.0,>=4.0 in ./venv/lib/python3.10/site-packages (from aiohttp>=3.7->ray[serve]==2.9.3) (4.0.3)\r\nRequirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp>=3.7->ray[serve"]
[432.083461, "o", "]==2.9.3) (23.2.0)\r\nRequirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp>=3.7->ray[serve]==2.9.3) (6.0.5)\r\nRequirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp>=3.7->ray[serve]==2.9.3) (1.9.4)\r\n"]
[432.175177, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 60 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 252 bytes\r\n\u001b[68CCollecting starlette\r\n  Obtaining dependency information for starlette from https://pypi.dev.golem.network/packages/40/9e/6bfa6be40034fa04cc50e2a81d24a4e5b89279c688b51380d70ac31c0556/starlette-0.32.0.post1-py3-none-any.whl.metadata\r\n"]
[432.228966, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 212 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C[DEBUG websocat::ws_peer] in"]
[432.229233, "o", "coming binary\r\n\u001b[109C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[37C  Downloading https://pypi.dev.golem.network/packages/40/9e/6bfa6be40034fa04cc50e2a81d24a4e5b89279c688b51380d70ac31c0556/starlette-0.32.0.post1-py3-none-any.whl.metadata\r\n\u001b[?25l     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[432.229489, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[65C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[124C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[34C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[93C\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m5.8 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.10/site-packages (from fastapi<=0.108.0->ray[serve]==2.9.3) (4.12.1)\r\n"]
[432.243553, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100CRequirement already satisfied: nvidia-ml-py>=11.450.129 in ./venv/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[serve]==2.9.3) (12.555.43)\r\n"]
[432.243814, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 172 bytes\r\n\u001b[100C"]
[432.243998, "o", "Requirement already satisfied: psutil>=5.6.0 in ./venv/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[serve]==2.9.3) (5.9.8)\r\n"]
[432.244904, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 172 bytes\r\n\u001b[100CRequirement already satisfied: blessed>=1.17.1 in ./venv/lib/python3.10/site-packages (from gpustat>=1.0.0->ray[serve]==2.9.3) (1.20.0)\r\n"]
[432.433164, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 68 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 244 bytes\r\n\u001b[68C"]
[432.433441, "o", "Collecting anyio<5,>=3.4.0\r\n  Obtaining dependency information for anyio<5,>=3.4.0 from https://pypi.dev.golem.network/packages/7b/a2/10639a79341f6c019dedc95bd48a4928eed9f1d1197f4c04f546fc7ae0ff/anyio-4.4.0-py3-none-any.whl.metadata\r\n"]
[432.481436, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C  Downloading https://pypi.dev.golem.network/packages/7b/a2/10639a79341f6c019dedc95bd48a4928eed9f1d1197f4c04f546fc7ae0ff/anyio-4.4.0-py3-none-any.whl.metadata\r\n\u001b[?25l"]
[432.481834, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[100C     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[432.482032, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[34C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[93C\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m4.6 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[432.501138, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100CRequirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[serve]==2.9.3) (4.2.2)\r\n"]
[432.502293, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100CRequirement already satisfied: distlib<1,>=0.3.7 in /usr/local/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[serve]==2.9.3) (0.3.8)\r\n"]
[432.543351, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 172 bytes\r\n\u001b[100CRequirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.10/site-packages (from jsonschema->ray[serve]==2.9.3) (0.18.1)\r\n"]
[432.543781, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 172 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[69CRequirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.10/site-packages (from jsonschema->ray[serve]==2.9.3) (0.35.1)\r\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.10/site-packages (from jsonschema->ray[serve]==2.9.3) (2023.12.1)\r\n"]
[432.551522, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 164 bytes\r\n\u001b[100CRequirement already satisfied: six~=1.16 in ./venv/lib/python3.10/site-packages (from opencensus->ray[serve]==2.9.3) (1.16.0)\r\n"]
[432.552407, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 180 bytes\r\n\u001b[100CRequirement already satisfied: google-api-core<3.0.0,>=1.0.0 in ./venv/lib/python3.10/site-packages (from opencensus->ray[serve]==2.9.3) (2.8.0)\r\n"]
[432.552905, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 180 bytes\r\n\u001b[100CRequirement already satisfied: opencensus-context>=0.1.3 in ./venv/lib/python3.10/site-packages (from opencensus->ray[serve]==2.9.3) (0.1.3)\r\n"]
[432.568042, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 180 bytes\r\n\u001b[100C"]
[432.568131, "o", "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->ray[serve]==2.9.3) (3.3.2)\r\n"]
[432.568405, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 164 bytes\r\n\u001b[100C"]
[432.568635, "o", "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->ray[serve]==2.9.3) (3.7)\r\n"]
[432.568831, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 172 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 172 bytes\r\n\u001b[69C"]
[432.569035, "o", "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests->ray[serve]==2.9.3) (2020.12.5)\r\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->ray[serve]==2.9.3) (2.2.1)\r\n"]
[432.596091, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 164 bytes\r\n\u001b[100CRequirement already satisfied: wrapt in ./venv/lib/python3.10/site-packages (from smart-open->ray[serve]==2.9.3) (1.16.0)\r\n"]
[432.751472, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 60 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 236 bytes\r\n\u001b[68CCollecting h11>=0.8\r\n  Obtaining dependency information for h11>=0.8 from https://pypi.dev.golem.network/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\r\n"]
[432.801646, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100C"]
[432.801954, "o", "  Downloading https://pypi.dev.golem.network/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl.metadata\r\n"]
[432.802196, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[68C\u001b[?25l     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[432.802372, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n  [DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[61C\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m8.2 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[432.955356, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 68 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 252 bytes\r\n\u001b[68C"]
[432.955436, "o", "Collecting python-dotenv>=0.13\r\n  Obtaining dependency information for python-dotenv>=0.13 from https://pypi.dev.golem.network/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl.metadata\r\n"]
[433.01163, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 204 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C  Downloading https://pypi.dev.golem.network/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl.metadata\r\n\u001b[?25l"]
[433.012112, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[100C"]
[433.012353, "o", "     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[433.012774, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[34C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[93C\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m23.2 kB\u001b[39m \u001b[31m359.8 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[433.223783, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 84 bytes\r\n\u001b[99CCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\r\n"]
[433.224268, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 308 bytes\r\n\u001b[100C"]
[433.224497, "o", "  Obtaining dependency information for uvloop!=0.15.0,!=0.15.1,>=0.14.0 from https://pypi.dev.golem.network/packages/ab/ed/12729fba5e3b7e02ee70b3ea230b88e60a50375cf63300db22607694d2f0/uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n"]
[433.275395, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 244 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C  Downloading https://pypi.dev.golem.network/packages/ab/ed/12729fba5e3b7e02ee70b3ea230b88e60a50375cf63300db22607694d2f0/uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n\u001b[?25l"]
[433.275742, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[100C     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[433.276695, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[34C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[93C"]
[433.276767, "o", "\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m4.9 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[433.585943, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 68 bytes\r\n\u001b[99C"]
[433.586246, "o", "Collecting websockets>=10.4\r\n[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 332 bytes\r\n\u001b[100C  Obtaining dependency information for websockets>=10.4 from https://pypi.dev.golem.network/packages/9a/12/c7a7504f5bf74d6ee0533f6fc7d30d8f4b79420ab179d1df2484b07602eb/websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n"]
[433.637968, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 284 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C"]
[433.638092, "o", "  Downloading https://pypi.dev.golem.network/packages/9a/12/c7a7504f5bf74d6ee0533f6fc7d30d8f4b79420ab179d1df2484b07602eb/websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n\u001b[?25l"]
[433.638752, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[100C     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[433.640488, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[34C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[93C\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m6.6 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[433.817256, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 68 bytes\r\n\u001b[99CCollecting httptools>=0.5.0\r\n"]
[433.817596, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 332 bytes\r\n\u001b[100C"]
[433.817896, "o", "  Obtaining dependency information for httptools>=0.5.0 from https://pypi.dev.golem.network/packages/65/e7/dd5ba95c84047118a363f0755ad78e639e0529be92424bb020496578aa3b/httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n"]
[433.867591, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 284 bytes\r\n\u001b[100C  Downloading https://pypi.dev.golem.network/packages/65/e7/dd5ba95c84047118a363f0755ad78e639e0529be92424bb020496578aa3b/httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n"]
[433.867885, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C\u001b[?25l[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[68C     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[433.868415, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n  [DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[61C\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m3.6 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[434.030617, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 60 bytes\r\n\u001b[99CCollecting sniffio>=1.1\r\n"]
[434.03202, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 244 bytes\r\n\u001b[100C  Obtaining dependency information for sniffio>=1.1 from https://pypi.dev.golem.network/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata\r\n"]
[434.084082, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[109C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[37C  Downloading https://pypi.dev.golem.network/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl.metadata\r\n\u001b[?25l     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[434.085229, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[65C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[124C\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m3.9 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[434.233976, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 68 bytes\r\n\u001b[99CCollecting exceptiongroup>=1.0.2\r\n"]
[434.242262, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 260 bytes\r\n\u001b[100C"]
[434.24257, "o", "  Obtaining dependency information for exceptiongroup>=1.0.2 from https://pypi.dev.golem.network/packages/01/90/79fe92dd413a9cab314ef5c591b5aa9b9ba787ae4cadab75055b0ae00b33/exceptiongroup-1.2.1-py3-none-any.whl.metadata\r\n"]
[434.285471, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 204 bytes\r\n\u001b[100C  Downloading https://pypi.dev.golem.network/packages/01/90/79fe92dd413a9cab314ef5c591b5aa9b9ba787ae4cadab75055b0ae00b33/exceptiongroup-1.2.1-py3-none-any.whl.metadata\r\n"]
[434.285801, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 100 bytes\r\n\u001b[68C"]
[434.285975, "o", "\u001b[?25l[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[109C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[37C     \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\u001b[K     \u001b[32m-\u001b[39m \u001b[32m6.6 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[434.336904, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100CRequirement already satisfied: wcwidth>=0.1.4 in ./venv/lib/python3.10/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[serve]==2.9.3) (0.2.13)\r\n"]
[434.355824, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 228 bytes\r\n\u001b[100CRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in ./venv/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]==2.9.3) (1.63.1)\r\n"]
[434.356904, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 212 bytes\r\n\u001b[100CRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in ./venv/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]==2.9.3) (2.29.0)\r\n"]
[434.499217, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 228 bytes\r\n\u001b[100C"]
[434.499332, "o", "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]==2.9.3) (4.9)\r\n"]
[434.499766, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 236 bytes\r\n\u001b[100C"]
[434.500001, "o", "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]==2.9.3) (0.4.0)\r\n"]
[434.500327, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 236 bytes\r\n\u001b[100CRequirement already satisfied: cachetools<6.0,>=2.0.0 in ./venv/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]==2.9.3) (5.3.3)\r\n"]
[434.5403, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 260 bytes\r\n\u001b[100CRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in ./venv/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]==2.9.3) (0.6.0)\r\n"]
[434.61031, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68CDownloading https://pypi.dev.golem.network/packages/d4/e0/d5d6482e992a1892f3a9a62f6a9154944ae5b276e7da1cf92faa02e3a107/fastapi-0.108.0-py3-none-any.whl\r\n\u001b[?25l"]
[434.611171, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[68C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m92.0 kB\u001b[39m \u001b[31m275.2 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[434.663352, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C"]
[434.663567, "o", "Downloading https://pypi.dev.golem.network/packages/40/9e/6bfa6be40034fa04cc50e2a81d24a4e5b89279c688b51380d70ac31c0556/starlette-0.32.0.post1-py3-none-any.whl\r\n\u001b[?25l[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[434.66451, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[90C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m70.0 kB\u001b[39m \u001b[31m363.2 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[434.716408, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68CDownloading https://pypi.dev.golem.network/packages/0a/f8/8d22bbdb42ccd71716fb6fa8c8e10f05fc0964787a9e0a54b975d8c0fd73/aiorwlock-1.4.0-py3-none-any.whl\r\n\u001b[?25l[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[434.71763, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[90C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m10.0 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[434.770849, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 236 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68CDownloading https://pypi.dev.golem.network/packages/3d/ae/e7eddbdca559f14a9a38cf04782a5d715cf350aad498d0862fb02b4ebe10/watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\b\n\u001b[?25l[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[434.806544, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[90C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m818.8 kB\u001b[39m \u001b[31m43.8 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[434.824524, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[72C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[130C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m1.2 MB\u001b[39m \u001b[31m24.9 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[434.875547, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68CDownloading https://pypi.dev.golem.network/packages/7b/a2/10639a79341f6c019dedc95bd48a4928eed9f1d1197f4c04f546fc7ae0ff/anyio-4.4.0-py3-none-any.whl\r\n\u001b[?25l"]
[434.876427, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[68C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m86.8 kB\u001b[39m \u001b[31m359.8 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[434.926987, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100CDownloading https://pypi.dev.golem.network/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl\r\n"]
[434.927078, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[67C\u001b[?25l   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[434.928195, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[130C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[58C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m58.3 kB\u001b[39m \u001b[31m360.8 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[434.983209, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 268 bytes\r\n\u001b[100C"]
[434.983368, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[109C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[36C"]
[434.983608, "o", "Downloading https://pypi.dev.golem.network/packages/65/e7/dd5ba95c84047118a363f0755ad78e639e0529be92424bb020496578aa3b/httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n\u001b[?25l   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[434.991268, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[63C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[122C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m341.4 kB\u001b[39m \u001b[31m53.7 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.042052, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100C"]
[435.042348, "o", "Downloading https://pypi.dev.golem.network/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl\r\n"]
[435.04259, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[67C"]
[435.042782, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[108C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[36C\u001b[?25l   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m19.9 kB\u001b[39m \u001b[31m204.3 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.095324, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 228 bytes\r\n\u001b[100C"]
[435.095404, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68CDownloading https://pypi.dev.golem.network/packages/ab/ed/12729fba5e3b7e02ee70b3ea230b88e60a50375cf63300db22607694d2f0/uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n\u001b[?25l"]
[435.096338, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.132422, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[90C"]
[435.132682, "o", "\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m1.3 MB\u001b[39m \u001b[31m43.7 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.170249, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[70C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[129C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m2.5 MB\u001b[39m \u001b[31m38.2 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.186208, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[70C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[129C\r\u001b[K   \u001b[32m\\\u001b[39m \u001b[32m3.4 MB\u001b[39m \u001b[31m40.2 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.237739, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 268 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68CDownloading https://pypi.dev.golem.network/packages/9a/12/c7a7504f5bf74d6ee0533f6fc7d30d8f4b79420ab179d1df2484b07602eb/websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n\u001b[?25l"]
[435.24032, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C"]
[435.240386, "o", "   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.241722, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[90C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m130.2 kB\u001b[39m \u001b[31m81.3 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.296564, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C"]
[435.296845, "o", "Downloading https://pypi.dev.golem.network/packages/b2/f9/e6f30ba6094733e4f9794fd098ca0543a19b07ac1fa3075d595bf0f1fb60/uvicorn-0.30.1-py3-none-any.whl\r\n\u001b[?25l[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C"]
[435.296917, "o", "   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.297192, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C"]
[435.29764, "o", "[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[90C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m62.4 kB\u001b[39m \u001b[31m350.7 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.346486, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100CDownloading https://pypi.dev.golem.network/packages/01/90/79fe92dd413a9cab314ef5c591b5aa9b9ba787ae4cadab75055b0ae00b33/exceptiongroup-1.2.1-py3-none-any.whl\r\n"]
[435.346874, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C\u001b[?25l"]
[435.347813, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[67C"]
[435.348001, "o", "   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.349308, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[130C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[58C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m16.5 kB\u001b[39m \u001b[31m206.4 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.401731, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C"]
[435.401822, "o", "Downloading https://pypi.dev.golem.network/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl\r\n\u001b[?25l"]
[435.402219, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.402761, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[90C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m10.2 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.452809, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100CDownloading https://pypi.dev.golem.network/packages/d4/e0/d5d6482e992a1892f3a9a62f6a9154944ae5b276e7da1cf92faa02e3a107/fastapi-0.108.0-py3-none-any.whl\r\n"]
[435.453922, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C\u001b[?25l"]
[435.45436, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[67C"]
[435.454559, "o", "   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.455291, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[130C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[58C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m92.0 kB\u001b[39m \u001b[31m179.5 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.51073, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C"]
[435.510819, "o", "Downloading https://pypi.dev.golem.network/packages/40/9e/6bfa6be40034fa04cc50e2a81d24a4e5b89279c688b51380d70ac31c0556/starlette-0.32.0.post1-py3-none-any.whl\r\n\u001b[?25l"]
[435.511224, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.511513, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[90C"]
[435.511685, "o", "\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m70.0 kB\u001b[39m \u001b[31m361.3 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.566971, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100CDownloading https://pypi.dev.golem.network/packages/0a/f8/8d22bbdb42ccd71716fb6fa8c8e10f05fc0964787a9e0a54b975d8c0fd73/aiorwlock-1.4.0-py3-none-any.whl\r\n"]
[435.567325, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C\u001b[?25l"]
[435.56768, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[67C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[108C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[36C"]
[435.567819, "o", "   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m10.0 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.624901, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 236 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68CDownloading https://pypi.dev.golem.network/packages/3d/ae/e7eddbdca559f14a9a38cf04782a5d715cf350aad498d0862fb02b4ebe10/watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\b\n\u001b[?25l"]
[435.625228, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.662112, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[90C"]
[435.662195, "o", "\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m965.0 kB\u001b[39m \u001b[31m29.0 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.667156, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[72C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[130C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m1.2 MB\u001b[39m \u001b[31m29.2 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.71948, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100CDownloading https://pypi.dev.golem.network/packages/7b/a2/10639a79341f6c019dedc95bd48a4928eed9f1d1197f4c04f546fc7ae0ff/anyio-4.4.0-py3-none-any.whl\r\n"]
[435.719952, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C\u001b[?25l"]
[435.72048, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[67C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.72218, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[130C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[58C"]
[435.722405, "o", "\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m86.8 kB\u001b[39m \u001b[31m79.0 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.774994, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100CDownloading https://pypi.dev.golem.network/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl\r\n"]
[435.776834, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C\u001b[?25l"]
[435.77749, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[67C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.777717, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[130C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[58C"]
[435.777861, "o", "\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m58.3 kB\u001b[39m \u001b[31m275.0 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.832619, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 268 bytes\r\n\u001b[100C"]
[435.832898, "o", "Downloading https://pypi.dev.golem.network/packages/65/e7/dd5ba95c84047118a363f0755ad78e639e0529be92424bb020496578aa3b/httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[67C\u001b[?25l   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.842106, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[130C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[58C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m341.4 kB\u001b[39m \u001b[31m35.1 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.896124, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100C"]
[435.896415, "o", "Downloading https://pypi.dev.golem.network/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl\r\n[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C\u001b[?25l"]
[435.896714, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[67C"]
[435.896905, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[108C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[36C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m19.9 kB\u001b[39m \u001b[31m203.2 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[435.948939, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 228 bytes\r\n\u001b[100CDownloading https://pypi.dev.golem.network/packages/ab/ed/12729fba5e3b7e02ee70b3ea230b88e60a50375cf63300db22607694d2f0/uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n"]
[435.94937, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[99C"]
[435.949713, "o", "\u001b[?25l[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[9C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[67C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[435.98684, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[130C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[58C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m1.2 MB\u001b[39m \u001b[31m39.8 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[436.022693, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[70C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[129C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m2.9 MB\u001b[39m \u001b[31m43.2 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[436.032238, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[70C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[129C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m3.4 MB\u001b[39m \u001b[31m43.8 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[436.102857, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 268 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C[DEBUG websocat::ws_peer] in"]
[436.103132, "o", "coming binary\r\n\u001b[109C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[36CDownloading https://pypi.dev.golem.network/packages/9a/12/c7a7504f5bf74d6ee0533f6fc7d30d8f4b79420ab179d1df2484b07602eb/websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n\u001b[?25l   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[436.103768, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[63C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[122C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m130.2 kB\u001b[39m \u001b[31m222.5 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[436.158048, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C"]
[436.158339, "o", "Downloading https://pypi.dev.golem.network/packages/b2/f9/e6f30ba6094733e4f9794fd098ca0543a19b07ac1fa3075d595bf0f1fb60/uvicorn-0.30.1-py3-none-any.whl\r\n\u001b[?25l[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[436.159669, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[90C\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m62.4 kB\u001b[39m \u001b[31m344.5 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[436.211644, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 196 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68C[DEBUG websocat::ws_peer] in"]
[436.21174, "o", "coming binary\r\n\u001b[109C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[36CDownloading https://pypi.dev.golem.network/packages/01/90/79fe92dd413a9cab314ef5c591b5aa9b9ba787ae4cadab75055b0ae00b33/exceptiongroup-1.2.1-py3-none-any.whl\r\n\u001b[?25l   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[436.212626, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[63C[DEBUG websocat::readdebt] Fulfilling the debt of 116 bytes\r\n\u001b[122C"]
[436.212854, "o", "\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m16.5 kB\u001b[39m \u001b[31m251.9 MB/s\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[436.26632, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 188 bytes\r\n\u001b[100C[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[10C[DEBUG websocat::readdebt] Fulfilling the debt of 44 bytes\r\n\u001b[68CDownloading https://pypi.dev.golem.network/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl\r\n\u001b[?25l"]
[436.266664, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 92 bytes\r\n\u001b[99C   \u001b[32m-\u001b[39m \u001b[32m0 bytes\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m"]
[436.266909, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[31C[DEBUG websocat::readdebt] Fulfilling the debt of 108 bytes\r\n\u001b[90C"]
[436.266995, "o", "\r\u001b[K   \u001b[32m-\u001b[39m \u001b[32m10.2 kB\u001b[39m \u001b[31m?\u001b[39m \u001b[33m0:00:00\u001b[39m\r\n\u001b[?12l\u001b[?25h"]
[436.53621, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 204 bytes\r\n\u001b[100CInstalling collected packages: websockets, uvloop, sniffio, python-dotenv, httptools, h11, exceptiongroup, aiorwlock, uvicorn, anyio, watchfiles, starlette, fastapi\r\n"]
[437.002718, "o", "[INFO  websocat::ws_peer] Received WebSocket ping\r\n\u001b[49C"]
[441.625443, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[90C[DEBUG websocat::readdebt] Fulfilling the debt of 28 bytes\r\n\u001b[17C"]
[442.001206, "o", "[INFO  websocat::ws_peer] Received WebSocket ping\r\n\u001b[66C"]
[443.455593, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[107C[DEBUG websocat::readdebt] Fulfilling the debt of 276 bytes\r\n\u001b[35CSuccessfully installed aiorwlock-1.4.0 anyio-4.4.0 exceptiongroup-1.2.1 fastapi-0.108.0 h11-0.14.0 httptools-0.6.1 python-dotenv-1.0.1 sniffio-1.3.1 starlette-0.32.0.post1 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-12.0\r\n"]
[443.900548, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 292 bytes\r\n\u001b[100C\r\n\u001b[1m[\u001b[0m\u001b[34mnotice\u001b[39m\u001b[1m]\u001b[0m A new release of pip is available: \u001b[31m23.0.1\u001b[39m -> \u001b[32m24.0\u001b[39m\r\n\u001b[1m[\u001b[0m\u001b[34mnotice\u001b[39m\u001b[1m]\u001b[0m To update, run: \u001b[32mpip install --upgrade pip\u001b[39m\r\n"]
[443.94348, "o", "[DEBUG websocat::ws_peer] incoming binary\r\n\u001b[41C[DEBUG websocat::readdebt] Fulfilling the debt of 176 bytes\r\n\u001b[100CConnection to 192.168.0.4 closed.\r\n"]
[443.947053, "o", "  \u001b[2m[5/7]\u001b[0m \u001b[36mInitializing command runner\u001b[39m\r\n  \u001b[2m[6/7]\u001b[0m No setup commands to run.\r\n"]
[443.947285, "o", "  \u001b[2m[7/7]\u001b[0m \u001b[36mStarting the Ray runtime\u001b[39m\r\n"]
[444.372219, "o", "Warning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\n"]
[451.887302, "o", "Usage stats collection is disabled.\r\n"]
[451.934372, "o", "\n\u001b[37mLocal node IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[458.54591, "o", "\n\u001b[32m--------------------\u001b[39m\r\n"]
[458.586482, "o", "\u001b[32mRay runtime started.\u001b[39m\r\n\u001b[32m--------------------\u001b[39m\r\n\n\u001b[36mNext steps\u001b[39m\r\n  To add another node to this Ray cluster, run\r\n  \u001b[1m  ray start --address='192.168.0.4:6379'\u001b[0m\r\n  \r\n  To connect to this Ray cluster:\r\n"]
[458.639547, "o", "    \u001b[35mimport\u001b[39m ray\r\n    ray\u001b[35m.\u001b[39minit(_node_ip_address\u001b[35m=\u001b[39m\u001b[33m'192.168.0.4'\u001b[39m)\r\n  \r\n  To submit a Ray job using the Ray Jobs CLI:\r\n  \u001b[1m  RAY_ADDRESS='http://192.168.0.4:8265' ray job submit --working-dir . -- python my_script.py\u001b[0m\r\n  \r\n  See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html \r\n  for more information on submitting Ray jobs to the Ray cluster.\r\n  \r\n  To terminate the Ray runtime, run\r\n  \u001b[1m  ray stop\u001b[0m\r\n  \r\n  To view the status of the cluster, use\r\n    \u001b[1mray status\u001b[0m\r\n  \r\n  To monitor and debug Ray, view the dashboard at \r\n    \u001b[1m192.168.0.4:8265\u001b[0m\r\n  \r\n  \u001b[4mIf connection to the dashboard fails, check your firewall settings and network configuration.\u001b[24m\r\n"]
[458.757671, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[458.768663, "o", "  \u001b[37mNew status\u001b[39m: \u001b[1mup-to-date\u001b[0m\r\n"]
[458.776838, "o", "\n"]
[458.777131, "o", "\u001b[36mUseful commands:\u001b[39m\r\n  To terminate the cluster:\r\n"]
[458.77733, "o", "  \u001b[1m  ray down /home/mateusz/devel/ray-on-golem/examples/ray-serve-whoami/whoami.yaml\u001b[0m\r\n  \r\n  To retrieve the IP address of the cluster head:\r\n"]
[458.777509, "o", "  \u001b[1m  ray get-head-ip /home/mateusz/devel/ray-on-golem/examples/ray-serve-whoami/whoami.yaml\u001b[0m\r\n  \r\n  To port-forward the cluster's Ray Dashboard to the local machine:\r\n"]
[458.777742, "o", "  \u001b[1m  ray dashboard /home/mateusz/devel/ray-on-golem/examples/ray-serve-whoami/whoami.yaml\u001b[0m\r\n  \r\n  To submit a job to the cluster, port-forward the Ray Dashboard in another terminal and run:\r\n"]
[458.777967, "o", "  \u001b[1m  ray job submit --address http://localhost:<dashboard-port> --working-dir . -- python my_script.py\u001b[0m\r\n  \r\n  To connect to a terminal on the cluster head for debugging:\r\n"]
[458.778258, "o", "  \u001b[1m  ray attach /home/mateusz/devel/ray-on-golem/examples/ray-serve-whoami/whoami.yaml\u001b[0m\r\n  \r\n  To monitor autoscaling:\r\n  \u001b[1m  ray exec /home/mateusz/devel/ray-on-golem/examples/ray-serve-whoami/whoami.yaml 'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'\u001b[0m\r\n  \r\n"]
[458.9185, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[471.373286, "o", "r"]
[471.421595, "o", "a"]
[471.533588, "o", "y"]
[471.645405, "o", " "]
[471.6612, "o", "w"]
[471.805381, "o", "h"]
[471.853151, "o", "o"]
[471.925533, "o", "ami-old.yaml "]
[472.125598, "o", "\b\b\b\b\b\b\b\b\b\b\u001b[4P.yaml "]
[474.266786, "o", "\b"]
[474.490465, "o", "\b"]
[474.698792, "o", "\b\b\b"]
[475.021375, "o", "\b\b\b\b\b\b\b"]
[475.677789, "o", "\u001b[1@e"]
[475.916219, "o", "\u001b[1@x"]
[476.333663, "o", "\u001b[1@e"]
[476.877795, "o", "\u001b[1@c"]
[477.098582, "o", "\u001b[1@ "]
[477.261039, "o", "whoami.yaml "]
[477.994762, "o", "'"]
[478.154808, "o", "r"]
[478.186702, "o", "a"]
[478.346726, "o", "y"]
[478.443033, "o", " "]
[478.602924, "o", "s"]
[478.780309, "o", "t"]
[478.796689, "o", "a"]
[478.909434, "o", "t"]
[479.034539, "o", "u"]
[479.10108, "o", "s"]
[479.82144, "o", "'"]
[479.885134, "o", "\r\n"]
[480.628556, "o", "2024-06-13 13:00:33,633 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[480.629065, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[480.649215, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[480.678916, "o", "  Not starting the webserver, as it's already running\r\n"]
[480.777087, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Paymen"]
[480.777192, "o", "t Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n"]
[480.777322, "o", "    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984468964815587444 GLM  │  accepted   │  0 GLM     │  0.121810146895302345 GLM  │  13.3217 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121810146895302345 GLM  │                 │\r\n    └────────────────────┴────────"]
[480.777359, "o", "────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[480.785894, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[481.18781, "o", "Warning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\n"]
[485.481207, "o", "======== Autoscaler status: 2024-06-13 11:00:20.323846 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\nPending:\r\n ray.worker.default, 2 launching\r\n None: ray.worker.default, waiting-for-ssh\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 0B/429.56MiB memory\r\n 0B/214.78MiB object_store_memory\r\n\nDemands:\r\n (no resource demands)\r\n"]
[485.581897, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[485.725574, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[496.717732, "o", "ray exec whoami.yaml 'serve run ray-serve-whoami:whoami'"]
[498.298911, "o", "\r\n"]
[499.077807, "o", "2024-06-13 13:00:52,083 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[499.078457, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[499.078551, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[499.099622, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[499.132506, "o", "  Not starting the webserver, as it's already running\r\n"]
[499.20594, "o", "\n"]
[499.20605, "o", "  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n"]
[499.206118, "o", "    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬─"]
[499.206182, "o", "───────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984468964815587444 GLM  │  accepted   │  0 GLM     │  0.121810146895302345 GLM  │  13.3217 MATIC  │\r\n    │  network: polygon  │                │                             "]
[499.206238, "o", "│  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n"]
[499.206291, "o", "    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121810146895302345 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[499.215162, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[499.615582, "o", "Warning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\n"]
[503.76895, "o", "2024-06-13 11:00:55,879 INFO scripts.py:438 -- Running import path: 'ray-serve-whoami:whoami'.\r\n"]
[503.778187, "o", "2024-06-13 11:00:55,888 INFO worker.py:1540 -- Connecting to existing Ray cluster at address: 192.168.0.4:6379...\r\n"]
[503.780914, "o", "2024-06-13 11:00:55,891 INFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m192.168.0.4:8265 \u001b[39m\u001b[0m\r\n"]
[510.287008, "o", "\u001b[36m(ProxyActor pid=1064)\u001b[39m INFO 2024-06-13 11:01:02,347 proxy 192.168.0.4 proxy.py:1143 - Proxy actor 956b65323db531bbac541c0301000000 starting on node e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d.\r\n\u001b[36m(ProxyActor pid=1064)\u001b[39m INFO 2024-06-13 11:01:02,353 proxy 192.168.0.4 proxy.py:1357 - Starting HTTP server on node: e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d listening on port 8000\r\n"]
[510.489674, "o", "\u001b[36m(ProxyActor pid=1064)\u001b[39m INFO:     Started server process [1064]\r\n"]
[510.588121, "o", "\u001b[36m(ServeController pid=1034)\u001b[39m INFO 2024-06-13 11:01:02,658 controller 1034 deployment_state.py:1547 - Deploying new version of deployment WhoamiDeployment in application 'default'. Setting initial target number of replicas to 4.\r\n"]
[510.689457, "o", "\u001b[36m(ServeController pid=1034)\u001b[39m INFO 2024-06-13 11:01:02,760 controller 1034 deployment_state.py:1831 - Adding 4 replicas to deployment WhoamiDeployment in application 'default'.\r\n"]
[528.001083, "o", "\u001b[H\u001b[2J"]
[528.017932, "o", "\u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[529.739796, "o", "\r(reverse-i-search)`': \u001b[K"]
[529.981848, "o", "\b\b\ba': source ~/.envs/ray-serve-test-0/bin/activ\u001b[7ma\u001b[0mte\b\b\b"]
[530.18991, "o", "\r(reverse-i-search)`ac': source ~/.envs/ray-serve-test-0/bin/\u001b[7mac\u001b[0mtivate\b\b\b\b\b\b\b\b"]
[531.277494, "o", "\r\u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ source ~/.envs/ray-serve-test-0/bin/activate\b\b\b\b\b\b\b\b\r\n"]
[531.278888, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[532.189909, "o", "r"]
[532.253811, "o", "a"]
[532.365767, "o", "y"]
[532.461601, "o", " "]
[533.501752, "o", "e"]
[533.709836, "o", "x"]
[533.933611, "o", "e"]
[534.091939, "o", "c"]
[534.235785, "o", " "]
[534.443306, "o", "w"]
[534.587147, "o", "h"]
[534.651305, "o", "o"]
[534.750618, "o", "ami-old.yaml "]
[534.891434, "o", "\b\b\b\b\b\b\b\b\b\b\u001b[4P.yaml "]
[535.851108, "o", "'"]
[535.94718, "o", "r"]
[536.027217, "o", "a"]
[536.155229, "o", "y"]
[536.235394, "o", " "]
[536.39529, "o", "s"]
[536.603206, "o", "t"]
[536.651045, "o", "a"]
[536.731382, "o", "t"]
[536.843106, "o", "u"]
[536.939353, "o", "s"]
[537.773826, "o", "'"]
[537.837342, "o", "\r\n"]
[538.561784, "o", "2024-06-13 13:01:31,567 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[538.56228, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[538.562349, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[538.583474, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[538.61291, "o", "  Not starting the webserver, as it's already running\r\n"]
[538.69472, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n"]
[538.694919, "o", "    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n"]
[538.695167, "o", "    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984468964815587444 GLM  │  accepted   │  0 GLM     │  0.121810146895302345 GLM  │  13.3217 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n   "]
[538.695354, "o", " │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121810146895302345 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[538.713482, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[542.573558, "o", "======== Autoscaler status: 2024-06-13 11:01:13.427318 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 0B/429.56MiB memory\r\n 44B/214.78MiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 4+ pending tasks/actors\r\n"]
[542.671861, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[542.813703, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[548.043303, "o", "\u001b[H\u001b[2Jgas\u001b[12C│\u001b[2;5H├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\u001b[4;5H│  driver: erc20\u001b[5C│  99.6491 GLM   │  24.984468964815587444 GLM  │  accepted   │  0 GLM\u001b[5C│  0.121810146895302345 GLM  │  13.3217 MATIC  │\u001b[6;5H│  network: polygon  │\u001b[16C│\u001b[29C│  confirmed  │  0 GLM\u001b[5C│  0.019622057074187837 GLM  │   \u001b[14C│\u001b[8;5H│  token: GLM\u001b[8C│\u001b[16C│\u001b[29C│  requested  │  0 GLM\u001b[5C│  0.121810146895302345 GLM  │   \u001b[14C│\u001b[12D\n└────────────────────┴────────────────┴────────────────"]
[548.043463, "o", "─────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\u001b[13;3HYou can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n\n\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\r\n\u001b[0mWarning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\n2024-06-13 11:00:55,879 INFO scripts.py:438 -- Running import path: 'ray-serve-whoami:whoami'.\r\n2024-06-13 11:00:55,888 INFO worker.py:1540 -- Connecting to existing Ray cluster at address: 192.168.0.4:6379...\r\n2024-06-13 11:00:55,891 INFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m192.168.0.4:8265 \r\n\u001b[0m\u001b[36m(ProxyActor pid=1064)\u001b[C\u001b[39mINFO 2024-06-13 11:01:02,347 proxy 192.168.0.4 proxy.py:1143 - Proxy actor 956b65323db53"]
[548.043533, "o", "1bbac541c0301000000 starting on node e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d.\r\n\u001b[36m(ProxyActor pid=1064)\u001b[C\u001b[39mINFO 2024-06-13 11:01:02,353 proxy 192.168.0.4 proxy.py:1357 - Starting HTTP server on node: e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d listening on port 8000\r\n\u001b[36m(ProxyActor pid=1064)\u001b[C\u001b[39mINFO:\u001b[5CStarted server process [1064]\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:01:02,658 controller 1034 deployment_state.py:1547 - Deploying new version of deployment WhoamiDeployment in application 'default'. Setting initial target number of replicas to 4.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:01:02,760 controller 1034 deployment_state.py:1831 - Adding 4 replicas to deployment WhoamiDeployment in application 'default'.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:01:32,806 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 4 replicas that have taken more than 30s to be sche"]
[548.043606, "o", "duled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n"]
[558.286715, "o", "\u001b[H\u001b[2J13.3217 MATIC  │\u001b[2;5H│  network: polygon  │\u001b[16C│\u001b[29C│  confirmed  │  0 GLM\u001b[5C│  0.019622057074187837 GLM  │   \u001b[14C│\u001b[4;5H│  token: GLM\u001b[8C│\u001b[16C│\u001b[29C│  requested  │  0 GLM\u001b[5C│  0.121810146895302345 GLM  │   \u001b[14C│\u001b[6;5H└────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\u001b[9;3HYou can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n\n\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\r\n\u001b[0m======== Autoscaler status: 2024-06-13 11:01:13.427318 ========\r\nNode status\r\n------------------------------"]
[558.2868, "o", "---------------------------------\r\nActive:\rA\n1 ray.head.default\r\nPending:\rP\nray.worker.default, 1 launching\r \nNone: ray.worker.default, waiting-for-ssh\r \nNone: ray.worker.default, waiting-for-ssh\r\nRecent failures:\rR\n(no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\rU\n0B/429.56MiB memory\r \n44B/214.78MiB object_store_memory\r\n\nDemands:\rD\n{'CPU': 1.0}: 4+ pending tasks/actors\r\nShared connection to 192.168.0.4 closed.\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[561.166055, "o", "ray exec whoami.yaml 'ray status'"]
[562.205856, "o", "\r\n"]
[562.950182, "o", "2024-06-13 13:01:55,955 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[562.950766, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[562.950905, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[562.970849, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[563.000473, "o", "  Not starting the webserver, as it's already running\r\n"]
[563.068104, "o", "\n"]
[563.068262, "o", "  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬─"]
[563.068453, "o", "───────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984468964815587444 GLM  │  accepted   │  0 GLM     │  0.121810146895302345 GLM  │  13.3217 MATIC  │\r\n    │  network: polygon  │                │                             "]
[563.068529, "o", "│  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121810146895302345 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[563.077612, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[566.965327, "o", "======== Autoscaler status: 2024-06-13 11:01:42.299012 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 0B/429.56MiB memory\r\n 44B/214.78MiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 4+ pending tasks/actors\r\n"]
[567.06714, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[567.1945, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[581.006186, "o", "v"]
[581.102566, "o", "i"]
[581.150056, "o", "m"]
[581.24656, "o", " "]
[582.718565, "o", "w"]
[582.798394, "o", "h"]
[582.846049, "o", "o"]
[582.943207, "o", "ami-old.yaml "]
[583.919011, "o", "\b\b\b\b\b\b\b\b\b\b\u001b[4P.yaml "]
[585.022355, "o", "\r\n"]
[585.043869, "o", "\u001b[H"]
[585.044234, "o", "\u001b[H\u001b[2J\u001b[?25l\u001b[33B\"whoami.yaml\""]
[585.044365, "o", " 135L, 4755B"]
[585.04707, "o", "\u001b[2;1H▽\b  \r\nzz           \u001b[H\u001b]11;?\u0007"]
[585.054025, "o", "\u001b[36m\u001b[96m# Ray on Golem cluster name\u001b[39m\r\n\u001b[1m\u001b[36m\u001b[96mcluster_name\u001b[0m\u001b[38;5;224m:\u001b[39m ray-serve-whoami\u001b[K\r\n\u001b[K\n\u001b[36m\u001b[96m# The maximum number of workers the cluster will have at any given time\u001b[39m\r\n\u001b[1m\u001b[36m\u001b[96mmax_workers\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m10\u001b[39m\r\n\n\u001b[36m\u001b[96m# The number of minutes that need to pass before an idle worker node is removed by the Autoscaler\u001b[39m\r\n\u001b[1m\u001b[36m\u001b[96midle_timeout_minutes\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m5\u001b[39m\r\n\n\u001b[36m\u001b[96m# The cloud provider-specific configuration properties.\u001b[39m\r\n\u001b[1m\u001b[36m\u001b[96mprovider\u001b[0m\u001b[38;5;224m:\u001b[39m\r\n  \u001b[1m\u001b[36m\u001b[96mtype\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m\"external\"\u001b[39m\r\n  \u001b[1m\u001b[36m\u001b[96muse_internal_ips\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95mtrue\u001b[39m\r\n  \u001b[1m\u001b[36m\u001b[96mmodule\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m\"ray_on_golem.provider.node_provider.GolemNodeProvider\"\u001b[39m\r\n  \u001b[1m\u001b[36m\u001b[96mparameters\u001b[0m\u001b[38;5;224m:\u001b[39m\u001b[17;4H\u001b[36m\u001b[96m # Blockchain used for payments.\r\n\u001b[39m   \u001b[36m\u001b[96m # `holesky` means running free nodes on testnet,\r\n\u001b[39m   \u001b[36m\u001b[96m "]
[585.054242, "o", "# `polygon` is for mainnet operations.\r\n\u001b[39m   \u001b[36m\u001b[96m #payment_network: \"holesky\"\u001b[39m\r\n    \u001b[1m\u001b[36m\u001b[96mpayment_network\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m\"polygon\"\u001b[39m\r   \n\n\u001b[36m\u001b[96m # Maximum amount of GLMs that's going to be spent for the whole cluster\u001b[39m\r\n    \u001b[1m\u001b[36m\u001b[96mtotal_budget\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m5\u001b[39m\r   \n\n\u001b[36m\u001b[96m # Common parameters for all node types. Can be overridden in available_node_types\u001b[39m\r\n    \u001b[1m\u001b[36m\u001b[96mnode_config\u001b[0m\u001b[38;5;224m:\u001b[39m\u001b[11D\n\u001b[36m\u001b[96m # Parameters for golem demands (same for head and workers)\u001b[39m\u001b[58D\n\u001b[1m\u001b[36m\u001b[96mdemand\u001b[0m\u001b[38;5;224m:\u001b[39m\u001b[6D\n\u001b[36m\u001b[96m # Check available versions at https://registry.golem.network/explore/golem/ray-on-golem\u001b[88D\n #image_tag: \"golem/ray-on-golem:0.11.0-py3.10.13-ray2.9.3\"\u001b[39m\u001b[58D\n\u001b[1m\u001b[36m\u001b[96mimage_tag\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m\"blueshade/ray-on-golem:0.11.0a2-py3.10.13-ray2.9.3\"\u001b[39m\u001b[42C\n\n1,1\u001b[11CTop\u001b[H\u001b[?12l\u001b[?25h"]
[589.422771, "o", "\u001b[?25l\u001b[33B\u001b[K/\u001b[?12l\u001b[?25h"]
[589.982726, "o", "C\u001b[?25l"]
[590.002064, "o", "\u001b[26;7H\u001b[7m\u001b[36m\u001b[96mC\u001b[0m\u001b[37C\u001b[30m\u001b[43m\u001b[103mC\u001b[30;11HC\u001b[39m\u001b[49m\u001b[34;114H26,7\u001b[10CTop\u001b[17D\u001b[K\r/C\u001b[?12l\u001b[?25h"]
[590.126433, "o", "P\u001b[?25l"]
[590.143708, "o", "\u001b[H\u001b[H\u001b[2J\u001b[9C\u001b[36m\u001b[96m # Per cpu expected cost is calculated as a sum of:\u001b[51D\n # - start_price / cpu_count\u001b[28D\n # - env_per_hour_price * duration_hours / cpu_count\u001b[52D\n # - cpu_per_hour_price * duration_hours * cpu_load \u001b[6;10H # Estimated expected load and duration for worker that tells budget control to pick the least expensive Golem provider of\u001b[7;10H # If not provided, offers will be picked at random.\u001b[39m\u001b[51D\n\u001b[1m\u001b[36m\u001b[96mcpu_load\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m0.8\u001b[39m\u001b[13D\n\u001b[1m\u001b[36m\u001b[96mduration_hours\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m0.5\u001b[39m\u001b[36m\u001b[96m # 30 minutes\u001b[33D\n\n # Amount of GLMs for expected usage which Golem provider offer will be rejected if exceeded.\u001b[39m\u001b[92D\n\u001b[1m\u001b[36m\u001b[96mmax_cost\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m1.5\u001b[39m\u001b[14;8H\u001b[36m\u001b[96m # Amount of GLMs for worker initiation which Golem provider offer will be rejected if exceeded.\u001b[39m\u001b[95D\n\u001b[1m\u001b[36m\u001b[96mmax_start_price\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m0.5\u001b[39m\u001b[17;8H\u001b[36m\u001b[96m # Amount of GLMs for \u001b[39m\u001b[7m\u001b[36m\u001b[96mCP\u001b[0m\u001b[36m\u001b[96mU"]
[590.143789, "o", " utilisation per hour which Golem provider offer will be rejected if exceeded.\u001b[39m\u001b[18;9H\u001b[1m\u001b[36m\u001b[96mmax_cpu_per_hour_price\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m0.5\u001b[39m\u001b[20;8H\u001b[36m\u001b[96m # Amount of GLMs for each hour that worker runs which Golem provider offer will be rejected if exceeded.\u001b[39m\u001b[21;9H\u001b[1m\u001b[36m\u001b[96mmax_env_per_hour_price\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m0.5\u001b[39m\r\n\n\n\u001b[36m\u001b[96m# Tells the autoscaler the allowed node types and the resources they provide\u001b[39m\r\n\u001b[1m\u001b[36m\u001b[96mavailable_node_types\u001b[0m\u001b[38;5;224m:\u001b[39m\r\n  \u001b[1m\u001b[36m\u001b[96mray.head.default\u001b[0m\u001b[38;5;224m:\u001b[39m\r\n    \u001b[1m\u001b[36m\u001b[96mresources\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[38;5;224m{\u001b[39m\u001b[8D\n\u001b[1m\u001b[30m\u001b[43m\u001b[103mCP\u001b[0m\u001b[1m\u001b[36m\u001b[96mU\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m0\u001b[39m\r\n    \u001b[38;5;224m}\u001b[39m\b\b\n\n\u001b[36m\u001b[96m # Additional parameters specific for this node type added on top of node_config from provider.parameters.node_config\u001b[39m\r\n    \u001b[1m\u001b[36m\u001b[96mnode_config\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[38;5;224m{}\u001b[39m\r\n   \u001b[36m\u001b[96m #node_config:\u001b[39m\u001b[96C\n65,30\u001b[9C47%\r/CP\u001b[1"]
[590.143816, "o", "10C\u001b[K\r/CP\u001b[?12l\u001b[?25h"]
[590.366371, "o", "U\u001b[?25l"]
[590.383313, "o", "\u001b[17;32H\u001b[7m\u001b[36m\u001b[96mU\u001b[0m\u001b[28;11H\u001b[1m\u001b[30m\u001b[43m\u001b[103mU\u001b[0m\u001b[38;5;224m:\u001b[39m\u001b[34;114H65,30\u001b[9C47%\u001b[17D\u001b[K\r/CPU\u001b[?12l\u001b[?25h"]
[590.990732, "o", "\r\u001b[?25l"]
[591.007131, "o", "\u001b[17;30H\u001b[30m\u001b[43m\u001b[103mCPU\u001b[39m\u001b[49m\u001b[34;114H65,30\u001b[9C47%\u001b[17;30H\u001b[?12l\u001b[?25h"]
[591.567364, "o", "\u001b[?25l\u001b[34;115H4,0-1\r\u001b[18A\u001b[?12l\u001b[?25h"]
[591.742994, "o", "\u001b[?25l\u001b[34;115H3,28 \u001b[15;28H\u001b[?12l\u001b[?25h"]
[591.982741, "o", "\u001b[?25l\u001b[34;115H4,0-1\r\u001b[18A\u001b[?12l\u001b[?25h"]
[592.482508, "o", "\u001b[?25l\u001b[34;115H5,30 \u001b[17;30H\u001b[?12l\u001b[?25h"]
[592.514308, "o", "\u001b[?25l\u001b[34;115H6\u001b[18;30H\u001b[?12l\u001b[?25h"]
[592.544443, "o", "\u001b[?25l\u001b[34;115H7,0-1\r\u001b[15A\u001b[?12l\u001b[?25h"]
[592.575737, "o", "\u001b[?25l\u001b[34;115H8,30 \u001b[20;30H\u001b[?12l\u001b[?25h"]
[592.604032, "o", "\u001b[?25l\u001b[34;115H9\u001b[21;30H\u001b[?12l\u001b[?25h"]
[592.634037, "o", "\u001b[?25l\u001b[34;114H70,0-1\r\u001b[12A\u001b[?12l\u001b[?25h"]
[592.664653, "o", "\u001b[?25l\u001b[34;115H1\r\u001b[11A\u001b[?12l\u001b[?25h"]
[592.694822, "o", "\u001b[?25l\u001b[34;115H2,30 \u001b[24;30H\u001b[?12l\u001b[?25h"]
[592.725166, "o", "\u001b[?25l\u001b[34;115H3,21\u001b[25;21H\u001b[?12l\u001b[?25h"]
[592.75641, "o", "\u001b[?25l\u001b[34;115H4,19\u001b[26;19H\u001b[?12l\u001b[?25h"]
[592.791556, "o", "\u001b[?25l\b\b\b\n\u001b[38;5;224m\u001b[46m{\u001b[29;5H}\u001b[39m\u001b[49m\u001b[34;115H5,16\u001b[27;16H\u001b[?12l\u001b[?25h"]
[593.087298, "o", "\u001b[?25l\u001b[38;5;224m{\u001b[29;5H}\u001b[39m\u001b[34;115H4,19\u001b[26;19H\u001b[?12l\u001b[?25h"]
[593.262941, "o", "\u001b[?25l\u001b[34;115H3,21\u001b[25;21H\u001b[?12l\u001b[?25h"]
[593.423199, "o", "\u001b[?25l\u001b[34;115H2,30\u001b[24;30H\u001b[?12l\u001b[?25h"]
[594.174677, "o", "\u001b[?25l\r\u001b[36m\u001b[96m\u001b[48;5;242m# Tells the autoscaler the al\u001b[Cowed node types and the resources they provide\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\r\u001b[10B\u001b[1m-- VISUAL LINE --\u001b[0m\u001b[96C\u001b[K72,30\u001b[9C47%\u001b[24;30H\u001b[?12l\u001b[?25h"]
[594.544595, "o", "\u001b[?25l\u001b[36m\u001b[96m\u001b[48;5;242ml\u001b[39m\u001b[49m\r\n\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mavailable_node_types\u001b[0m\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\u001b[34;115H3,22\u001b[25;22H\u001b[?12l\u001b[?25h"]
[595.468847, "o", "\u001b[?25l\r\u001b[A\u001b[36m\u001b[96m# Tells the autoscaler the allowed node types and the resources they provide\u001b[39m\u001b[K\r\n\u001b[1m\u001b[36m\u001b[96mavailable_node_types\u001b[0m\u001b[38;5;224m:\u001b[39m\r\u001b[9B\u001b[K\u001b[113C73,21\u001b[9C47%\u001b[25;21H\u001b[?12l\u001b[?25h"]
[595.61956, "o", "\u001b[?5h\u001b[?5l\u001b[?25l\u001b[34;115H4,19\u001b[26;19H\u001b[?12l\u001b[?25h"]
[595.747621, "o", "\u001b[?25l\b\b\b\n\u001b[38;5;224m\u001b[46m{\u001b[29;5H}\u001b[39m\u001b[49m\u001b[34;115H5,16\u001b[27;16H\u001b[?12l\u001b[?25h"]
[596.399558, "o", "\u001b[?25l\r\u001b[48;5;242m    \u001b[49m\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mresources\u001b[0m\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[C \u001b[49m\r\u001b[7B\u001b[1m-- VISUAL LINE --\u001b[0m\u001b[96C\u001b[K75,16\u001b[9C47%\u001b[27;16H\u001b[?12l\u001b[?25h"]
[598.337636, "o", "\u001b[?25l\r    \u001b[1m\u001b[36m\u001b[96mresources\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[C\u001b[K\r\u001b[7B\u001b[K\u001b[113C75,16\u001b[9C47%\u001b[27;16H\u001b[?12l\u001b[?25h\u001b[?5h\u001b[?5l\u001b[?25l\u001b[38;5;224m{\u001b[29;5H}\u001b[39m\u001b[34;115H4\u001b[26;16H\u001b[?12l\u001b[?25h"]
[599.935961, "o", "\u001b[?25l\r\u001b[48;5;242m  \u001b[49m\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mray.head.defa\u001b[0m\u001b[1m\u001b[36m\u001b[96mu\u001b[0m\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mlt\u001b[0m\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\r\u001b[8B\u001b[1m-- VISUAL LINE --\u001b[0m\u001b[96C\u001b[K74,16\u001b[9C47%\u001b[26;16H\u001b[?12l\u001b[?25h"]
[600.260144, "o", "\u001b[?25l\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mult\u001b[0m\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\r\n\u001b[48;5;242m    \u001b[49m\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mresources\u001b[0m\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\u001b[38;5;224m\u001b[46m{\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\u001b[29;5H\u001b[38;5;224m\u001b[46m}\u001b[39m\u001b[49m\u001b[34;115H5\u001b[27;16H\u001b[?12l\u001b[?25h"]
[600.449771, "o", "\u001b[?25l\u001b[38;5;224m\u001b[48;5;242m{\u001b[39m\u001b[49m\r\n\u001b[48;5;242m        \u001b[49m\u001b[1m\u001b[30m\u001b[48;5;242mCPU\u001b[0m\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\u001b[35m\u001b[95m\u001b[48;5;242m0\u001b[39m\u001b[49m\r\n    \u001b[38;5;224m}\u001b[39m\u001b[34;115H6,15\u001b[28;15H\u001b[?12l\u001b[?25h"]
[600.608605, "o", "\u001b[?25l\u001b[48;5;242m \r\n    \u001b[49m\u001b[38;5;224m\u001b[48;5;242m}\u001b[39m\u001b[49m\u001b[34;115H7,6 \u001b[29;6H\u001b[?12l\u001b[?25h"]
[601.071254, "o", "\u001b[?25l\u001b[48;5;242m \u001b[49m\u001b[34;115H8,0-1\r\u001b[4A\u001b[?12l\u001b[?25h"]
[601.791825, "o", "\u001b[?25l\u001b[29;6H\u001b[K\u001b[34;115H7,6  \u001b[29;6H\u001b[?12l\u001b[?25h"]
[601.98505, "o", "\u001b[?25l\u001b[9C\u001b[A\u001b[K\r\n    \u001b[38;5;224m}\u001b[39m\u001b[34;115H6,15\u001b[28;15H\u001b[?12l\u001b[?25h"]
[602.16319, "o", "\u001b[?25l \u001b[A\u001b[38;5;224m\u001b[46m{\u001b[39m\u001b[49m\r\n        \u001b[1m\u001b[30m\u001b[43m\u001b[103mCPU\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m0\u001b[39m\r\n    \u001b[38;5;224m\u001b[46m}\u001b[39m\u001b[49m\u001b[34;115H5,16\u001b[27;16H\u001b[?12l\u001b[?25h"]
[603.533035, "o", "\u001b[?25l\r\u001b[A  \u001b[1m\u001b[36m\u001b[96mray.head.default\u001b[0m\u001b[38;5;224m:\u001b[39m\u001b[K\r\n    \u001b[1m\u001b[36m\u001b[96mresources\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[C\u001b[K\r\u001b[7B\u001b[K\u001b[113C75,16\u001b[9C47%\u001b[27;16H\u001b[?12l\u001b[?25h"]
[604.213288, "o", "\u001b[?5h\u001b[?5l\u001b[?25l\u001b[38;5;224m{\u001b[29;5H}\u001b[39m\u001b[34;115H6,14\u001b[28;14H\u001b[?12l\u001b[?25h"]
[604.615831, "o", "\u001b[?25l\u001b[2C\u001b[A\u001b[38;5;224m\u001b[46m{\u001b[29;5H}\u001b[39m\u001b[49m\u001b[34;115H7,5 \u001b[29;5H\u001b[?12l\u001b[?25h"]
[604.644866, "o", "\u001b[?25l\u001b[27;16H\u001b[38;5;224m{\u001b[29;5H}\u001b[39m\u001b[34;115H8,0-1\r\u001b[4A\u001b[?12l\u001b[?25h"]
[604.672171, "o", "\u001b[?25l\u001b[34;115H9,16 \u001b[31;16H\u001b[?12l\u001b[?25h"]
[604.702833, "o", "\u001b[?25l\u001b[98C\n\n\n80\u001b[32;16H\u001b[?12l\u001b[?25h"]
[604.733128, "o", "\u001b[?25l\u001b[99C\n\n1\u001b[33;16H\u001b[?12l\u001b[?25h"]
[604.763624, "o", "\u001b[?25l\u001b[H\u001b[32B\u001b[1;33r\u001b[33;1H\n\u001b[H\u001b[33;4H\u001b[36m\u001b[96m #  demand:\u001b[39m\u001b[34;114H\u001b[K82,14\u001b[9C48%\u001b[33;14H\u001b[?12l\u001b[?25h"]
[604.796073, "o", "\u001b[?25l\u001b[H\u001b[32B\n\u001b[H\u001b[33;4H\u001b[36m\u001b[96m #    min_mem_gib: 10\u001b[39m\u001b[34;114H\u001b[K83,16\u001b[9C49%\u001b[33;16H\u001b[?12l\u001b[?25h"]
[604.826265, "o", "\u001b[?25l\u001b[H\u001b[32B\n\u001b[H\u001b[34;114H\u001b[K84,0-1\u001b[8C50%\r\u001b[A\u001b[?12l\u001b[?25h"]
[604.85706, "o", "\u001b[?25l\u001b[H\u001b[32B\n\u001b[H\u001b[33;3H\u001b[1m\u001b[36m\u001b[96mray.worker.default\u001b[0m\u001b[38;5;224m:\u001b[39m\u001b[34;114H\u001b[K85,16\u001b[9C50%\u001b[33;16H\u001b[?12l\u001b[?25h"]
[604.886771, "o", "\u001b[?25l\u001b[H\u001b[32B\n\u001b[H\u001b[33;5H\u001b[1m\u001b[36m\u001b[96mmin_workers\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m2\u001b[39m\u001b[34;114H\u001b[K86,16\u001b[9C51%\u001b[33;16H\u001b[?12l\u001b[?25h"]
[605.520455, "o", "\u001b[?25l\u001b[H\u001b[32B\n\u001b[H\u001b[33;5H\u001b[1m\u001b[36m\u001b[96mmax_workers\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m10\u001b[39m\u001b[34;114H\u001b[K87,16\u001b[9C52%\u001b[33;16H\u001b[?12l\u001b[?25h"]
[606.021843, "o", "\u001b[?25l\u001b[H\u001b[32B\n\u001b[H\u001b[33;5H\u001b[1m\u001b[36m\u001b[96mresources\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[38;5;224m{\u001b[39m\u001b[34;114H\u001b[K88,16\u001b[9C53%\u001b[33;16H\u001b[?12l\u001b[?25h"]
[606.051828, "o", "\u001b[?25l\u001b[H\u001b[32B\n\u001b[H\u001b[33;9H\u001b[1m\u001b[30m\u001b[43m\u001b[103mCPU\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m2\u001b[39m\u001b[34;114H\u001b[K89,14\u001b[9C54%\u001b[33;14H\u001b[?12l\u001b[?25h"]
[606.963684, "o", "\u001b[?25l\u001b[H\u001b[32B\n\u001b[H\u001b[31;16H\u001b[38;5;224m\u001b[46m{\u001b[33;5H}\u001b[39m\u001b[49m\u001b[34;114H\u001b[K90,5\u001b[10C55%\u001b[33;5H\u001b[?12l\u001b[?25h"]
[607.154102, "o", "\u001b[?25l\u001b[H\u001b[32B\n\u001b[H\u001b[30;16H\u001b[38;5;224m{\u001b[32;5H}\u001b[39m\r\n    \u001b[1m\u001b[36m\u001b[96mnode_config\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[38;5;224m{}\u001b[39m\u001b[34;114H\u001b[K91,16\u001b[9C56%\u001b[33;16H\u001b[?12l\u001b[?25h"]
[607.311643, "o", "\u001b[?25l\u001b[H\u001b[32B\n\u001b[H\u001b[34;114H\u001b[K92,0-1\u001b[8C57%\r\u001b[A\u001b[?12l\u001b[?25h"]
[607.663737, "o", "\u001b[?25l\u001b[34;115H1,16 \u001b[32;16H\u001b[?12l\u001b[?25h"]
[607.827507, "o", "\u001b[?25l\u001b[3A\u001b[38;5;224m\u001b[46m{\u001b[31;5H}\u001b[39m\u001b[49m\u001b[34;115H0,5 \u001b[31;5H\u001b[?12l\u001b[?25h"]
[607.986643, "o", "\u001b[?25l\u001b[29;16H\u001b[38;5;224m{\u001b[31;5H}\u001b[39m\u001b[34;114H89,14\u001b[30;14H\u001b[?12l\u001b[?25h"]
[608.164376, "o", "\u001b[?25l\u001b[2C\u001b[A\u001b[38;5;224m\u001b[46m{\u001b[31;5H}\u001b[39m\u001b[49m\u001b[34;115H8,16\u001b[29;16H\u001b[?12l\u001b[?25h"]
[608.322669, "o", "\u001b[?25l\u001b[38;5;224m{\u001b[31;5H}\u001b[39m\u001b[34;115H7\u001b[28;16H\u001b[?12l\u001b[?25h"]
[608.590875, "o", "\u001b[?25l\u001b[34;115H6\u001b[27;16H\u001b[?12l\u001b[?25h"]
[608.783216, "o", "\u001b[?25l\u001b[34;115H5\u001b[26;16H\u001b[?12l\u001b[?25h"]
[609.43928, "o", "\u001b[?25l\u001b[34;115H4,0-1\r\u001b[9A\u001b[?12l\u001b[?25h"]
[609.999059, "o", "\u001b[?25l\u001b[34;115H5,16 \u001b[26;16H\u001b[?12l\u001b[?25h"]
[611.536098, "o", "\u001b[?25l\r\u001b[48;5;242m  \u001b[49m\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mray.worker.de\u001b[0m\u001b[1m\u001b[36m\u001b[96mf\u001b[0m\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mault\u001b[0m\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\u001b[34;1H\u001b[1m-- VISUAL LINE --\u001b[0m\u001b[96C\u001b[K85,16\u001b[9C57%\u001b[26;16H\u001b[?12l\u001b[?25h"]
[611.9534, "o", "\u001b[?25l\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mfault\u001b[0m\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\r\n\u001b[48;5;242m    \u001b[49m\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mmin_workers\u001b[0m\u001b[38;5;224m:\u001b[39m\u001b[48;5;242m \u001b[49m\u001b[35m\u001b[95m\u001b[48;5;242m2\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\u001b[34;115H6\u001b[27;16H\u001b[?12l\u001b[?25h"]
[612.129169, "o", "\u001b[?25l\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\r\n\u001b[48;5;242m    \u001b[49m\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mmax_workers\u001b[0m\u001b[38;5;224m:\u001b[39m\u001b[48;5;242m \u001b[49m\u001b[35m\u001b[95m\u001b[48;5;242m10\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\u001b[34;115H7\u001b[28;16H\u001b[?12l\u001b[?25h"]
[612.293564, "o", "\u001b[?25l\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\r\n\u001b[48;5;242m    \u001b[49m\u001b[1m\u001b[36m\u001b[96m\u001b[48;5;242mresources\u001b[0m\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\u001b[38;5;224m\u001b[46m{\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\u001b[31;5H\u001b[38;5;224m\u001b[46m}\u001b[39m\u001b[49m\u001b[34;115H8\u001b[29;16H\u001b[?12l\u001b[?25h"]
[612.466514, "o", "\u001b[?25l\u001b[38;5;224m\u001b[48;5;242m{\u001b[39m\u001b[49m\r\n\u001b[48;5;242m        \u001b[49m\u001b[1m\u001b[30m\u001b[48;5;242mCPU\u001b[0m\u001b[38;5;224m\u001b[48;5;242m:\u001b[39m\u001b[49m\u001b[48;5;242m \u001b[49m\u001b[35m\u001b[95m\u001b[48;5;242m2\u001b[39m\u001b[49m\r\n    \u001b[38;5;224m}\u001b[39m\u001b[34;115H9,15\u001b[30;15H\u001b[?12l\u001b[?25h"]
[612.641138, "o", "\u001b[?25l\u001b[48;5;242m \r\n    \u001b[49m\u001b[38;5;224m\u001b[48;5;242m}\u001b[39m\u001b[49m\u001b[34;114H90,6 \u001b[31;6H\u001b[?12l\u001b[?25h"]
[618.271357, "o", "\u001b[?25l\u001b[?12l\u001b[?25h"]
[618.577494, "o", "\u001b[?25l\r\u001b[5A  \u001b[1m\u001b[36m\u001b[96mray.worker.default\u001b[0m\u001b[38;5;224m:\u001b[39m\u001b[K\r\n    \u001b[1m\u001b[36m\u001b[96mmin_workers\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m2\u001b[39m\u001b[K\r\n    \u001b[1m\u001b[36m\u001b[96mmax_workers\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m10\u001b[39m\u001b[K\r\n    \u001b[1m\u001b[36m\u001b[96mresources\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[38;5;224m\u001b[46m{\u001b[39m\u001b[49m\u001b[K\r\n        \u001b[1m\u001b[30m\u001b[43m\u001b[103mCPU\u001b[0m\u001b[38;5;224m:\u001b[39m \u001b[35m\u001b[95m2\u001b[39m\u001b[K\r\n    \u001b[38;5;224m\u001b[46m}\u001b[39m\u001b[49m\u001b[34;1H\u001b[K\u001b[113C90,5\u001b[10C57%\u001b[31;5H\u001b[?12l\u001b[?25h"]
[618.963439, "o", "\u001b[?5h\u001b[?5l\u001b[?25l\u001b[34;114H\u001b[K\r:\u001b[?12l\u001b[?25h"]
[620.36677, "o", "q"]
[620.590601, "o", "\r"]
[620.592775, "o", "\u001b[?25l\u001b[K\u001b[?12l\u001b[?25h"]
[620.595314, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[622.126498, "o", "vim whoami.yaml "]
[622.396444, "o", "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bray exec whoami.yaml 'ray status'"]
[623.582452, "o", "\r\u001b[1;34r\u001b[34;1H\n"]
[624.292959, "o", "2024-06-13 13:02:57,298 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[624.293534, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[624.293706, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[624.31432, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[624.347689, "o", "  Not starting the webserver, as it's already running\r\n"]
[624.436231, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n"]
[624.436457, "o", "    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n"]
[624.436745, "o", "    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984468964815587444 GLM  │  accepted   │  0 GLM     │  0.121810146895302345 GLM  │  13.3217 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n   "]
[624.436802, "o", " │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121810146895302345 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n"]
[624.437013, "o", "  \r\n"]
[624.454565, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[628.457866, "o", "======== Autoscaler status: 2024-06-13 11:02:35.554244 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n 192.168.0.5: ray.worker.default, setting-up\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 0B/429.56MiB memory\r\n 44B/214.78MiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 4+ pending tasks/actors\r\n"]
[628.560713, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[628.709136, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[658.607703, "o", "\u001b[H\u001b[2J\u001b]11;\u0007a9d504a888d43682676ac5d47c9c7d2e918b566d listening on port 8000\r\n\u001b[36m(ProxyActor pid=1064)\u001b[C\u001b[39mINFO:\u001b[5CStarted server process [1064]\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:01:02,658 controller 1034 deployment_state.py:1547 - Deploying new version of deployment WhoamiDeployment in application 'default'. Setting initial target number of replicas to 4.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:01:02,760 controller 1034 deployment_state.py:1831 - Adding 4 replicas to deployment WhoamiDeployment in application 'default'.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:01:32,806 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 4 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n"]
[658.607796, "o", "\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:02:02,813 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 4 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:02:32,841 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 4 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:03:02,883 controller 1034 deployment_state.py:2150 - Deployment 'Whoami"]
[658.608071, "o", "Deployment' in application 'default' 4 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:03:08,806 controller 1034 proxy_state.py:537 - Didn't receive ready check response for proxy f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6 after 5.0s.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:03:08,806 controller 1034 proxy_state.py:805 - Proxy on node 'f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6' UNHEALTHY. Shutting down the unhealthy proxy and starting a new one.\r\n\u001b[36m(ProxyActor pid=1297, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:03:14,185 proxy 192.168.0.5 proxy.py:1143 - Proxy actor 7388f1fe3daf5bc83e53b16b01000000 starting on node f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6.\r\n\u001b[36"]
[658.608349, "o", "m(ProxyActor pid=1297, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:03:14,515 proxy 192.168.0.5 proxy.py:1357 - Starting HTTP server on node: f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6 listening on port 8000\r\n\u001b[36m(ProxyActor pid=1297, ip=192.168.0.5)\u001b[C\u001b[39mINFO:\u001b[5CStarted server process [1297]\r\n\u001b[36m(autoscaler +2m26s)\u001b[C\u001b[39mTip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\r\n\u001b[36m(autoscaler +2m26s)\u001b[C\u001b[39mResized to 2 CPUs.\r\n"]
[660.79449, "o", "\u001b[36m(ServeController pid=1034)\u001b[39m WARNING 2024-06-13 11:03:32,898 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 2 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n"]
[664.798826, "o", "\u001b[H\u001b[2J\u001b]11;\u000713.3217 MATIC  │\u001b[2;5H│  network: polygon  │\u001b[16C│\u001b[29C│  confirmed  │  0 GLM\u001b[5C│  0.019622057074187837 GLM  │   \u001b[14C│\u001b[4;5H│  token: GLM\u001b[8C│\u001b[16C│\u001b[29C│  requested  │  0 GLM\u001b[5C│  0.121810146895302345 GLM  │   \u001b[14C│\u001b[6;5H└────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\u001b[9;3HYou can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n\n\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\r\n\u001b[0m======== Autoscaler status: 2024-06-13 11:02:35.554244 ========\r\nNode status\r\n------------------------"]
[664.798913, "o", "---------------------------------------\r\nActive:\rA\n1 ray.head.default\r\nPending:\rP\nray.worker.default, 1 launching\r \n192.168.0.5: ray.worker.default, setting-up\r \nNone: ray.worker.default, waiting-for-ssh\r\nRecent failures:\rR\n(no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\rU\n0B/429.56MiB memory\r \n44B/214.78MiB object_store_memory\r\n\nDemands:\rD\n{'CPU': 1.0}: 4+ pending tasks/actors\r\nShared connection to 192.168.0.4 closed.\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[665.855404, "o", "ray exec whoami.yaml 'ray status'"]
[667.967444, "o", "\r\n"]
[668.702406, "o", "2024-06-13 13:03:41,707 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[668.703164, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[668.7232, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[668.753929, "o", "  Not starting the webserver, as it's already running\r\n"]
[668.810299, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n"]
[668.810467, "o", "    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼───────"]
[668.810494, "o", "─────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984468964815587444 GLM  │  accepted   │  0 GLM     │  0.121810146895302345 GLM  │  13.3217 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121810146895302345 GLM  │                 │\r\n"]
[668.810563, "o", "    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[668.821186, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[672.888917, "o", "======== Autoscaler status: 2024-06-13 11:03:25.803928 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 1 ray.worker.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 2.0/2.0 CPU\r\n 0B/19.98GiB memory\r\n 88B/8.59GiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 2+ pending tasks/actors"]
[672.930693, "o", "\r\n"]
[672.991398, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[673.138608, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[694.303934, "o", "ray exec whoami.yaml 'ray list actors --filter \"state=ALIVE\"'"]
[696.047902, "o", "\r\n"]
[696.76455, "o", "2024-06-13 13:04:09,769 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[696.7651, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[696.765126, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[696.784898, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[696.813922, "o", "  Not starting the webserver, as it's already running\r\n"]
[696.901108, "o", "\n"]
[696.901226, "o", "  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬──────"]
[696.901287, "o", "───────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n"]
[696.901388, "o", "    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984468964815587444 GLM  │  accepted   │  0 GLM     │  0.121810146895302345 GLM  │  13.3217 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n   "]
[696.901438, "o", " │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121810146895302345 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[696.909592, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[700.764184, "o", "\n======== List: 2024-06-13 11:04:12.828191 ========\r\nStats:\r\n------------------------------\r\nTotal: 5\r\n\nTable:\r\n------------------------------\r\n    ACTOR_ID                          CLASS_NAME                             STATE      JOB_ID  NAME                                                                                               NODE_ID                                                     PID  RAY_NAMESPACE\r\n 0  23e8e197ee2c32a113b7754501000000  ServeReplica:default:WhoamiDeployment  ALIVE    01000000  SERVE_REPLICA::default#WhoamiDeployment#UGPhxC                                                     f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1028  serve\r\n 1  2b7fcd2c77bc4197d88a64b301000000  ServeController                        ALIVE    01000000  SERVE_CONTROLLER_ACTOR                                                                             e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1034  serve\r\n 2  5cb00c64013d21cb1c63b9b101000000  ServeReplica:default:WhoamiDeploy"]
[700.764465, "o", "ment  ALIVE    01000000  SERVE_REPLICA::default#WhoamiDeployment#sHIBkq                                                     f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1027  serve\r\n 3  7388f1fe3daf5bc83e53b16b01000000  ProxyActor                             ALIVE    01000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6  f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1297  serve\r\n 4  956b65323db531bbac541c0301000000  ProxyActor                             ALIVE    01000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d  e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1064  serve\r\n\n"]
[700.824388, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[700.967299, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[738.048434, "o", "\u001b[H\u001b[2J\u001b]11;\u0007uto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:02:32,841 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 4 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:03:02,883 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 4 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resource"]
[738.048535, "o", "s available: {}. Use `ray status` for more details.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:03:08,806 controller 1034 proxy_state.py:537 - Didn't receive ready check response for proxy f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6 after 5.0s.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:03:08,806 controller 1034 proxy_state.py:805 - Proxy on node 'f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6' UNHEALTHY. Shutting down the unhealthy proxy and starting a new one.\r\n\u001b[36m(ProxyActor pid=1297, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:03:14,185 proxy 192.168.0.5 proxy.py:1143 - Proxy actor 7388f1fe3daf5bc83e53b16b01000000 starting on node f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6.\r\n\u001b[36m(ProxyActor pid=1297, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:03:14,515 proxy 192.168.0.5 proxy.py:1357 - Starting HTTP server on node: f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6 listening on port 8000\r\n\u001b[36m(ProxyActor pid=1297, ip=192.168.0.5"]
[738.048589, "o", ")\u001b[C\u001b[39mINFO:\u001b[5CStarted server process [1297]\r\n\u001b[36m(autoscaler +2m26s)\u001b[C\u001b[39mTip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\r\n\u001b[36m(autoscaler +2m26s)\u001b[C\u001b[39mResized to 2 CPUs.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:03:32,898 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 2 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:04:02,995 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 2 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Reso"]
[738.048927, "o", "urces required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:04:33,050 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 2 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n"]
[740.947622, "o", "\u001b[H\u001b[2J\u001b]11;\u0007"]
[740.963474, "o", "\u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[741.952119, "o", "\r(reverse-i-search)`': \u001b[K"]
[742.192191, "o", "\b\b\ba': source ~/.envs/ray-serve-test-0/bin/activ\u001b[7ma\u001b[0mte\b\b\b"]
[742.384061, "o", "\r(reverse-i-search)`ac': source ~/.envs/ray-serve-test-0/bin/\u001b[7mac\u001b[0mtivate\b\b\b\b\b\b\b\b"]
[742.576192, "o", "\r(reverse-i-search)`act': source ~/.envs/ray-serve-test-0/bin/\u001b[7mact\u001b[0mivate\b\b\b\b\b\b\b\b"]
[742.735933, "o", "\r(reverse-i-search)`acti': source ~/.envs/ray-serve-test-0/bin/\u001b[7macti\u001b[0mvate\b\b\b\b\b\b\b\b"]
[743.472332, "o", "\r\u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ source ~/.envs/ray-serve-test-0/bin/activate\b\b\b\b\b\b\b\b\r\n"]
[743.473694, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[745.118901, "o", "grep -E \"^ssh.*192.168.0.3$\" ~/.local/share/ray_on_golem/webserver_debug.log | sed -e \"s/ root@/ -L8000:localhost:8000 root@/\" | sed -e \"s/-vvv\\? //g\""]
[747.18406, "o", "\u001b[A\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"]
[747.456015, "o", "g"]
[747.956342, "o", "r"]
[747.987199, "o", "e"]
[748.01702, "o", "p"]
[748.047945, "o", " "]
[748.077662, "o", "-"]
[748.109831, "o", "E"]
[748.139608, "o", " "]
[748.170399, "o", "\""]
[748.200968, "o", "^"]
[748.231841, "o", "s"]
[748.261942, "o", "s"]
[748.291787, "o", "h"]
[748.321755, "o", "."]
[748.352808, "o", "*"]
[748.383126, "o", "1"]
[748.413938, "o", "9"]
[748.444771, "o", "2"]
[748.476087, "o", "."]
[748.505889, "o", "1"]
[748.536741, "o", "6"]
[748.768445, "o", "8"]
[748.944381, "o", "."]
[749.088044, "o", "0"]
[749.232187, "o", "."]
[749.664183, "o", "3"]
[749.952585, "o", "\b$\" ~/.local/share/ray_on_golem\u001b[P\u001b[A\b(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ grep -E \"^ssh.*192.168.0."]
[751.088087, "o", "4$\" ~/.local/share/ray_on_gole\u001b[1@m\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ grep -E \"^ssh.*192.168.0.4"]
[751.600352, "o", "\r\n\n"]
[751.615843, "o", "ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o PasswordAuthentication=no -o ServerAliveInterval=300 -o ServerAliveCountMax=3 -o 'ProxyCommand=websocat asyncstdio: ws://127.0.0.1:7465/net-api/v1/net/4518df86ca084b86aa9884071432a30c/tcp/192.168.0.4/22 --binary -H=Authorization:'\"'\"'Bearer 9d3cc81565354bda917085e0f5b6d4ed'\"'\"'' -i /tmp/ray_on_golem/ray_on_golem_rsa_40fbf84919 -L8000:localhost:8000 root@192.168.0.4\r\n"]
[751.616199, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[759.812999, "o", "ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o PasswordAuthentication=no -o ServerAliveInterval=300 -o ServerAliveCountMax=3 -o 'ProxyCommand=websocat asyncstdio: ws://127.0.0.1:7465/net-api/v1/net/4518df86ca084b86aa9884071432a30c/tcp/192.168.0.4/22 --binary -H=Authorization:'\"'\"'Bearer 9d3cc81565354bda917085e0f5b6d4ed'\"'\"'' -i /tmp/ray_on_golem/ray_on_golem_rsa_40fbf84919 -L8000:localhost:8000 root@192.168.0.4"]
[761.120121, "o", "\r\n"]
[761.567629, "o", "Warning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\n"]
[762.727373, "o", "Linux (none) 5.10.29-0-virt #1-Alpine SMP Mon, 12 Apr 2021 15:48:39 UTC x86_64\r\n\nThe programs included with the Debian GNU/Linux system are free software;\r\nthe exact distribution terms for each program are described in the\r\nindividual files in /usr/share/doc/*/copyright.\r\n\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\r\npermitted by applicable law.\r\nLast login: Thu Jun 13 11:04:09 2024 from 192.168.0.1\r\n"]
[762.734905, "o", "(venv) root@(none):~# "]
[767.968658, "o", "\u001b[H\u001b[2J\u001b]11;\u0007  \nYou can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n\n\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\r\n\n\u001b[0m======== List: 2024-06-13 11:04:12.828191 ========\r\nStats:\r\n------------------------------\r\nTotal: 5\r\n\nTable:\r\n------------------------------\u001b[26D\nACTOR_ID\u001b[26CCLASS_NAME\u001b[29CSTATE\u001b[6CJOB_ID  NAME\u001b[30C  \u001b[63CNODE_ID\u001b[53CPID  RAY_NAMESPACE\rY\n0  23e8e197ee2c32a113b7754501000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C01000000  SERVE_REPLICA::default#WhoamiDeployment#UGPhxC\u001b[53Cf30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1028  serve\b\b\n1  2b7fcd2c77bc4197d88a64b301000000  ServeController\u001b[24CALIVE\u001b[4C01000000  SERVE_CONTROLLER_ACTOR\u001b[12C  \u001b[63Ce7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1034  serve\b\b\n2  5cb00c64013d21cb1c63b9b101000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C01000000  SERVE_REPLICA::default#WhoamiDeployment#sHIBkq\u001b[53Cf30e"]
[767.96874, "o", "a419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1027  serve\b\b\n3  7388f1fe3daf5bc83e53b16b01000000  ProxyActor\u001b[29CALIVE\u001b[4C01000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6  f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1297  serve\b\b\n4  956b65323db531bbac541c0301000000  ProxyActor\u001b[29CALIVE\u001b[4C01000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d  e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1064  serve\r\n\nShared connection to 192.168.0.4 closed.\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[769.840103, "o", "ray exec whoami.yaml 'ray list actors --filter \"state=ALIVE\"'"]
[771.072624, "o", "\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ ray exec whoami.yaml 'ray status'\u001b[K\r\n\u001b[K\u001b[A(ray-serve-test-0) \u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C:\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C$ ray exec whoami.yaml 'ray status'"]
[772.448719, "o", "\r\n"]
[773.163904, "o", "2024-06-13 13:05:26,169 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[773.164598, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[773.164752, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[773.184323, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[773.215441, "o", "  Not starting the webserver, as it's already running\r\n"]
[773.278919, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n"]
[773.279165, "o", "    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼───────────────"]
[773.27925, "o", "──┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984468964815587444 GLM  │  accepted   │  0 GLM     │  0.121810146895302345 GLM  │  13.3217 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121810146895302345 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴─"]
[773.27932, "o", "───────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[773.292951, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[777.069757, "o", "======== Autoscaler status: 2024-06-13 11:04:58.550985 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 1 ray.worker.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 2.0/2.0 CPU\r\n 0B/19.98GiB memory\r\n 88B/8.59GiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 2+ pending tasks/actors"]
[777.115239, "o", "\r\n"]
[777.170036, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[777.293242, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[786.449265, "o", "python3 ray-serve-whoami-client.py"]
[788.241122, "o", "\r\n"]
[790.260936, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[790.297956, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[799.713362, "o", "seq 100 | xargs -n1 -P20 python3 ray-serve-whoami-client.py"]
[800.416704, "o", "\r\n"]
[802.077949, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[802.587933, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[803.351912, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[803.354175, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[804.100549, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[804.103756, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[804.106777, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[806.121119, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[806.123505, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[806.628303, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[807.127353, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[807.383213, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[807.385208, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[807.3879, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[807.389889, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[807.535333, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[807.53641, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[807.542886, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}"]
[807.543017, "o", "\r\n"]
[807.54683, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[807.627532, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[808.139564, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[808.646381, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[809.146523, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[809.909597, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[809.912312, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[810.005667, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[810.157357, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[810.160332, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[812.682525, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[812.682725, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[812.686155, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[812.689265, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[812.965583, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[812.966911, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[813.096899, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[814.807926, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[814.810243, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[814.810527, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[814.810615, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[815.460338, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[815.463458, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[817.49262, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[817.495564, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[818.238484, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[818.238724, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[818.241469, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[818.400631, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[818.401785, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[819.251191, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[820.523876, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[820.526698, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[820.527033, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[821.032421, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[821.266896, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[821.271204, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[821.28141, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[821.529136, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[822.039417, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[824.059139, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[824.062816, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[824.066535, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[824.067615, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[824.800198, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[824.800577, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[824.802343, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[824.95391, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[824.956151, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[825.07166, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[825.576146, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[826.077361, "o", "{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[827.825491, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[827.828115, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[827.833915, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[827.835127, "o", "{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n"]
[827.870224, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[831.505658, "o", "seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c"]
[834.496633, "o", "\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[835.232808, "o", "tseq 100 | xargs -n1 -P50 python3 ray-serve-whoami-clie\u001b[1@n\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ t"]
[835.344938, "o", "iseq 100 | xargs -n1 -P50 python3 ray-serve-whoami-cli\u001b[1@e\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ ti"]
[835.409066, "o", "mseq 100 | xargs -n1 -P50 python3 ray-serve-whoami-cl\u001b[1@i\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ tim"]
[835.457012, "o", "eseq 100 | xargs -n1 -P50 python3 ray-serve-whoami-c\u001b[1@l\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ time"]
[835.681345, "o", " seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-\u001b[1@c\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ time "]
[835.984415, "o", "\r\n\n"]
[854.066128, "o", "\u001b[H\u001b[2J\u001b]11;\u0007ed to get queue length from replica default#WhoamiDeployment#UGPhxC within 0.1s. If this happens repeatedly it's likely caused by high network latency in the cluster. You can configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.\r\n\u001b[36m(ProxyActor pid=1064)\u001b[C\u001b[39mWARNING 2024-06-13 11:06:45,535 proxy 192.168.0.4 2babee48-c6e0-4efe-a2a3-5587f4751134 / router.py:742 - Failed to get queue length from replica default#WhoamiDeployment#sHIBkq within 0.1s. If this happens repeatedly it's likely caused by high network latency in the cluster. You can configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.\r\n\u001b[36m(ProxyActor pid=1064)\u001b[C\u001b[39mWARNING 2024-06-13 11:06:45,535 proxy 192.168.0.4 2babee48-c6e0-4efe-a2a3-5587f4751134 / router.py:742 - Failed to get queue length from replica default#WhoamiDeployment#UGPhxC within 0.1s. If this happens repeatedly it's likely caused by high network latency in the cluster. You ca"]
[854.066448, "o", "n configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:45,135 default_WhoamiDeployment sHIBkq 87511abb-1df0-418c-bb19-79f81f601bd1 / replica.py:772 - __CALL__ OK 503.1ms\r\n\u001b[36m(ProxyActor pid=1064)\u001b[C\u001b[39mWARNING 2024-06-13 11:06:45,674 proxy 192.168.0.4 c24b5e24-7434-40cb-8416-dc851abe780b / router.py:742 - Failed to get queue length from replica default#WhoamiDeployment#sHIBkq within 0.2s. If this happens repeatedly it's likely caused by high network latency in the cluster. You can configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.\r\n\u001b[36m(ProxyActor pid=1064)\u001b[C\u001b[39mWARNING 2024-06-13 11:06:45,674 proxy 192.168.0.4 ee85bd88-7a39-4f46-ad0d-a8be9a8f05bd / router.py:742 - Failed to get queue length from replica default#WhoamiDeployment#sHIBkq within 0.2s. If this happens repeatedly it's likely caused by high network l"]
[854.066683, "o", "atency in the cluster. You can configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.\r\n\u001b[36m(ProxyActor pid=1064)\u001b[C\u001b[39mWARNING 2024-06-13 11:06:45,719 proxy 192.168.0.4 0f66a47f-7a39-44fe-980f-7aed943a1709 / router.py:742 - Failed to get queue length from replica default#WhoamiDeployment#sHIBkq within 0.2s. If this happens repeatedly it's likely caused by high network latency in the cluster. You can configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.\r\n\u001b[36m(ProxyActor pid=1064)\u001b[C\u001b[39mWARNING 2024-06-13 11:06:45,736 proxy 192.168.0.4 2babee48-c6e0-4efe-a2a3-5587f4751134 / router.py:742 - Failed to get queue length from replica default#WhoamiDeployment#sHIBkq within 0.2s. If this happens repeatedly it's likely caused by high network latency in the cluster. You can configure the deadline using the `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` environment variable.\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, "]
[854.066923, "o", "ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:45,388 default_WhoamiDeployment UGPhxC 82a3ad30-b3e9-4877-867f-5445f9f803ea / replica.py:772 - __CALL__ OK 503.1ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:45,640 default_WhoamiDeployment sHIBkq b0b11aa3-44d8-45a3-bb7b-9b5f7a7ec02c / replica.py:772 - __CALL__ OK 502.3ms\r\n"]
[854.212557, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:45,892 default_WhoamiDeployment UGPhxC 37826cfd-7357-4e44-93ca-fd78b31b092b / replica.py:772 - __CALL__ OK 503.4ms\r\n"]
[854.473013, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:46,144 default_WhoamiDeployment sHIBkq 94ab279b-cbfd-4c5c-aa30-c63abb0b23f9 / replica.py:772 - __CALL__ OK 501.7ms\r\n"]
[854.735557, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:46,396 default_WhoamiDeployment UGPhxC a486c0c9-b9cc-4016-958f-79006988659e / replica.py:772 - __CALL__ OK 503.0ms\r\n"]
[854.999507, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:46,650 default_WhoamiDeployment sHIBkq 1284cb93-d253-4e4f-9c07-600a253ab004 / replica.py:772 - __CALL__ OK 503.4ms\r\n"]
[855.253595, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:46,900 default_WhoamiDeployment UGPhxC b1a43862-aae5-4350-b9c4-cc210d7db5ad / replica.py:772 - __CALL__ OK 503.2ms\r\n"]
[855.51559, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:47,154 default_WhoamiDeployment sHIBkq 227611f6-5a00-4737-bdff-8cd6309fd8e5 / replica.py:772 - __CALL__ OK 503.7ms\r\n"]
[855.791634, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:47,403 default_WhoamiDeployment UGPhxC 0b4a5068-9515-4c30-a6a7-e7dc87e1a7a5 / replica.py:772 - __CALL__ OK 502.9ms\r\n"]
[856.046756, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:47,658 default_WhoamiDeployment sHIBkq 7538fc39-f03c-4f72-8410-254658cd8d13 / replica.py:772 - __CALL__ OK 503.8ms\r\n"]
[856.302962, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:47,907 default_WhoamiDeployment UGPhxC eb6d3227-51d1-4ef3-b989-ba2e2d08c126 / replica.py:772 - __CALL__ OK 503.4ms\r\n"]
[856.572995, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:48,166 default_WhoamiDeployment sHIBkq a2782086-7f3d-46c1-bbba-c015090b881c / replica.py:772 - __CALL__ OK 503.0ms\r\n"]
[856.734028, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:48,411 default_WhoamiDeployment UGPhxC 4cec8402-ae80-4ac6-a262-c4ed03d154be / replica.py:772 - __CALL__ OK 502.8ms\r\n"]
[856.99739, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:48,673 default_WhoamiDeployment sHIBkq 3563166e-ba48-47d0-b14a-120ff755b37b / replica.py:772 - __CALL__ OK 503.2ms\r\n"]
[857.255109, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:48,915 default_WhoamiDeployment UGPhxC 0a9d7ccf-bcef-449d-90a1-07701e9fa190 / replica.py:772 - __CALL__ OK 503.7ms\r\n"]
[857.818161, "o", "\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:06:49,419 default_WhoamiDeployment UGPhxC 26761cb3-1ca1-4d88-a7b4-36161fd8ff58 / replica.py:772 - __CALL__ OK 503.2ms\r\n"]
[858.049647, "o", "\u001b[H\u001b[2J\u001b]11;\u0007{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result"]
[858.049735, "o", "\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n{\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ time seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c\r\n"]
[868.54072, "o", "     61 {\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n     39 {\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n"]
[868.541014, "o", "\nreal\u001b[4C0m32,556s\r\nuser\u001b[4C0m23,762s\r\nsys\u001b[5C0m1,989s\r\n"]
[868.54124, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[873.793895, "o", "time seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c"]
[874.73756, "o", "\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ seq 100 | xargs -n1 -P20 python3 ray-serve-whoami-client.py\u001b[K"]
[876.017465, "o", "\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ \u001b[20Ppython3 ray-serve-whoami-client.py\r\n\u001b[K\u001b[A(ray-serve-test-0) \u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C:\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C$ python3 ray-serve-whoami-client.py"]
[877.68171, "o", "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\u001b[Pray exec whoami.yaml 'ray status'"]
[879.82587, "o", "\r\n"]
[880.567667, "o", "2024-06-13 13:07:13,573 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[880.56839, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[880.568525, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[880.588458, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[880.618691, "o", "  Not starting the webserver, as it's already running\r\n"]
[880.705198, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n"]
[880.705327, "o", "    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼───────"]
[880.705386, "o", "─────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────"]
[880.705436, "o", "────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[880.714313, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[884.73832, "o", "======== Autoscaler status: 2024-06-13 11:06:58.553309 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 1 ray.worker.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 2.0/2.0 CPU\r\n 0B/19.98GiB memory\r\n 88B/8.59GiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 2+ pending tasks/actors"]
[884.778637, "o", "\r\n"]
[884.794367, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[884.918906, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[896.001757, "o", "\u001b[H\u001b[2J\u001b]11;\u0007ea5-8758-4aa7-b8f3-d004e3010ff0 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:53,450 default_WhoamiDeployment UGPhxC 6b98a7c5-d9f8-4eb9-b785-07f5aaf13088 / replica.py:772 - __CALL__ OK 504.0ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:53,953 default_WhoamiDeployment UGPhxC 744c0d98-0582-4e29-8364-4745e7e96faf / replica.py:772 - __CALL__ OK 503.0ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:54,457 default_WhoamiDeployment UGPhxC 30efb9f4-015d-47d5-98f2-f323ac76bf2f / replica.py:772 - __CALL__ OK 503.7ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:54,961 default_WhoamiDeployment UGPhxC 4002b722-6658-4e1f-8214-226360ecdef6 / replica.py:772 - __CALL__ OK 503.1ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mIN"]
[896.002034, "o", "FO 2024-06-13 11:06:55,466 default_WhoamiDeployment UGPhxC 71634e89-8ec8-4540-a06a-dcf6e9a7370e / replica.py:772 - __CALL__ OK 503.8ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:55,969 default_WhoamiDeployment UGPhxC 58d5eef0-63b8-4443-98c9-be25d0bdf09c / replica.py:772 - __CALL__ OK 503.0ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:56,484 default_WhoamiDeployment UGPhxC c24b5e24-7434-40cb-8416-dc851abe780b / replica.py:772 - __CALL__ OK 503.7ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:56,987 default_WhoamiDeployment UGPhxC ee85bd88-7a39-4f46-ad0d-a8be9a8f05bd / replica.py:772 - __CALL__ OK 503.0ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:57,491 default_WhoamiDeployment UGPhxC 0f66a47f-7a39-44fe-980f-7aed943a1709 / replica.py:772 - __CALL__ OK 503.1ms\r\n\u001b[36m(ServeReplica:default:W"]
[896.002216, "o", "hoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:57,995 default_WhoamiDeployment UGPhxC 2babee48-c6e0-4efe-a2a3-5587f4751134 / replica.py:772 - __CALL__ OK 503.0ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:58,498 default_WhoamiDeployment UGPhxC db9add5e-5fef-4a43-a684-b5a982a820e9 / replica.py:772 - __CALL__ OK 502.8ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:59,001 default_WhoamiDeployment UGPhxC e5290d27-02f4-4a54-8f34-ea860b5e5ad1 / replica.py:772 - __CALL__ OK 502.9ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:06:59,505 default_WhoamiDeployment UGPhxC 87f2193f-417e-431e-b521-8a8f683032d4 / replica.py:772 - __CALL__ OK 503.0ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:07:00,009 default_WhoamiDeployment UGPhxC e5b92fe4-5883-431d-a581-cc5417dd9ac7 / replica.py:772 "]
[896.002426, "o", "- __CALL__ OK 503.0ms\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mWARNING 2024-06-13 11:07:03,219 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 2 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n"]
[896.475501, "o", "0"]
[901.189961, "o", "\u001b[36m(ServeController pid=1034)\u001b[39m WARNING 2024-06-13 11:07:33,254 controller 1034 deployment_state.py:2150 - Deployment 'WhoamiDeployment' in application 'default' 2 replicas that have taken more than 30s to be scheduled. This may be due to waiting for the cluster to auto-scale or for a runtime environment to be installed. Resources required for each replica: {\"CPU\": 1}, total resources available: {}. Use `ray status` for more details.\r\n"]
[910.610108, "o", "\u001b[H\u001b[2J\u001b]11;\u0007\u001b[4C│  network: polygon  │\u001b[16C│\u001b[29C│  confirmed  │  0 GLM\u001b[5C│  0.019622057074187837 GLM  │   \u001b[14C│\u001b[3;5H│  token: GLM\u001b[8C│\u001b[16C│\u001b[29C│  requested  │  0 GLM\u001b[5C│  0.121812370493913456 GLM  │   \u001b[14C│\u001b[5;5H└────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\u001b[8;3HYou can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n\n\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\r\n\u001b[0m======== Autoscaler status: 2024-06-13 11:06:58.553309 ========\r\nNode status\r\n--------------------------------------------"]
[910.610193, "o", "-------------------\r\nActive:\rA\n1 ray.head.default\r \n1 ray.worker.default\r\nPending:\rP\nray.worker.default, 1 launching\r \nNone: ray.worker.default, waiting-for-ssh\r\nRecent failures:\rR\n(no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\rU\n2.0/2.0 CPU\r \n0B/19.98GiB memory\r \n88B/8.59GiB object_store_memory\r\n\nDemands:\rD\n{'CPU': 1.0}: 2+ pending tasks/actors\r\nShared connection to 192.168.0.4 closed.\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[911.633694, "o", "ray exec whoami.yaml 'ray status'"]
[912.754284, "o", "\r\n"]
[913.464993, "o", "2024-06-13 13:07:46,470 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[913.465555, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[913.465671, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[913.485237, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[913.517537, "o", "  Not starting the webserver, as it's already running\r\n"]
[913.585716, "o", "\n"]
[913.585827, "o", "  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Paymen"]
[913.585973, "o", "t Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │ "]
[913.586052, "o", " reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │ "]
[913.586129, "o", "                │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[913.596237, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[917.611117, "o", "======== Autoscaler status: 2024-06-13 11:07:39.303920 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 1 ray.worker.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 2.0/2.0 CPU\r\n 0B/19.98GiB memory\r\n 88B/8.59GiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 2+ pending tasks/actors\r\n"]
[917.695222, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[917.816498, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[921.394269, "o", "ray exec whoami.yaml 'ray status'"]
[922.466129, "o", "\r\n"]
[923.180102, "o", "2024-06-13 13:07:56,185 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[923.180848, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[923.200906, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[923.230098, "o", "  Not starting the webserver, as it's already running\r\n"]
[923.291335, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payme"]
[923.29156, "o", "nt Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────"]
[923.291606, "o", "────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up"]
[923.291718, "o", ": https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[923.303469, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[927.166392, "o", "======== Autoscaler status: 2024-06-13 11:07:50.537105 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 1 ray.worker.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 2.0/2.0 CPU\r\n 0B/19.98GiB memory\r\n 88B/8.59GiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 2+ pending tasks/actors"]
[927.210662, "o", "\r\n"]
[927.269038, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[927.405915, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[928.882181, "o", "ray exec whoami.yaml 'ray status'"]
[929.087794, "o", "\r\n"]
[929.796136, "o", "2024-06-13 13:08:02,801 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[929.796951, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[929.797002, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[929.817284, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[929.847253, "o", "  Not starting the webserver, as it's already running\r\n"]
[929.918304, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n"]
[929.918482, "o", "    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼───────"]
[929.918621, "o", "─────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────"]
[929.918761, "o", "────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[929.944779, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[933.792807, "o", "======== Autoscaler status: 2024-06-13 11:07:50.537105 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 1 ray.worker.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 2.0/2.0 CPU\r\n 0B/19.98GiB memory\r\n 88B/8.59GiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 2+ pending tasks/actors"]
[933.834584, "o", "\r\n"]
[933.892723, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[934.030873, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[935.250239, "o", "ray exec whoami.yaml 'ray status'"]
[935.378379, "o", "\r\n"]
[936.117425, "o", "2024-06-13 13:08:09,122 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[936.117981, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[936.118092, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[936.138444, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[936.167793, "o", "  Not starting the webserver, as it's already running\r\n"]
[936.24368, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n"]
[936.243881, "o", "    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n"]
[936.244097, "o", "    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────"]
[936.24422, "o", "────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[936.261531, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[940.170402, "o", "======== Autoscaler status: 2024-06-13 11:07:50.537105 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 1 ray.worker.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 2.0/2.0 CPU\r\n 0B/19.98GiB memory\r\n 88B/8.59GiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 2+ pending tasks/actors\r\n"]
[940.269657, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[940.421841, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[977.203064, "o", "ray exec whoami.yaml 'ray status'"]
[977.410778, "o", "\r\n"]
[978.121512, "o", "2024-06-13 13:08:51,126 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[978.122101, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[978.122212, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[978.142253, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[978.171036, "o", "  Not starting the webserver, as it's already running\r\n"]
[978.326448, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n"]
[978.326663, "o", "    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬─"]
[978.326818, "o", "───────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n"]
[978.326965, "o", "    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴──────"]
[978.32712, "o", "──────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[978.34558, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[982.377441, "o", "======== Autoscaler status: 2024-06-13 11:08:28.797262 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 1 ray.worker.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 2.0/2.0 CPU\r\n 0B/19.98GiB memory\r\n 88B/8.59GiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 2+ pending tasks/actors"]
[982.418709, "o", "\r\n"]
[982.480569, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[982.609429, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1021.251551, "o", "ray exec whoami.yaml 'ray status'"]
[1022.338968, "o", "\r\n"]
[1023.066862, "o", "2024-06-13 13:09:36,072 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1023.067345, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[1023.067459, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[1023.087585, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[1023.118379, "o", "  Not starting the webserver, as it's already running\r\n"]
[1023.189708, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n"]
[1023.189865, "o", "    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n   "]
[1023.189923, "o", " │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[1023.19879, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[1027.214477, "o", "======== Autoscaler status: 2024-06-13 11:09:22.293984 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 1 ray.worker.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 2.0/2.0 CPU\r\n 0B/19.98GiB memory\r\n 88B/8.59GiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 2+ pending tasks/actors"]
[1027.258826, "o", "\r\n"]
[1027.317475, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1027.462156, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1071.187774, "o", "ray exec whoami.yaml 'ray status'"]
[1071.364057, "o", "\r\n"]
[1072.083668, "o", "2024-06-13 13:10:25,089 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1072.084303, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[1072.104242, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[1072.134525, "o", "  Not starting the webserver, as it's already running\r\n"]
[1072.199094, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payme"]
[1072.19937, "o", "nt Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────"]
[1072.199404, "o", "────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up"]
[1072.199419, "o", ": https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[1072.211884, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[1076.078168, "o", "======== Autoscaler status: 2024-06-13 11:10:11.043838 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 1 ray.worker.default\r\nPending:\r\n ray.worker.default, 1 launching\r\n None: ray.worker.default, waiting-for-ssh\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 2.0/2.0 CPU\r\n 0B/19.98GiB memory\r\n 88B/8.59GiB object_store_memory\r\n\nDemands:\r\n {'CPU': 1.0}: 2+ pending tasks/actors\r\n"]
[1076.180154, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1076.306981, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1276.64578, "o", "ray exec whoami.yaml 'ray status'"]
[1277.045556, "o", "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btime seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c"]
[1277.34989, "o", "\r\n"]
[1292.423757, "o", "     27 {\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n     25 {\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n     23 {\"result\":\"192.168.0.7 2b5aa2e02a4a8e2a0f44502b01000000\"}\r\n     25 {\"result\":\"192.168.0.7 f4c84c2017e10e052f5cc45601000000\"}\r\n"]
[1292.424131, "o", "\nreal\u001b[4C0m15,074s\r\nuser\u001b[4C0m23,329s\r\nsys\u001b[5C0m1,918s\r\n"]
[1292.424419, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1303.653883, "o", "time seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c"]
[1304.085682, "o", "\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ \u001b[21Pray exec whoami.yaml 'ray status'\r\n\u001b[K\u001b[A(ray-serve-test-0) \u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C:\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C$ ray exec whoami.yaml 'ray status'"]
[1307.045933, "o", "\r\n"]
[1307.747538, "o", "2024-06-13 13:14:20,752 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1307.748152, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[1307.748313, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[1307.768909, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[1307.797429, "o", "  Not starting the webserver, as it's already running\r\n"]
[1307.927289, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n"]
[1307.927537, "o", "    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼───────────────"]
[1307.927598, "o", "──┤\r\n"]
[1307.927824, "o", "    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboardin"]
[1307.927881, "o", "g/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[1307.948703, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[1311.931783, "o", "======== Autoscaler status: 2024-06-13 11:13:57.680895 ========\r\nNode status\r\n---------------------------------------------------------------\r\nActive:\r\n 1 ray.head.default\r\n 2 ray.worker.default\r\nPending:\r\n (no pending nodes)\r\nRecent failures:\r\n (no failures)\r\n\nResources\r\n---------------------------------------------------------------\r\nUsage:\r\n 4.0/4.0 CPU\r\n 0B/23.69GiB memory\r\n 132B/10.18GiB object_store_memory\r\n\nDemands:\r\n (no resource demands)\r\n"]
[1312.041396, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1312.185617, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1314.069671, "o", "ray exec whoami.yaml 'ray status'"]
[1314.406524, "o", "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\btime seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c"]
[1315.734735, "o", "\r\n"]
[1330.725729, "o", "     23 {\"result\":\"192.168.0.5 23e8e197ee2c32a113b7754501000000\"}\r\n     26 {\"result\":\"192.168.0.5 5cb00c64013d21cb1c63b9b101000000\"}\r\n     24 {\"result\":\"192.168.0.7 2b5aa2e02a4a8e2a0f44502b01000000\"}\r\n     27 {\"result\":\"192.168.0.7 f4c84c2017e10e052f5cc45601000000\"}\r\n"]
[1330.725922, "o", "\nreal\u001b[4C0m14,991s\r\nuser\u001b[4C0m23,354s\r\nsys\u001b[5C0m2,136s\r\n"]
[1330.726394, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1333.158798, "o", "ray exec whoami.yaml 'ray list actors --filter \"state=ALIVE\"'"]
[1333.74982, "o", "\r\n"]
[1334.457236, "o", "2024-06-13 13:14:47,462 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1334.457877, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[1334.457977, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[1334.477562, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[1334.507399, "o", "  Not starting the webserver, as it's already running\r\n"]
[1334.801302, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n"]
[1334.801389, "o", "  Your wallet:\r\n"]
[1334.801622, "o", "    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n"]
[1334.80183, "o", "    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼───────────────"]
[1334.802015, "o", "──────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴───"]
[1334.802065, "o", "─────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[1334.819839, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[1338.789051, "o", "\n======== List: 2024-06-13 11:14:50.853304 ========\r\nStats:\r\n------------------------------\r\nTotal: 8\r\n\nTable:\r\n------------------------------\r\n    ACTOR_ID                          CLASS_NAME                             STATE      JOB_ID  NAME                                                                                               NODE_ID                                                     PID  RAY_NAMESPACE\r\n 0  1eb2cf43739f2e63192abd5a01000000  ProxyActor                             ALIVE    01000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23  944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   1674  serve\r\n 1  23e8e197ee2c32a113b7754501000000  ServeReplica:default:WhoamiDeployment  ALIVE    01000000  SERVE_REPLICA::default#WhoamiDeployment#UGPhxC                                                     f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1028  serve\r\n 2  2b5aa2e02a4a8e2a0f44502b01000000  ServeReplica:default:WhoamiDeploy"]
[1338.789318, "o", "ment  ALIVE    01000000  SERVE_REPLICA::default#WhoamiDeployment#YODyJq                                                     944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   1223  serve\r\n 3  2b7fcd2c77bc4197d88a64b301000000  ServeController                        ALIVE    01000000  SERVE_CONTROLLER_ACTOR                                                                             e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1034  serve\r\n 4  5cb00c64013d21cb1c63b9b101000000  ServeReplica:default:WhoamiDeployment  ALIVE    01000000  SERVE_REPLICA::default#WhoamiDeployment#sHIBkq                                                     f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1027  serve\r\n 5  7388f1fe3daf5bc83e53b16b01000000  ProxyActor                             ALIVE    01000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6  f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1297  serve\r\n 6  956b65323db531bbac541c0"]
[1338.789391, "o", "301000000  ProxyActor                             ALIVE    01000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d  e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1064  serve\r\n 7  f4c84c2017e10e052f5cc45601000000  ServeReplica:default:WhoamiDeployment  ALIVE    01000000  SERVE_REPLICA::default#WhoamiDeployment#ZVCRqK                                                     944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   1224  serve\r\n\n"]
[1339.85502, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1339.98071, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1350.19818, "o", "\u001b[H\u001b[2J\u001b]11;\u0007659-693c-47e0-b52f-befa9ad1f803 / replica.py:772 - __CALL__ OK 503.0ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:14:39,684 default_WhoamiDeployment sHIBkq 8dfdef86-68b6-484d-aaf8-495c528ec12d / replica.py:772 - __CALL__ OK 502.9ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1223, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:40,599 default_WhoamiDeployment YODyJq 5277d9e7-2ac0-4627-863a-63b74b5c37b1 / replica.py:772 - __CALL__ OK 503.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1224, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:40,532 default_WhoamiDeployment ZVCRqK 88898d9f-cebf-486a-857d-e2102ab7a159 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:14:39,941 default_WhoamiDeployment UGPhxC 2bc9c47c-b5b2-45f4-a0f1-788126aa7d80 / replica.py:772 - __CALL__ OK 503.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[C\u001b[39mIN"]
[1350.19843, "o", "FO 2024-06-13 11:14:40,188 default_WhoamiDeployment sHIBkq a7c1f406-9262-4d1a-80f9-cb3fe56fd8f6 / replica.py:772 - __CALL__ OK 503.8ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1224, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:41,037 default_WhoamiDeployment ZVCRqK 9e893c23-b1b5-48a4-ace2-81d479bf1811 / replica.py:772 - __CALL__ OK 503.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1223, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:41,105 default_WhoamiDeployment YODyJq 00046f36-39e7-4ae8-ae83-31d4c1e9f6b1 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:14:40,445 default_WhoamiDeployment UGPhxC a98e7aee-9072-4821-9f8d-14684d204d84 / replica.py:772 - __CALL__ OK 503.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1224, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:41,541 default_WhoamiDeployment ZVCRqK f8e36302-4b01-4938-a5a3-4027e6ee40e4 / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:W"]
[1350.198635, "o", "hoamiDeployment pid=1027, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:14:40,698 default_WhoamiDeployment sHIBkq f5270b6b-d85f-42f1-86ab-fb68200949d2 / replica.py:772 - __CALL__ OK 503.7ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1223, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:41,609 default_WhoamiDeployment YODyJq 7cb277d8-57c8-4ba7-9cbd-22ca1d16252a / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:14:41,201 default_WhoamiDeployment sHIBkq 12cb3998-7dc8-4da2-95bd-ad1b953bcab2 / replica.py:772 - __CALL__ OK 503.0ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1224, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:42,046 default_WhoamiDeployment ZVCRqK ab925571-f482-4ef6-884c-6cd2fd3d135f / replica.py:772 - __CALL__ OK 503.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1224, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:42,551 default_WhoamiDeployment ZVCRqK ea322486-f45e-4b06-8dd3-e6a2ceb5274f / replica.py:772 "]
[1350.198858, "o", "- __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:14:41,705 default_WhoamiDeployment sHIBkq 9052dd90-1cb9-416a-ba0f-0dd07e721eca / replica.py:772 - __CALL__ OK 503.1ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1224, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:43,055 default_WhoamiDeployment ZVCRqK f4b58ee7-d36d-4496-9d07-ee2982ba1b82 / replica.py:772 - __CALL__ OK 503.3ms\r\n"]
[1352.593374, "o", "^C2024-06-13 11:15:04,691\u001b[7CINFO scripts.py:517 -- Got KeyboardInterrupt, shutting down...\r\n"]
[1352.703754, "o", "\u001b[36m(ServeController pid=1034)\u001b[39m INFO 2024-06-13 11:15:04,754 controller 1034 deployment_state.py:1857 - Removing 4 replicas from deployment 'WhoamiDeployment' in application 'default'.\r\n"]
[1354.916818, "o", "\u001b[36m(ServeController pid=1034)\u001b[39m INFO 2024-06-13 11:15:06,964 controller 1034 deployment_state.py:2185 - Replica default#WhoamiDeployment#YODyJq is stopped.\r\n"]
[1355.015092, "o", "\u001b[36m(ServeController pid=1034)\u001b[39m INFO 2024-06-13 11:15:07,069 controller 1034 deployment_state.py:2185 - Replica default#WhoamiDeployment#ZVCRqK is stopped.\r\n"]
[1355.218166, "o", "\u001b[36m(ServeController pid=1034)\u001b[39m INFO 2024-06-13 11:15:07,278 controller 1034 deployment_state.py:2185 - Replica default#WhoamiDeployment#UGPhxC is stopped.\r\n"]
[1355.320113, "o", "\u001b[36m(ServeController pid=1034)\u001b[39m INFO 2024-06-13 11:15:07,383 controller 1034 deployment_state.py:2185 - Replica default#WhoamiDeployment#sHIBkq is stopped.\r\n"]
[1356.661394, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1356.785912, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1358.342505, "o", "\r\n"]
[1358.342878, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1358.534839, "o", "\r\n"]
[1358.534948, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1362.311024, "o", "\u001b[H\u001b[2J\u001b]11;\u0007Total: 8\r\n\nTable:\r\n------------------------------\u001b[5;5HACTOR_ID\u001b[26CCLASS_NAME\u001b[29CSTATE\u001b[6CJOB_ID  NAME\u001b[30C  \u001b[63CNODE_ID\u001b[53CPID  RAY_NAMESPACE\rY\n0  1eb2cf43739f2e63192abd5a01000000  ProxyActor\u001b[29CALIVE\u001b[4C01000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23  944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   1674  serve\b\b\n1  23e8e197ee2c32a113b7754501000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C01000000  SERVE_REPLICA::default#WhoamiDeployment#UGPhxC\u001b[53Cf30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1028  serve\b\b\n2  2b5aa2e02a4a8e2a0f44502b01000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C01000000  SERVE_REPLICA::default#WhoamiDeployment#YODyJq\u001b[53C944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   1223  serve\b\b\n3  2b7fcd2c77bc4197d88a64b301000000  ServeController\u001b[24CALIVE\u001b[4C01000000  SERVE_CONTROLLER_ACTOR\u001b[12C  \u001b[63Ce7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1034  serve\b"]
[1362.311305, "o", "\b\n4  5cb00c64013d21cb1c63b9b101000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C01000000  SERVE_REPLICA::default#WhoamiDeployment#sHIBkq\u001b[53Cf30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1027  serve\b\b\n5  7388f1fe3daf5bc83e53b16b01000000  ProxyActor\u001b[29CALIVE\u001b[4C01000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6  f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1297  serve\b\b\n6  956b65323db531bbac541c0301000000  ProxyActor\u001b[29CALIVE\u001b[4C01000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d  e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1064  serve\b\b\n7  f4c84c2017e10e052f5cc45601000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C01000000  SERVE_REPLICA::default#WhoamiDeployment#ZVCRqK\u001b[53C944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   1224  serve\r\n\nShared connection to 192.168.0.4 closed.\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m"]
[1362.311366, "o", "~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1364.294708, "o", "\r(reverse-i-search)`': \u001b[K"]
[1364.66273, "o", "\b\b\bp': time seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.\u001b[7mp\u001b[0my | sort -n | uniq -c\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"]
[1364.902522, "o", "\r(reverse-i-search)`p\u001b[1@y': time seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.\u001b[7mpy\u001b[0m\b\b"]
[1365.046104, "o", "\r(reverse-i-search)`py\u001b[1@t': time seq 100 | xargs -n1 -P50 \u001b[7mpyt\u001b[0mhon3 ray-serve-whoami-client.py\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"]
[1365.126477, "o", "\r(reverse-i-search)`pyt\u001b[1@h': time seq 100 | xargs -n1 -P50 \u001b[7mpyth\u001b[0m\b\b\b\b"]
[1365.206866, "o", "\r(reverse-i-search)`pyth\u001b[1@o': time seq 100 | xargs -n1 -P50 \u001b[7mpytho\u001b[0m\b\b\b\b\b"]
[1366.183008, "o", "\r(reverse-i-search)`pytho': \u001b[25Pseq 100 | xargs -n1 -P20 \u001b[7mpytho\u001b[0mn3 ray-serve-whoami-client.py\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"]
[1367.222901, "o", "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\u001b[25P"]
[1368.535055, "o", "\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ python3 ray-serve-whoami-client.py\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n"]
[1368.756626, "o", "Traceback (most recent call last):\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 793, in urlopen\r\n"]
[1368.760102, "o", "    response = self._make_request(\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 537, in _make_request\r\n"]
[1368.760475, "o", "    response = conn.getresponse()\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/connection.py\", line 466, in getresponse\r\n"]
[1368.761367, "o", "    httplib_response = super().getresponse()\r\n  File \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\r\n"]
[1368.763757, "o", "    response.begin()\r\n  File \"/usr/lib/python3.10/http/client.py\", line 318, in begin\r\n"]
[1368.764107, "o", "    version, status, reason = self._read_status()\r\n  File \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\r\n"]
[1368.764513, "o", "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\r\n  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\r\n"]
[1368.76558, "o", "    return self._sock.recv_into(b)\r\n"]
[1368.765857, "o", "ConnectionResetError: [Errno 104] Connection reset by peer\r\n\nDuring handling of the above exception, another exception occurred:\r\n\nTraceback (most recent call last):\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/adapters.py\", line 667, in send\r\n"]
[1368.767394, "o", "    resp = conn.urlopen(\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 847, in urlopen\r\n"]
[1368.767819, "o", "    retries = retries.increment(\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/util/retry.py\", line 470, in increment\r\n"]
[1368.768765, "o", "    raise reraise(type(error), error, _stacktrace)\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/util/util.py\", line 38, in reraise\r\n"]
[1368.769122, "o", "    raise value.with_traceback(tb)\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 793, in urlopen\r\n"]
[1368.769867, "o", "    response = self._make_request(\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 537, in _make_request\r\n"]
[1368.77018, "o", "    response = conn.getresponse()\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/connection.py\", line 466, in getresponse\r\n"]
[1368.770541, "o", "    httplib_response = super().getresponse()\r\n  File \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\r\n"]
[1368.771682, "o", "    response.begin()\r\n  File \"/usr/lib/python3.10/http/client.py\", line 318, in begin\r\n"]
[1368.772067, "o", "    version, status, reason = self._read_status()\r\n  File \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\r\n"]
[1368.772451, "o", "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\r\n  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\r\n"]
[1368.773192, "o", "    return self._sock.recv_into(b)\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\r\n\nDuring handling of the above exception, another exception occurred:\r\n\nTraceback (most recent call last):\r\n  File \"/home/mateusz/devel/ray-on-golem/examples/ray-serve-whoami/ray-serve-whoami-client.py\", line 5, in <module>\r\n    response = requests.post(\"http://127.0.0.1:8000/\", json=post_text)\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/api.py\", line 115, in post\r\n"]
[1368.773886, "o", "    return request(\"post\", url, data=data, json=json, **kwargs)\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/api.py\", line 59, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r\n"]
[1368.774956, "o", "    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r\n"]
[1368.775267, "o", "    r = adapter.send(request, **kwargs)\r\n  File \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/adapters.py\", line 682, in send\r\n"]
[1368.775649, "o", "    raise ConnectionError(err, request=request)\r\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\r\n"]
[1368.805447, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1372.854413, "o", "\u001b[H\u001b[2J\u001b]11;\u0007f36-39e7-4ae8-ae83-31d4c1e9f6b1 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1028, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:14:40,445 default_WhoamiDeployment UGPhxC a98e7aee-9072-4821-9f8d-14684d204d84 / replica.py:772 - __CALL__ OK 503.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1224, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:41,541 default_WhoamiDeployment ZVCRqK f8e36302-4b01-4938-a5a3-4027e6ee40e4 / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:14:40,698 default_WhoamiDeployment sHIBkq f5270b6b-d85f-42f1-86ab-fb68200949d2 / replica.py:772 - __CALL__ OK 503.7ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1223, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:41,609 default_WhoamiDeployment YODyJq 7cb277d8-57c8-4ba7-9cbd-22ca1d16252a / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[C\u001b[39mIN"]
[1372.854684, "o", "FO 2024-06-13 11:14:41,201 default_WhoamiDeployment sHIBkq 12cb3998-7dc8-4da2-95bd-ad1b953bcab2 / replica.py:772 - __CALL__ OK 503.0ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1224, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:42,046 default_WhoamiDeployment ZVCRqK ab925571-f482-4ef6-884c-6cd2fd3d135f / replica.py:772 - __CALL__ OK 503.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1224, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:42,551 default_WhoamiDeployment ZVCRqK ea322486-f45e-4b06-8dd3-e6a2ceb5274f / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1027, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:14:41,705 default_WhoamiDeployment sHIBkq 9052dd90-1cb9-416a-ba0f-0dd07e721eca / replica.py:772 - __CALL__ OK 503.1ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1224, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:14:43,055 default_WhoamiDeployment ZVCRqK f4b58ee7-d36d-4496-9d07-ee2982ba1b82 / replica.py:772 - __CALL__ OK 503.3ms\r\n^C2024-06-13 11:15:04,691\u001b[7"]
[1372.854757, "o", "CINFO scripts.py:517 -- Got KeyboardInterrupt, shutting down...\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:15:04,754 controller 1034 deployment_state.py:1857 - Removing 4 replicas from deployment 'WhoamiDeployment' in application 'default'.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:15:06,964 controller 1034 deployment_state.py:2185 - Replica default#WhoamiDeployment#YODyJq is stopped.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:15:07,069 controller 1034 deployment_state.py:2185 - Replica default#WhoamiDeployment#ZVCRqK is stopped.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:15:07,278 controller 1034 deployment_state.py:2185 - Replica default#WhoamiDeployment#UGPhxC is stopped.\r\n\u001b[36m(ServeController pid=1034)\u001b[C\u001b[39mINFO 2024-06-13 11:15:07,383 controller 1034 deployment_state.py:2185 - Replica default#WhoamiDeployment#sHIBkq is stopped.\r\nShared connection to 192.168.0.4 closed.\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-"]
[1372.855042, "o", "serve-test-0/ray-serve-whoami\u001b[0m$\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1374.678645, "o", "ray exec whoami.yaml 'serve run ray-serve-whoami:whoami'"]
[1375.399312, "o", "\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ ray exec whoami.yaml '\u001b[21Pray status'\r\n\u001b[K\u001b[A(ray-serve-test-0) \u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C:\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C$ ray exec whoami.yaml 'ray status'"]
[1376.423229, "o", "\b\b\b\b\b\b\b\b\b\b\bserve run ray-serve-whoami:whoami'"]
[1377.543002, "o", "\r\n"]
[1378.259643, "o", "2024-06-13 13:15:31,264 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1378.260378, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[1378.280279, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[1378.311329, "o", "  Not starting the webserver, as it's already running\r\n"]
[1378.385214, "o", "\n"]
[1378.385282, "o", "  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n"]
[1378.385383, "o", "    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n"]
[1378.385496, "o", "    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n   "]
[1378.385553, "o", " │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[1378.395497, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[1378.799588, "o", "Warning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\n"]
[1382.932325, "o", "2024-06-13 11:15:35,030 INFO scripts.py:438 -- Running import path: 'ray-serve-whoami:whoami'.\r\n"]
[1382.938604, "o", "2024-06-13 11:15:35,038 INFO worker.py:1540 -- Connecting to existing Ray cluster at address: 192.168.0.4:6379...\r\n"]
[1382.943371, "o", "2024-06-13 11:15:35,042 INFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m192.168.0.4:8265 \u001b[39m\u001b[0m\r\n"]
[1389.380743, "o", "\u001b[36m(ProxyActor pid=1585)\u001b[39m INFO 2024-06-13 11:15:41,396 proxy 192.168.0.4 proxy.py:1143 - Proxy actor 689919bf4a4bccc45d050e1102000000 starting on node e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d.\r\n\u001b[36m(ProxyActor pid=1585)\u001b[39m INFO 2024-06-13 11:15:41,401 proxy 192.168.0.4 proxy.py:1357 - Starting HTTP server on node: e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d listening on port 8000\r\n"]
[1389.583378, "o", "\u001b[36m(ServeController pid=1551)\u001b[39m INFO 2024-06-13 11:15:41,647 controller 1551 deployment_state.py:1547 - Deploying new version of deployment WhoamiDeployment in application 'default'. Setting initial target number of replicas to 4.\r\n\u001b[36m(ProxyActor pid=1585)\u001b[39m INFO:     Started server process [1585]\r\n"]
[1389.686513, "o", "\u001b[36m(ServeController pid=1551)\u001b[39m INFO 2024-06-13 11:15:41,750 controller 1551 deployment_state.py:1831 - Adding 4 replicas to deployment WhoamiDeployment in application 'default'.\r\n"]
[1401.143667, "o", "\u001b[36m(ServeController pid=1551)\u001b[39m WARNING 2024-06-13 11:15:53,222 controller 1551 proxy_state.py:537 - Didn't receive ready check response for proxy f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6 after 5.0s.\r\n"]
[1401.187034, "o", "\u001b[36m(ServeController pid=1551)\u001b[39m INFO 2024-06-13 11:15:53,222 controller 1551 proxy_state.py:805 - Proxy on node 'f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6' UNHEALTHY. Shutting down the unhealthy proxy and starting a new one.\r\n"]
[1402.540697, "o", "2024-06-13 11:15:54,637 SUCC scripts.py:483 -- \u001b[32mDeployed Serve app successfully.\u001b[39m\r\n"]
[1407.075996, "o", "\u001b[36m(ServeController pid=1551)\u001b[39m WARNING 2024-06-13 11:15:59,165 controller 1551 proxy_state.py:537 - Didn't receive ready check response for proxy 944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23 after 5.0s.\r\n\u001b[36m(ServeController pid=1551)\u001b[39m INFO 2024-06-13 11:15:59,165 controller 1551 proxy_state.py:805 - Proxy on node '944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23' UNHEALTHY. Shutting down the unhealthy proxy and starting a new one.\r\n"]
[1407.458908, "o", "\u001b[36m(ProxyActor pid=1613, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:15:59,101 proxy 192.168.0.5 proxy.py:1143 - Proxy actor def8be5260669340cd47559e02000000 starting on node f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6.\r\n"]
[1407.831998, "o", "\u001b[36m(ProxyActor pid=1613, ip=192.168.0.5)\u001b[39m INFO 2024-06-13 11:15:59,434 proxy 192.168.0.5 proxy.py:1357 - Starting HTTP server on node: f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6 listening on port 8000\r\n"]
[1408.1957, "o", "\u001b[36m(ProxyActor pid=1613, ip=192.168.0.5)\u001b[39m INFO:     Started server process [1613]\r\n"]
[1417.225575, "o", "\u001b[36m(ServeController pid=1551)\u001b[39m WARNING 2024-06-13 11:16:09,265 controller 1551 proxy_state.py:537 - Didn't receive ready check response for proxy 944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23 after 10.0s.\r\n\u001b[36m(ServeController pid=1551)\u001b[39m INFO 2024-06-13 11:16:09,265 controller 1551 proxy_state.py:805 - Proxy on node '944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23' UNHEALTHY. Shutting down the unhealthy proxy and starting a new one.\r\n"]
[1428.327369, "o", "\u001b[H\u001b[2J\u001b]11;\u0007\u001b[4Craise value.with_traceback(tb)\r  \nFile \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 793, in urlopen\u001b[3;5Hresponse = self._make_request(\r  \nFile \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 537, in _make_request\u001b[5;5Hresponse = conn.getresponse()\r  \nFile \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/urllib3/connection.py\", line 466, in getresponse\u001b[7;5Hhttplib_response = super().getresponse()\r  \nFile \"/usr/lib/python3.10/http/client.py\", line 1375, in getresponse\u001b[9;5Hresponse.begin()\r  \nFile \"/usr/lib/python3.10/http/client.py\", line 318, in begin\u001b[59D\nversion, status, reason = self._read_status()\r  \nFile \"/usr/lib/python3.10/http/client.py\", line 279, in _read_status\u001b[66D\nline = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\r  \nFile \"/usr/lib/python3.10/socket.py\", line 705, in readinto\u001b[57D\nreturn self._sock.recv_into(b)\r\nurllib3.exceptions.ProtocolError: ('C"]
[1428.327454, "o", "onnection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\r\n\nDuring handling of the above exception, another exception occurred:\r\n\nTraceback (most recent call last):\rTr\nFile \"/home/mateusz/devel/ray-on-golem/examples/ray-serve-whoami/ray-serve-whoami-client.py\", line 5, in <module>\r  Fi\nresponse = requests.post(\"http://127.0.0.1:8000/\", json=post_text)\r  \nFile \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/api.py\", line 115, in post\r  Fi\nreturn request(\"post\", url, data=data, json=json, **kwargs)\r  \nFile \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/api.py\", line 59, in request\r  Fi\nreturn session.request(method=method, url=url, **kwargs)\r  \nFile \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\r  Fi\nresp = self.send(prep, **send_kwargs)\r  \nFile \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\r  Fi\nr = adapter.send(r"]
[1428.327665, "o", "equest, **kwargs)\r  \nFile \"/home/mateusz/.envs/ray-serve-test-0/lib/python3.10/site-packages/requests/adapters.py\", line 682, in send\r  Fi\nraise ConnectionError(err, request=request)\r\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1429.271333, "o", "python3 ray-serve-whoami-client.py"]
[1430.407738, "o", "\r\n"]
[1432.341189, "o", "{\"result\":\"192.168.0.7 da10f043de52778e4d7df6f302000000\"}\r\n"]
[1432.358389, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1435.927167, "o", "\r(reverse-i-search)`': \u001b[K"]
[1436.759027, "o", "\b\b\bt': python3 ray-serve-whoami-clien\u001b[7mt\u001b[0m.py\b\b\b\b"]
[1436.871349, "o", "\r(reverse-i-search)`ti': \u001b[7mti\u001b[0mme seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c\r(reverse-i-search)`ti': "]
[1436.983757, "o", "\b\b\b\u001b[1@m': \u001b[7mtim\u001b[0m\b\b\b"]
[1437.06367, "o", "\b\b\b\u001b[1@e': \u001b[7mtime\u001b[0m\b\b\b\b"]
[1437.511631, "o", "\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ time seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ \r\n\n"]
[1453.221796, "o", "     28 {\"result\":\"192.168.0.5 16a24373b9829c412873499002000000\"}\r\n     19 {\"result\":\"192.168.0.5 311697f513d3a9daff2777a602000000\"}\r\n     27 {\"result\":\"192.168.0.7 96c433e9af360c519399d4bc02000000\"}\r\n     26 {\"result\":\"192.168.0.7 da10f043de52778e4d7df6f302000000\"}\r\n"]
[1453.221956, "o", "\nreal\u001b[4C0m15,711s\r\nuser\u001b[4C0m22,790s\r\nsys\u001b[5C0m2,090s\r\n"]
[1453.222137, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1455.99188, "o", "time seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c"]
[1456.232047, "o", "\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ \u001b[20Ppython3 ray-serve-whoami-client.py\r\n\u001b[K\u001b[A(ray-serve-test-0) \u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C:\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C$ python3 ray-serve-whoami-client.py"]
[1458.407558, "o", "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bray exec whoami.yaml 'ray list actors --filter \"state=ALIVE\"'"]
[1459.991583, "o", "\r\n"]
[1460.731644, "o", "2024-06-13 13:16:53,737 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1460.732323, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[1460.732372, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[1460.752652, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[1460.78195, "o", "  Not starting the webserver, as it's already running\r\n"]
[1460.857944, "o", "\n"]
[1460.858289, "o", "  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼──────"]
[1460.858427, "o", "───────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴────────"]
[1460.858461, "o", "─────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[1460.876652, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[1464.72098, "o", "\n======== List: 2024-06-13 11:16:56.816065 ========\r\nStats:\r\n------------------------------\r\nTotal: 8\r\n\nTable:\r\n------------------------------\r\n    ACTOR_ID                          CLASS_NAME                             STATE      JOB_ID  NAME                                                                                               NODE_ID                                                     PID  RAY_NAMESPACE\r\n 0  16a24373b9829c412873499002000000  ServeReplica:default:WhoamiDeployment  ALIVE    02000000  SERVE_REPLICA::default#WhoamiDeployment#Xrhamb                                                     f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1439  serve\r\n 1  311697f513d3a9daff2777a602000000  ServeReplica:default:WhoamiDeployment  ALIVE    02000000  SERVE_REPLICA::default#WhoamiDeployment#LKswXn                                                     f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1438  serve\r\n 2  5d98b42de1cf13d16fcf997002000000  ServeController                  "]
[1464.721077, "o", "      ALIVE    02000000  SERVE_CONTROLLER_ACTOR                                                                             e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1551  serve\r\n 3  689919bf4a4bccc45d050e1102000000  ProxyActor                             ALIVE    02000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d  e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1585  serve\r\n 4  96c433e9af360c519399d4bc02000000  ServeReplica:default:WhoamiDeployment  ALIVE    02000000  SERVE_REPLICA::default#WhoamiDeployment#KZaJXA                                                     944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   1757  serve\r\n 5  987c14c2433e7162eb3e687a02000000  ProxyActor                             ALIVE    02000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23  944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   2089  serve\r\n 6  da10f043de52778e4d7df6f"]
[1464.721322, "o", "302000000  Se"]
[1464.762869, "o", "rveReplica:default:WhoamiDeployment  ALIVE    02000000  SERVE_REPLICA::default#WhoamiDeployment#KhQspG                                                     944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   1756  serve\r\n 7  def8be5260669340cd47559e02000000  ProxyActor                             ALIVE    02000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6  f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1613  serve\r\n\n"]
[1464.852391, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1464.994885, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1469.783515, "o", "\r\u001b[7mThis IS window 1 (bash).\u001b[27m  \b\b"]
[1471.527113, "o", "\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz\u001b[51C"]
[1471.928072, "o", "\u001b[0m\u001b[H\u001b[2J\u001b]11;\u0007168-3fcb-4d40-80fd-66d0c27e272c / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:41,806 default_WhoamiDeployment Xrhamb e69fe824-9f25-4d6c-ae6c-647ef6512cef / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1756, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:42,911 default_WhoamiDeployment KhQspG c5cadc02-2b2e-4b0d-b852-60354a6b1a8e / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:42,310 default_WhoamiDeployment Xrhamb bef66d99-71f9-4061-8a38-a287f36ebbf0 / replica.py:772 - __CALL__ OK 502.9ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1757, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:43,133 default_WhoamiDeployment KZaJXA a0697986-cfc9-42f7-b63d-ec77b362172c / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1756, ip=192.168.0.7)\u001b[C\u001b[3"]
[1471.928179, "o", "9mINFO 2024-06-13 11:16:43,423 default_WhoamiDeployment KhQspG e72411f5-9fdf-4ee0-a8cd-1f34e4bd0779 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:42,814 default_WhoamiDeployment Xrhamb ca5ca299-d0b8-4607-beb7-0fd22dadc44e / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1757, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:43,638 default_WhoamiDeployment KZaJXA 05e6a289-3777-474b-8811-3a4e1443ff91 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1756, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:43,927 default_WhoamiDeployment KhQspG a0d444ed-692a-4524-9771-f57bfbfa733f / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1757, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:44,142 default_WhoamiDeployment KZaJXA 355d5981-24d0-4408-bd5e-de9582534597 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:defau"]
[1471.92841, "o", "lt:WhoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:43,318 default_WhoamiDeployment Xrhamb bfbc064b-662a-4994-ae9c-099d8a79bc35 / replica.py:772 - __CALL__ OK 503.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1756, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:44,432 default_WhoamiDeployment KhQspG 15c2ee45-2550-4f5a-a342-adbe1e5c09ee / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1757, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:44,647 default_WhoamiDeployment KZaJXA 77cd0550-34a3-40d3-974b-29d52b611007 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:43,832 default_WhoamiDeployment Xrhamb 2573e375-53df-48bf-b9c6-d34890bceee7 / replica.py:772 - __CALL__ OK 503.9ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:44,335 default_WhoamiDeployment Xrhamb 9d0c560f-f0f4-4791-9305-0c2f6196ca3e / replica.py:"]
[1471.928488, "o", "772 - __CALL__ OK 502.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1757, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:45,151 default_WhoamiDeployment KZaJXA bacd1439-8fd4-4fc6-aa94-03769fae7819 / replica.py:772 - __CALL__ OK 503.1ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:44,839 default_WhoamiDeployment Xrhamb 62c1186f-a1b1-416d-b01b-d62e58d403c4 / replica.py:772 - __CALL__ OK 503.2ms\r\n"]
[1473.380602, "o", "^C2024-06-13 11:17:05,478\u001b[7CINFO scripts.py:517 -- Got KeyboardInterrupt, shutting down...\r\n"]
[1473.458852, "o", "\u001b[36m(ServeController pid=1551)\u001b[39m INFO 2024-06-13 11:17:05,536 controller 1551 deployment_state.py:1857 - Removing 4 replicas from deployment 'WhoamiDeployment' in application 'default'.\r\n"]
[1475.696446, "o", "\u001b[36m(ServeController pid=1551)\u001b[39m INFO 2024-06-13 11:17:07,750 controller 1551 deployment_state.py:2185 - Replica default#WhoamiDeployment#LKswXn is stopped.\r\n"]
[1475.771691, "o", "\u001b[36m(ServeController pid=1551)\u001b[39m INFO 2024-06-13 11:17:07,856 controller 1551 deployment_state.py:2185 - Replica default#WhoamiDeployment#Xrhamb is stopped.\r\n"]
[1476.575845, "o", "\u001b[36m(ServeController pid=1551)\u001b[39m INFO 2024-06-13 11:17:08,591 controller 1551 deployment_state.py:2185 - Replica default#WhoamiDeployment#KhQspG is stopped.\r\n\u001b[36m(ServeController pid=1551)\u001b[39m INFO 2024-06-13 11:17:08,592 controller 1551 deployment_state.py:2185 - Replica default#WhoamiDeployment#KZaJXA is stopped.\r\n"]
[1477.618656, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1477.743886, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1479.799891, "o", "\u001b[H\u001b[2J\u001b]11;\u0007Total: 8\r\n\nTable:\r\n------------------------------\u001b[5;5HACTOR_ID\u001b[26CCLASS_NAME\u001b[29CSTATE\u001b[6CJOB_ID  NAME\u001b[30C  \u001b[63CNODE_ID\u001b[53CPID  RAY_NAMESPACE\rY\n0  16a24373b9829c412873499002000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C02000000  SERVE_REPLICA::default#WhoamiDeployment#Xrhamb\u001b[53Cf30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1439  serve\b\b\n1  311697f513d3a9daff2777a602000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C02000000  SERVE_REPLICA::default#WhoamiDeployment#LKswXn\u001b[53Cf30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1438  serve\b\b\n2  5d98b42de1cf13d16fcf997002000000  ServeController\u001b[24CALIVE\u001b[4C02000000  SERVE_CONTROLLER_ACTOR\u001b[12C  \u001b[63Ce7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1551  serve\b\b\n3  689919bf4a4bccc45d050e1102000000  ProxyActor\u001b[29CALIVE\u001b[4C02000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d  e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1585  serve\b"]
[1479.799979, "o", "\b\n4  96c433e9af360c519399d4bc02000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C02000000  SERVE_REPLICA::default#WhoamiDeployment#KZaJXA\u001b[53C944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   1757  serve\b\b\n5  987c14c2433e7162eb3e687a02000000  ProxyActor\u001b[29CALIVE\u001b[4C02000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23  944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   2089  serve\b\b\n6  da10f043de52778e4d7df6f302000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C02000000  SERVE_REPLICA::default#WhoamiDeployment#KhQspG\u001b[53C944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   1756  serve\b\b\n7  def8be5260669340cd47559e02000000  ProxyActor\u001b[29CALIVE\u001b[4C02000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6  f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1613  serve\r\n\nShared connection to 192.168.0.4 closed.\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m"]
[1479.800303, "o", "~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1480.871772, "o", "ray exec whoami.yaml 'ray list actors --filter \"state=ALIVE\"'"]
[1482.071568, "o", "\r\n"]
[1482.774699, "o", "2024-06-13 13:17:15,780 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1482.775384, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[1482.775762, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[1482.795642, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[1482.825737, "o", "  Not starting the webserver, as it's already running\r\n"]
[1482.898068, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n"]
[1482.898307, "o", "    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n"]
[1482.898506, "o", "    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼───────────────"]
[1482.898666, "o", "──┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n"]
[1482.898812, "o", "    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[1482.918676, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[1486.669924, "o", "No resource in the cluster\r\n"]
[1486.771932, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1486.898573, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1492.888317, "o", "\u001b[H\u001b[2J\u001b]11;\u0007299-d0b8-4607-beb7-0fd22dadc44e / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1757, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:43,638 default_WhoamiDeployment KZaJXA 05e6a289-3777-474b-8811-3a4e1443ff91 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1756, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:43,927 default_WhoamiDeployment KhQspG a0d444ed-692a-4524-9771-f57bfbfa733f / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1757, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:44,142 default_WhoamiDeployment KZaJXA 355d5981-24d0-4408-bd5e-de9582534597 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:43,318 default_WhoamiDeployment Xrhamb bfbc064b-662a-4994-ae9c-099d8a79bc35 / replica.py:772 - __CALL__ OK 503.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1756, ip=192.168.0.7)\u001b[C\u001b[39mIN"]
[1492.888584, "o", "FO 2024-06-13 11:16:44,432 default_WhoamiDeployment KhQspG 15c2ee45-2550-4f5a-a342-adbe1e5c09ee / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1757, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:44,647 default_WhoamiDeployment KZaJXA 77cd0550-34a3-40d3-974b-29d52b611007 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:43,832 default_WhoamiDeployment Xrhamb 2573e375-53df-48bf-b9c6-d34890bceee7 / replica.py:772 - __CALL__ OK 503.9ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:44,335 default_WhoamiDeployment Xrhamb 9d0c560f-f0f4-4791-9305-0c2f6196ca3e / replica.py:772 - __CALL__ OK 502.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1757, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:16:45,151 default_WhoamiDeployment KZaJXA bacd1439-8fd4-4fc6-aa94-03769fae7819 / replica.py:772 - __CALL__ OK 503.1ms\r\n\u001b[36m(ServeReplica:default:W"]
[1492.888762, "o", "hoamiDeployment pid=1439, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:16:44,839 default_WhoamiDeployment Xrhamb 62c1186f-a1b1-416d-b01b-d62e58d403c4 / replica.py:772 - __CALL__ OK 503.2ms\r\n^C2024-06-13 11:17:05,478\u001b[7CINFO scripts.py:517 -- Got KeyboardInterrupt, shutting down...\r\n\u001b[36m(ServeController pid=1551)\u001b[C\u001b[39mINFO 2024-06-13 11:17:05,536 controller 1551 deployment_state.py:1857 - Removing 4 replicas from deployment 'WhoamiDeployment' in application 'default'.\r\n\u001b[36m(ServeController pid=1551)\u001b[C\u001b[39mINFO 2024-06-13 11:17:07,750 controller 1551 deployment_state.py:2185 - Replica default#WhoamiDeployment#LKswXn is stopped.\r\n\u001b[36m(ServeController pid=1551)\u001b[C\u001b[39mINFO 2024-06-13 11:17:07,856 controller 1551 deployment_state.py:2185 - Replica default#WhoamiDeployment#Xrhamb is stopped.\r\n\u001b[36m(ServeController pid=1551)\u001b[C\u001b[39mINFO 2024-06-13 11:17:08,591 controller 1551 deployment_state.py:2185 - Replica default#WhoamiDeployment#KhQspG is stopped.\r\n\u001b[36m(ServeController pid=1551)\u001b[C\u001b[39mINFO 2024-06-13 11:1"]
[1492.888983, "o", "7:08,592 controller 1551 deployment_state.py:2185 - Replica default#WhoamiDeployment#KZaJXA is stopped.\r\nShared connection to 192.168.0.4 closed.\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1493.975718, "o", "ray exec whoami.yaml 'serve run ray-serve-whoami:whoami'"]
[1495.336371, "o", "\r\n"]
[1496.046703, "o", "2024-06-13 13:17:29,052 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1496.047201, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[1496.047348, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[1496.067967, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[1496.098406, "o", "  Not starting the webserver, as it's already running\r\n"]
[1496.17907, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n"]
[1496.179151, "o", "    Payment Driver status: OK\r\n    \r\n"]
[1496.179392, "o", "    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼───────────────"]
[1496.179451, "o", "──┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n"]
[1496.179676, "o", "    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[1496.201792, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[1496.216038, "o", "muxclient: master hello exchange failed\r\n"]
[1496.803803, "o", "Warning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\n"]
[1501.159594, "o", "2024-06-13 11:17:33,255 INFO scripts.py:438 -- Running import path: 'ray-serve-whoami:whoami'.\r\n"]
[1501.166025, "o", "2024-06-13 11:17:33,264 INFO worker.py:1540 -- Connecting to existing Ray cluster at address: 192.168.0.4:6379...\r\n"]
[1501.170201, "o", "2024-06-13 11:17:33,268 INFO worker.py:1715 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32m192.168.0.4:8265 \u001b[39m\u001b[0m\r\n"]
[1507.566645, "o", "\u001b[36m(ProxyActor pid=1737)\u001b[39m INFO 2024-06-13 11:17:39,559 proxy 192.168.0.4 proxy.py:1143 - Proxy actor 26af716be52a6546b11890af03000000 starting on node e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d.\r\n\u001b[36m(ProxyActor pid=1737)\u001b[39m INFO 2024-06-13 11:17:39,562 proxy 192.168.0.4 proxy.py:1357 - Starting HTTP server on node: e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d listening on port 8000\r\n"]
[1507.761954, "o", "\u001b[36m(ServeController pid=1704)\u001b[39m INFO 2024-06-13 11:17:39,805 controller 1704 deployment_state.py:1547 - Deploying new version of deployment WhoamiDeployment in application 'default'. Setting initial target number of replicas to 4.\r\n"]
[1507.762681, "o", "\u001b[36m(ProxyActor pid=1737)\u001b[39m INFO:     Started server process [1737]\r\n"]
[1507.850079, "o", "\u001b[36m(ServeController pid=1704)\u001b[39m INFO 2024-06-13 11:17:39,907 controller 1704 deployment_state.py:1831 - Adding 4 replicas to deployment WhoamiDeployment in application 'default'.\r\n"]
[1519.21842, "o", "\u001b[36m(ServeController pid=1704)\u001b[39m WARNING 2024-06-13 11:17:51,288 controller 1704 proxy_state.py:537 - Didn't receive ready check response for proxy f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6 after 5.0s.\r\n\u001b[36m(ServeController pid=1704)\u001b[39m INFO 2024-06-13 11:17:51,289 controller 1704 proxy_state.py:805 - Proxy on node 'f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6' UNHEALTHY. Shutting down the unhealthy proxy and starting a new one.\r\n"]
[1520.7134, "o", "2024-06-13 11:17:52,809 SUCC scripts.py:483 -- \u001b[32mDeployed Serve app successfully.\u001b[39m\r\n"]
[1523.608285, "o", "\u001b[H\u001b[2J\u001b]11;\u0007\u001b[33mLoaded cached provider configuration\r\nIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\r\n\u001b[36mRay On Golem 0.11.0a2\rRa\n\u001b[39mNot starting the webserver, as it's already running\r  \n\nRunning Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r/c\nYour wallet:\u001b[10;5HStatus for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\u001b[62D\nPayment Driver status: OK\u001b[13;5H┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\u001b[12D\n│  platform\u001b[10C│  total amount  │  reserved\u001b["]
[1523.608571, "o", "19C│  amount\u001b[5C│  incoming  │  outgoing\u001b[18C│  gas\u001b[12C│\u001b[12D\n├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\u001b[12D\n│  driver: erc20\u001b[5C│  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM\u001b[5C│  0.121812370493913456 GLM  │  13.3199 MATIC  │\u001b[12D\n│  network: polygon  │\u001b[16C│\u001b[29C│  confirmed  │  0 GLM\u001b[5C│  0.019622057074187837 GLM  │   \u001b[14C│\u001b[12D\n│  token: GLM\u001b[8C│\u001b[16C│\u001b[29C│  requested  │  0 GLM\u001b[5C│  0.121812370493913456 GLM  │   \u001b[14C│\u001b[12D\n└────────────────────┴────────────────┴"]
[1523.608778, "o", "─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\u001b[28;3HYou can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n\n\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\r\n\u001b[0mNo resource in the cluster\r\nShared connection to 192.168.0.4 closed.\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1524.632436, "o", "ray exec whoami.yaml 'ray list actors --filter \"state=ALIVE\"'"]
[1525.623872, "o", "\r\n"]
[1526.327131, "o", "2024-06-13 13:17:59,332 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1526.327926, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[1526.347631, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[1526.377175, "o", "  Not starting the webserver, as it's already running\r\n"]
[1526.447429, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n"]
[1526.447802, "o", "    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼───────────────"]
[1526.447935, "o", "──┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/"]
[1526.44804, "o", "#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[1526.464275, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[1530.319462, "o", "\n======== List: 2024-06-13 11:18:02.414804 ========\r\nStats:\r\n------------------------------\r\nTotal: 7\r\n\nTable:\r\n------------------------------\r\n    ACTOR_ID                          CLASS_NAME                             STATE      JOB_ID  NAME                                                                                               NODE_ID                                                     PID  RAY_NAMESPACE\r\n 0  08ec7b29ff59ea2694a9b77803000000  ProxyActor                             ALIVE    03000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6  f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1861  serve\r\n 1  26af716be52a6546b11890af03000000  ProxyActor                             ALIVE    03000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d  e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1737  serve\r\n 2  3e21576f05e06b1d443d1c8103000000  ServeController                  "]
[1530.31974, "o", "      ALIVE    03000000  SERVE_CONTROLLER_ACTOR                                                                             e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1704  serve\r\n 3  8b0f2facdc22f4e4ed44ea4503000000  ServeReplica:default:WhoamiDeployment  ALIVE    03000000  SERVE_REPLICA::default#WhoamiDeployment#uzbkyc                                                     f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1683  serve\r\n 4  98837e3a59592650e9f88e6303000000  ServeReplica:default:WhoamiDeployment  ALIVE    03000000  SERVE_REPLICA::default#WhoamiDeployment#WaHNPc                                                     944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   2164  serve\r\n 5  98b2ccb6c2bac5593dca353403000000  ServeReplica:default:WhoamiDeployment  ALIVE    03000000  SERVE_REPLICA::default#WhoamiDeployment#aWeKWS                                                     944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   2163  serve\r\n 6  eb3a4cb0afe5db0b3d01696"]
[1530.319929, "o", "303000000  ServeReplica:default:WhoamiDeployment  ALIVE    03000000  SERVE_REPLICA::default#WhoamiDeployment#uXspkS                                                     f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1684  serve\r\n\n"]
[1530.422374, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1530.545321, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1532.904622, "o", "ray exec whoami.yaml 'ray list actors --filter \"state=ALIVE\"'"]
[1534.360669, "o", "\u001b[A\r(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ time seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c"]
[1537.416416, "o", "\r\n"]
[1553.377951, "o", "     25 {\"result\":\"192.168.0.5 8b0f2facdc22f4e4ed44ea4503000000\"}\r\n     24 {\"result\":\"192.168.0.5 eb3a4cb0afe5db0b3d01696303000000\"}\r\n     23 {\"result\":\"192.168.0.7 98837e3a59592650e9f88e6303000000\"}\r\n     28 {\"result\":\"192.168.0.7 98b2ccb6c2bac5593dca353403000000\"}\r\n"]
[1553.378202, "o", "\nreal\u001b[4C0m15,962s\r\nuser\u001b[4C0m23,161s\r\nsys\u001b[5C0m2,045s\r\n"]
[1553.378404, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1560.456717, "o", "\u001b[H\u001b[2J\u001b]11;\u0007\u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ source ~/.envs/ray-serve-test-0/bin/activate\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ grep -E \"^ssh.*192.168.0.4$\" ~/.local/share/ray_on_golem/webserver_debug.log | sed -e \"s/ root@/ -L8000:localhost:8000 root@/\" | sed -e \"s/-vvv\\? //g\"\r\nssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o PasswordAuthentication=no -o ServerAliveInterval=300 -o ServerAliveCountMax=3 -o 'ProxyCommand=websocat asyncstdio: ws://127.0.0.1:7465/net-api/v1/net/4518df86ca084b86aa9884071432a30c/tcp/192.168.0.4/22 --binary -H=Authorization:'\"'\"'Bearer 9d3cc81565354bda917085e0f5b6d4ed'\"'\"'' -i /tmp/ray_on_golem/ray_on_golem_rsa_40fbf84919 -L8000:localhost:8000 root@192.168.0.4\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o PasswordAuthenticati"]
[1560.456825, "o", "on=no -o ServerAliveInterval=300 -o ServerAliveCountMax=3 -o 'ProxyCommand=websocat asyncstdio: ws://127.0.0.1:7465/net-api/v1/net/4518df86ca084b86aa9884071432a30c/tcp/192.168.0.4/22 --binary -H=Authorization:'\"'\"'Bearer 9d3cc81565354bda917085e0f5b6d4ed'\"'\"'' -i /tmp/ray_on_golem/ray_on_golem_rsa_40fbf84919 -L8000:localhost:8000 root@192.168.0.4\r\nWarning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\nLinux (none) 5.10.29-0-virt #1-Alpine SMP Mon, 12 Apr 2021 15:48:39 UTC x86_64\r\n\nThe programs included with the Debian GNU/Linux system are free software;\r\nthe exact distribution terms for each program are described in the\r\nindividual files in /usr/share/doc/*/copyright.\r\n\nDebian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent\r\npermitted by applicable law.\r\nLast login: Thu Jun 13 11:04:09 2024 from 192.168.0.1\r\n(venv) root@(none):~# channel 3: open failed: connect failed: Connection refused\r\n"]
[1563.95819, "o", "\nlogout\r\n"]
[1563.958531, "o", "Connection to 192.168.0.4 closed.\r\n"]
[1563.962336, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1566.440613, "o", "\r\nexit\r\n"]
[1566.446995, "o", "\u001b[H\u001b[2J\u001b]11;\u0007 0  08ec7b29ff59ea2694a9b77803000000  ProxyActor\u001b[29CALIVE\u001b[4C03000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6  f30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1861  serve\b\b\n1  26af716be52a6546b11890af03000000  ProxyActor\u001b[29CALIVE\u001b[4C03000000  SERVE_CONTROLLER_ACTOR:SERVE_PROXY_ACTOR-e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d  e7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1737  serve\b\b\n2  3e21576f05e06b1d443d1c8103000000  ServeController\u001b[24CALIVE\u001b[4C03000000  SERVE_CONTROLLER_ACTOR\u001b[12C  \u001b[63Ce7558745fa524ed6a9d504a888d43682676ac5d47c9c7d2e918b566d   1704  serve\b\b\n3  8b0f2facdc22f4e4ed44ea4503000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C03000000  SERVE_REPLICA::default#WhoamiDeployment#uzbkyc\u001b[53Cf30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1683  serve\b\b\n4  98837e3a59592650e9f88e6303000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C03000000  SERVE_REPLICA::default#Whoa"]
[1566.44708, "o", "miDeployment#WaHNPc\u001b[53C944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   2164  serve\b\b\n5  98b2ccb6c2bac5593dca353403000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C03000000  SERVE_REPLICA::default#WhoamiDeployment#aWeKWS\u001b[53C944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23   2163  serve\b\b\n6  eb3a4cb0afe5db0b3d01696303000000  ServeReplica:default:WhoamiDeployment  ALIVE\u001b[4C03000000  SERVE_REPLICA::default#WhoamiDeployment#uXspkS\u001b[53Cf30ea419caa238874335c5fa8a3b0c585256408419c80e2f00a45df6   1684  serve\r\n\nShared connection to 192.168.0.4 closed.\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ time seq 100 | xargs -n1 -P50 python3 ray-serve-whoami-client.py | sort -n | uniq -c\u001b[25D\n25 {\"result\":\"192.168.0.5 8b0f2facdc22f4e4ed44ea4503000000\"}\u001b[60D\n24 {\"result\":\"192.168.0.5 eb3a4cb0afe5db0b3d01696303000000\"}\u001b[60D\n23 {\"result\":\"192.168.0.7 98837e3a59592650e9f88e6303000000\"}\u001b[60D\n28 {\"result\":\"192.168.0.7 98b2ccb6c2bac5593dca3534030000"]
[1566.447313, "o", "00\"}\r\n\nreal\u001b[4C0m15,962s\r\nuser\u001b[4C0m23,161s\r\nsys\u001b[5C0m2,045s\r\n(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1569.592974, "o", "\r\nexit\r\n"]
[1569.600611, "o", "\u001b[H\u001b[2J\u001b]11;\u0007\u001b[36m(ServeReplica:default:WhoamiDeployment pid=2164, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:18:22,924 default_WhoamiDeployment WaHNPc a2c28cd4-b1ee-4925-85a5-991ed66d7c35 / replica.py:772 - __CALL__ OK 504.8ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1683, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:18:22,130 default_WhoamiDeployment uzbkyc 1932def2-b899-4244-8286-c86b92a67c4a / replica.py:772 - __CALL__ OK 502.8ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=2163, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:18:23,300 default_WhoamiDeployment aWeKWS 1e974ac6-d2cb-44b0-a3bc-3deace625a10 / replica.py:772 - __CALL__ OK 503.6ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1684, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:18:22,473 default_WhoamiDeployment uXspkS f4e2b2d9-a9c7-4415-8d9c-d093a2ede2fe / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=2164, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:18:23,431 default_WhoamiDeployment WaHNPc b39b4812-5cf2"]
[1569.600867, "o", "-43ac-8653-fd55a549a611 / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1683, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:18:22,634 default_WhoamiDeployment uzbkyc a9023079-c87e-4c40-8c38-d5328e536758 / replica.py:772 - __CALL__ OK 503.5ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1684, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:18:22,978 default_WhoamiDeployment uXspkS 4883f77b-4644-487b-a0d7-245da8bd5c5f / replica.py:772 - __CALL__ OK 503.5ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=2163, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:18:23,814 default_WhoamiDeployment aWeKWS a67d0124-113d-454a-bcb6-133a9886df07 / replica.py:772 - __CALL__ OK 503.4ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1683, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:18:23,138 default_WhoamiDeployment uzbkyc d6f43ff5-a59d-4bc3-9224-808d6c7e5d41 / replica.py:772 - __CALL__ OK 503.1ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=2163, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:18:2"]
[1569.601053, "o", "4,321 default_WhoamiDeployment aWeKWS 6950b7a1-237b-43fd-a50b-5c3d8586dd1f / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=1683, ip=192.168.0.5)\u001b[C\u001b[39mINFO 2024-06-13 11:18:23,641 default_WhoamiDeployment uzbkyc b369264b-3653-4742-a980-f6a6de608d78 / replica.py:772 - __CALL__ OK 503.0ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=2163, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:18:24,826 default_WhoamiDeployment aWeKWS 3da15a64-bd54-4e74-9eae-7a0143d4ce7f / replica.py:772 - __CALL__ OK 503.2ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=2163, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:18:25,331 default_WhoamiDeployment aWeKWS de5cde59-66ac-40bf-8991-48f42be12074 / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ServeReplica:default:WhoamiDeployment pid=2163, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:18:25,837 default_WhoamiDeployment aWeKWS 12257990-3f91-4bd3-8d6c-2ed442af8215 / replica.py:772 - __CALL__ OK 503.3ms\r\n\u001b[36m(ProxyActor pid=2503, ip=192.168.0.7)\u001b[C\u001b[39"]
[1569.601225, "o", "mINFO 2024-06-13 11:18:37,430 proxy 192.168.0.7 proxy.py:1143 - Proxy actor d8d95ad298c4719bd713e94803000000 starting on node 944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23.\r\n\u001b[36m(ProxyActor pid=2503, ip=192.168.0.7)\u001b[C\u001b[39mINFO 2024-06-13 11:18:37,714 proxy 192.168.0.7 proxy.py:1357 - Starting HTTP server on node: 944edd3276472d2ab8f3837e5759f186777e31a7fed44ece71b41b23 listening on port 8000\r\n\u001b[36m(ProxyActor pid=2503, ip=192.168.0.7)\u001b[C\u001b[39mINFO:\u001b[5CStarted server process [2503]\r\n"]
[1571.686194, "o", "^C"]
[1571.726712, "o", "2024-06-13 11:18:43,781\u001b[7CINFO scripts.py:517 -- Got KeyboardInterrupt, shutting down...\r\n"]
[1571.773864, "o", "\u001b[36m(ServeController pid=1704)\u001b[39m INFO 2024-06-13 11:18:43,864 controller 1704 deployment_state.py:1857 - Removing 4 replicas from deployment 'WhoamiDeployment' in application 'default'.\r\n"]
[1574.288825, "o", "\u001b[36m(ServeController pid=1704)\u001b[39m INFO 2024-06-13 11:18:46,295 controller 1704 deployment_state.py:2185 - Replica default#WhoamiDeployment#aWeKWS is stopped.\r\n"]
[1574.4914, "o", "\u001b[36m(ServeController pid=1704)\u001b[39m INFO 2024-06-13 11:18:46,505 controller 1704 deployment_state.py:2185 - Replica default#WhoamiDeployment#WaHNPc is stopped.\r\n"]
[1574.690439, "o", "\u001b[36m(ServeController pid=1704)\u001b[39m INFO 2024-06-13 11:18:46,715 controller 1704 deployment_state.py:2185 - Replica default#WhoamiDeployment#uzbkyc is stopped.\r\n\u001b[36m(ServeController pid=1704)\u001b[39m INFO 2024-06-13 11:18:46,717 controller 1704 deployment_state.py:2185 - Replica default#WhoamiDeployment#uXspkS is stopped.\r\n"]
[1575.841413, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1575.96705, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1577.144808, "o", "r"]
[1577.19286, "o", "a"]
[1577.352759, "o", "y"]
[1577.416796, "o", " "]
[1577.496669, "o", "w"]
[1577.640882, "o", "o"]
[1577.736773, "o", "d"]
[1577.848475, "o", "n"]
[1577.945156, "o", " "]
[1578.121004, "o", "-"]
[1578.665132, "o", "\b\u001b[K"]
[1579.336839, "o", "\b\u001b[K"]
[1579.544765, "o", "\b\u001b[K"]
[1579.736441, "o", "\b\u001b[K"]
[1579.910379, "o", "\b\u001b[K"]
[1580.057048, "o", "\b\u001b[K"]
[1581.672761, "o", "d"]
[1582.120753, "o", "p"]
[1583.094546, "o", "w"]
[1583.40097, "o", "\b\u001b[K"]
[1583.545193, "o", "\b\u001b[K"]
[1583.736859, "o", "o"]
[1584.024695, "o", "w"]
[1584.185045, "o", "n"]
[1584.264691, "o", " "]
[1584.616428, "o", "-"]
[1584.760949, "o", "-"]
[1585.224811, "o", "\b\u001b[K"]
[1585.384792, "o", "\b\u001b[K"]
[1585.879958, "o", "w"]
[1585.976249, "o", "h"]
[1586.040741, "o", "o"]
[1586.161274, "o", "ami-old.yaml "]
[1586.489556, "o", "\b\b\b\b\b\b\b\b\b\b\u001b[4P.yaml "]
[1587.608571, "o", "-"]
[1587.769057, "o", "-"]
[1587.976801, "o", "y"]
[1588.024492, "o", "e"]
[1588.216578, "o", "s"]
[1588.360715, "o", "\r\n"]
[1589.094417, "o", "2024-06-13 13:19:02,099 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1589.094965, "o", "\u001b[33mLoaded cached provider configuration\u001b[39m\r\n"]
[1589.095097, "o", "\u001b[33mIf you experience issues with the cloud provider, try re-running the command with \u001b[1m--no-config-cache\u001b[0m\u001b[33m.\u001b[39m\r\n"]
[1589.095189, "o", "Destroying cluster. \u001b[4mConfirm [y/N]:\u001b[24m y \u001b[2m[automatic, due to --yes]\u001b[0m\r\n"]
[1589.102434, "o", "2024-06-13 13:19:02,107 INFO util.py:375 -- setting max workers for head node type to 0\r\n"]
[1589.122781, "o", "\u001b[36mRay On Golem 0.11.0a2\u001b[39m\r\n"]
[1589.1528, "o", "  Not starting the webserver, as it's already running\r\n"]
[1589.244972, "o", "\n  Running Ray on Golem on the mainnet requires GLM and MATIC tokens on the Polygon blockchain (see: https://docs.golem.network/docs/creators/ray/mainnet).\r\n  Your wallet:\r\n"]
[1589.245126, "o", "    \r\n    Status for account: 0xfebfd0884073054c430a3d175ac9a02f3b83dd49\r\n    Payment Driver status: OK\r\n    \r\n    ┌────────────────────┬────────────────┬─────────────────────────────┬─────────────┬────────────┬────────────────────────────┬─────────────────┐\r\n    │  platform          │  total amount  │  reserved                   │  amount     │  incoming  │  outgoing                  │  gas            │\r\n"]
[1589.245224, "o", "    ├────────────────────┼────────────────┼─────────────────────────────┼─────────────┼────────────┼────────────────────────────┼─────────────────┤\r\n    │  driver: erc20     │  99.6491 GLM   │  24.984466741216976333 GLM  │  accepted   │  0 GLM     │  0.121812370493913456 GLM  │  13.3199 MATIC  │\r\n    │  network: polygon  │                │                             │  confirmed  │  0 GLM     │  0.019622057074187837 GLM  │                 │\r\n    │  token: GLM        │                │                             │  requested  │  0 GLM     │  0.121812370493913456 GLM  │                 │\r\n    └────────────────────┴────────"]
[1589.245248, "o", "────────┴─────────────────────────────┴─────────────┴────────────┴────────────────────────────┴─────────────────┘\r\n  \r\n  You can use the Golem Onboarding portal to top up: https://glm.golem.network/#/onboarding/budget?yagnaAddress=0xfebfd0884073054c430a3d175ac9a02f3b83dd49&network=polygon\r\n  \r\n"]
[1589.253833, "o", "\u001b[37mFetched IP\u001b[39m: \u001b[1m192.168.0.4\u001b[0m\r\n"]
[1589.662876, "o", "Warning: Permanently added '192.168.0.4' (ED25519) to the list of known hosts.\r\n"]
[1594.249371, "o", "1/1 stopped.\r"]
[1594.266484, "o", "1/4 stopped.\r"]
[1594.520467, "o", "2/4 stopped.\r"]
[1594.563218, "o", "3/4 stopped.\r"]
[1594.85312, "o", "4/4 stopped.\r"]
[1600.233554, "o", "\u001b[33mStopped only 5 out of 6 Ray processes within the grace period 16 seconds. Set `\u001b[1m-v\u001b[0m\u001b[33m` to see more details. Remaining processes [psutil.Process(pid=799, name='gcs_server', status='terminated', started='11:00:03')] will be forcefully terminated.\u001b[39m\r\n"]
[1600.274822, "o", "\u001b[33mYou can also use `\u001b[1m--force\u001b[0m\u001b[33m` to forcefully terminate processes or set higher `--grace-period` to wait longer time for proper termination.\u001b[39m\r\n"]
[1600.33409, "o", "Shared connection to 192.168.0.4 closed.\r\n"]
[1600.345949, "o", "2024-06-13 13:19:13,351 INFO node_provider.py:173 -- NodeProvider: node0: Terminating node\r\n"]
[1601.204619, "o", "2024-06-13 13:19:14,209 INFO node_provider.py:173 -- NodeProvider: node1: Terminating node\r\n"]
[1601.208466, "o", "2024-06-13 13:19:14,213 INFO node_provider.py:173 -- NodeProvider: node4: Terminating node\r\n"]
[1601.211436, "o", "Requested \u001b[1m3\u001b[0m nodes to shut down.\u001b[2m [interval=1s]\u001b[0m\r\n"]
[1606.22684, "o", "\u001b[1m0\u001b[0m nodes remaining after 5 second(s).\r\n\u001b[32mNo nodes remaining.\u001b[39m\r\n"]
[1606.365212, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1611.977292, "o", "r"]
[1612.040912, "o", "a"]
[1612.201196, "o", "y"]
[1612.441421, "o", "-"]
[1612.649484, "o", "o"]
[1612.841478, "o", "n"]
[1612.959964, "o", "-golem "]
[1614.296998, "o", "s"]
[1614.441345, "o", "t"]
[1614.569327, "o", "o"]
[1614.68132, "o", "p"]
[1614.761075, "o", " "]
[1616.009028, "o", "\r\n"]
[1616.723829, "o", "Requesting webserver shutdown...\r\n"]
[1616.725604, "o", "Webserver shutdown requested, will stop soon.\r\n"]
[1616.730597, "o", "Waiting 180 seconds for the webserver process to stop.\r\n"]
[1660.784415, "o", "Webserver process stopped.\r\n"]
[1660.78487, "o", "Shutdown completed.\r\n"]
[1660.924204, "o", "(ray-serve-test-0) \u001b[1m\u001b[32mmateusz@fladra\u001b[0m:\u001b[1m\u001b[34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[0m$ "]
[1669.817589, "o", "\r\nexit\r\n"]
[1669.823857, "o", "\u001b[?1l\u001b>\u001b]11;\u0007\u001b[34;1H\r\n\u001b[?1049l\u001b[23;0;0t[screen is terminating]\r\n"]
[1669.825175, "o", "\u001b[?2004h"]
[1669.825268, "o", "\u001b]0;mateusz@fladra: ~/devel/ray-serve-test-0/ray-serve-whoami\u0007\u001b[01;32mmateusz@fladra\u001b[00m:\u001b[01;34m~/devel/ray-serve-test-0/ray-serve-whoami\u001b[00m$ "]
[1671.46522, "o", "\u001b[?2004l\r\r\nexit\r\n"]
