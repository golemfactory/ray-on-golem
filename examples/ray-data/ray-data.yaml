# Ray on Golem cluster name
cluster_name: golem-cluster

# The maximum number of workers the cluster will have at any given time
max_workers: 10

# The number of minutes that need to pass before an idle worker node is removed by the Autoscaler
idle_timeout_minutes: 15

# The cloud provider-specific configuration properties.
provider:
  type: "external"
  use_internal_ips: true
  module: "ray_on_golem.provider.node_provider.GolemNodeProvider"
  parameters:
    # Port of golem webserver that has connection with golem network
    webserver_port: 4578

    enable_registry_stats: false

    # Blockchain used for payments.
    # Goerli means running free nodes on testnet,
    # Polygon is for mainnet operations.
    payment_network: "holesky"
    # payment_network: "polygon"

    # Maximum amount of GLMs that's going to be spent for the whole cluster
    total_budget: 2

    node_config:
      subnet_tag: "public"
      # priority_head_subnet_tag: "public"

      # Parameters for golem demands (same for head and workers)
      demand:
        # Check available versions at https://registry.golem.network/explore/golem/ray-on-golem
        image_tag: "lucjandudek/ray-on-golem:0.11.0dev-py3.10.13-ray2.9.3-data"

        # List of urls which will be added to the Computation Manifest
        # Requires protocol to be defined in all URLs
        # If not provided demand will not use Computation Manifest
        # outbound_urls: ["https://pypi.dev.golem.network"]

      budget_control:
        per_cpu_expected_usage:
          # Estimated expected load and duration for worker that tells budget control to pick the least expensive Golem provider offers first.
          # If not provided, offers will be picked at random.
          cpu_load: 0.9
          duration_hours: 1

          # Amount of GLMs for expected usage which Golem provider offer will be rejected if exceeded.
          # Max cost is calculated as a sum of:
          # - start_price / duration / cpu count
          # - (env_per_hour_price * duration ) / cpu count
          # - cpu_per_hour_price / duration / cpu_load
          max_cost: 5

        # Amount of GLMs for worker initiation which Golem provider offer will be rejected if exceeded.
        max_start_price: 0.01

        # Amount of GLMs for CPU utilisation per hour which Golem provider offer will be rejected if exceeded.
        max_cpu_per_hour_price: 0.4

        # Amount of GLMs for each second that worker runs which Golem provider offer will be rejected if exceeded.
        max_env_per_hour_price: 0.3

        # Enables mid agreement payments
        # payment_interval_hours:
        #   minimal: 1
        #   optimal: 1

# The files or directories to copy to the head and worker nodes
file_mounts:
  # remote_path: local_path
  {
    "/app/ray_on_golem": "./ray_on_golem",
  }

rsync_exclude: [
  "**/__pycache__",
]

# <remote_path>: <local_path>
# Satisfy checks to disable warning about legacy fields at `ray up`.
# This will be removed by ray-on-golem on the fly.
head_node: True
worker_nodes: True

# Tells the autoscaler the allowed node types and the resources they provide
available_node_types:
  ray.head.default:
    min_workers: 0
    max_workers: 0
    resources: {}
    node_config: 
      demand:
        min_cpu_threads: 1
        max_cpu_threads: 4
        min_mem_gib: 2
        min_storage_gib: 1

  ray.worker.default:
    min_workers: 2
    resources: {}
    node_config: 
      demand:
        min_cpu_threads: 1
        max_cpu_threads: 16
        min_mem_gib: 2
        min_storage_gib: 1

# List of commands that will be run to initialize the nodes (before `setup_commands`)
initialization_commands:
  [
    # "cp -fR /app/golem/* $(python -c 'import site; print(site.getsitepackages()[0])')/golem",
    # "pip install ray[data]",
    # "pip install -Iv pandas==2.1.4",
    # "pip install tqdm"
  ]

# List of shell commands to run to set up nodes 
setup_commands: []

# Custom commands that will be run on the head node after common setup.
head_setup_commands: []

# Custom commands that will be run on worker nodes after common setup.
worker_setup_commands: []

# Command to start ray on the head node. You don't need to change this.
head_start_ray_commands:
  [
    "ray start --head --node-ip-address $NODE_IP --include-dashboard=True --dashboard-host 0.0.0.0 --disable-usage-stats --autoscaling-config=~/ray_bootstrap_config.yaml",
  ]

# Command to start ray on worker nodes. You don't need to change this.
worker_start_ray_commands: ["ray start --address $RAY_HEAD_IP:6379"]

# Authentication credentials that Ray will use to launch nodes
# auth:
# Custom username for ssh
# ssh_user: "root"

# If ssh_private_key will be not provided, temporary key will be created and used
# ssh_private_key: "~/.ssh/id_rsa"

# A list of paths to the files or directories to copy from the head node to the worker nodes
cluster_synced_files: []
